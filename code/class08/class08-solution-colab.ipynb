{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mraskj/css_fall2023/blob/main/code/class08/class08-solution-colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ae4a312",
      "metadata": {
        "id": "2ae4a312"
      },
      "source": [
        "# Class 8: Word Embeddings - Solution - Colab\n",
        "\n",
        "\n",
        "In this exercise we will explore whether word embeddings can be used to:\n",
        "    \n",
        "1. ideological scaling of party positions in the Danish parliament\n",
        "\n",
        "2. solve analogoes\n",
        "\n",
        "3. semantic scaling of latent dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e15c8711",
      "metadata": {
        "id": "e15c8711"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone GitHub directory into\n",
        "!git clone https://github.com/mraskj/css_fall2023.git"
      ],
      "metadata": {
        "id": "Ki_g2nEo6OmA"
      },
      "id": "Ki_g2nEo6OmA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install danlp\n",
        "# Note, however, that we will not use it later in the Colab version due to\n",
        "# conflicting versions. We install anyway, to follow the structure of the\n",
        "# local solution.\n",
        "!pip install danlp --quiet"
      ],
      "metadata": {
        "id": "Dzaa9t2J5-bW"
      },
      "id": "Dzaa9t2J5-bW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d13326ca",
      "metadata": {
        "id": "d13326ca"
      },
      "outputs": [],
      "source": [
        "# Import basic Python modules\n",
        "import os\n",
        "import platform\n",
        "\n",
        "# Regular expressions\n",
        "import re\n",
        "\n",
        "# Data management\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import namedtuple\n",
        "\n",
        "# Progress bars\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "\n",
        "# SpaCy\n",
        "import spacy\n",
        "\n",
        "# DANLP\n",
        "from danlp.models.embeddings import load_wv_with_gensim\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "# Plotting\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_style('darkgrid')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5534aa3d",
      "metadata": {
        "id": "5534aa3d"
      },
      "source": [
        "## 1 Ideological Scaling\n",
        "\n",
        "In this exercise, we will investigate the extent to which word embeddings can be leveraged to generate scaling estimates of parties' ideological positions in the Danish Parliament. The exercise is based on Rheault and Cochrane (2020), which you read for today's class.\n",
        "\n",
        "While pretrained embeddings generally perform well (see Rodriguez and Spirling 2022), we will train our own local word embeddings using a model called `Doc2Vec` in `gensim`. Unlike the standard `Word2Vec`, the `Doc2Vec` model enables to include metadata such as the party of the speaker, age, gender, and so on. You can think of it as including control variables in a regression model. Like Rheault and Cochrane (2020), we utilize this feature to scale positions of four Danish parties in a two-dimensional space using PCA-reduced embeddings. To ease the burden of fitting our model, we only scale positions for speeches given by legislators from either\n",
        "*EL*, *S*, *V*, or *DF*. We could use legislator-level \"fixed effects\", but we investigate the positions at the party level. Our selection of parties includes the two historical mainstream parties and the farest wing parties in recent decades (at least on average).\n",
        "\n",
        "We work with the same data (parliamentary speeches in the Danish parliament from 2000-2021) as we have done in the previous exercises.\n",
        "\n",
        "1. Read in the data directly from GitHub. Discard each dataset if it has less than $10,000$ speeches.\n",
        "2. Keep only speeches from *EL*, *S*, *V*, and *DF*\n",
        "3. Clean, tokenize, and preproces the data. Explain the steps you take. It is on purpose that I don't specify which steps you should take. To practice for the exam, you should be able to account for your choices. The arguments can be short and does not need to hit a homerun. What matters is that you demonstrate that you have thought carefully about what you do.\n",
        "4. Prepare your cleaned, preprocessed, and tokenized data for the `Doc2Vec` using the two functions I have provided to you\n",
        "\n",
        "        generate_indicators(*metatags)\n",
        "        generate_iterator(tokens, metatags)\n",
        "    where `generate_indicators` take the metadata we want to include in our model and `generate_iterator` takes the\n",
        "    tokens from step 3 (for the tokens argument) and the metatags created from `generate_indicators` (for the\n",
        "    metatags argument). You should provide a party indicator as metatags.\n",
        "\n",
        "5. Fit the `Doc2Vec` using the specifications to an object called `d2v`:\n",
        "    - `vector_size=300`\n",
        "    - `window=20`\n",
        "    - `min_count=5`\n",
        "    - `workers=8`\n",
        "    - `epochs=10`\n",
        "    - `sample=1e-3`\n",
        "    \n",
        "    and save the model after training using `d2v.save(model_fdir)` where you replace *model_fdir* with your own directory\n",
        "\n",
        "\n",
        "6. Use PCA to reduce the resulting party embeddings to two dimensions. Adapt your code for *class05-exercise/solution* to your new setting.\n",
        "\n",
        "7. Plot the two-dimensional party embeddings and interpret the results (again: see and adapt code for *class05*)\n",
        "\n",
        "8. Inspect the top words associated with each of ends of the two components (left-right, north-south). You can copy-paste the PCAInterpret Python class from *class5* to do so. Once again, adapt the code if necessary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15df5952",
      "metadata": {
        "id": "15df5952"
      },
      "outputs": [],
      "source": [
        "def generate_tags(*metatags):\n",
        "    \"\"\"\n",
        "    Generate indicators by combining multiple input tags.\n",
        "\n",
        "    This function takes multiple metatags and combines them by joining each element\n",
        "    with a hyphen ('-'). The result is a list of combined tags, which can be\n",
        "    used to label samples.\n",
        "\n",
        "    Parameters:\n",
        "    *metatags (tuple): A tuple of input tags.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of combined indicators.\n",
        "\n",
        "    Example:\n",
        "    >>> generate_tags(('A', 'B'), ('1', '2'))\n",
        "    ['A-1', 'B-2']\n",
        "    \"\"\"\n",
        "    tags = ['-'.join(map(str, t)) for t in zip(*metatags)]\n",
        "    return tags\n",
        "\n",
        "def generate_iterator(words, metatags):\n",
        "    \"\"\"\n",
        "    Generate an iterator of namedtuples containing words and tags.\n",
        "\n",
        "    This function creates an iterator that combines a list of words and a list\n",
        "    of metatags into namedtuples. Each namedtuple has two fields: 'words' and\n",
        "    'metatags', where 'words' is a list of tokens and 'metatags' is a list of associated tags.\n",
        "\n",
        "    Parameters:\n",
        "    words (nested list): A list of tokenized words.\n",
        "    metatags (list): A list of tags or indicators used to fit a Doc2Vec.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of namedtuples containing 'words' and 'tags'.\n",
        "\n",
        "    Example:\n",
        "    >>> generate_iterator(['apple', 'banana'], ['fruit', 'yellow'])\n",
        "    [docs(words=['apple'], tags=['fruit']), docs(words=['banana'], tags=['yellow'])]\n",
        "    \"\"\"\n",
        "    speech_iterator = namedtuple('docs', 'words tags')\n",
        "    iterator = [speech_iterator(x, [y]) for x, y in zip(words, metatags)]\n",
        "    return iterator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1aa2ebe",
      "metadata": {
        "id": "f1aa2ebe"
      },
      "source": [
        "#### 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cdfc4c1",
      "metadata": {
        "id": "0cdfc4c1"
      },
      "outputs": [],
      "source": [
        "# Generate file ids\n",
        "files = ['20001',\n",
        "         '20011',\n",
        "         '20012',\n",
        "         '20021',\n",
        "         '20031',\n",
        "         '20041',\n",
        "         '20042',\n",
        "         '20051',\n",
        "         '20061',\n",
        "         '20071',\n",
        "         '20072',\n",
        "         '20081',\n",
        "         '20091',\n",
        "         '20101',\n",
        "         '20102',\n",
        "         '20111',\n",
        "         '20121',\n",
        "         '20131',\n",
        "         '20141',\n",
        "         '20142',\n",
        "         '20151',\n",
        "         '20161',\n",
        "         '20171',\n",
        "         '20181',\n",
        "         '20182',\n",
        "         '20191',\n",
        "         '20201',\n",
        "         '20211']\n",
        "\n",
        "# Specify base url\n",
        "base_url = 'https://raw.githubusercontent.com/mraskj/css_fall2023/master/data/ft-speeches/'\n",
        "\n",
        "# Read in data. Solution here:\n",
        "df = pd.DataFrame()\n",
        "for file in tqdm(files):\n",
        "    df_term = pd.read_csv(base_url + file + '.csv')\n",
        "    if len(df_term) > 10000:\n",
        "        df = pd.concat([df, df_term])\n",
        "df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97c3e7f8",
      "metadata": {
        "id": "97c3e7f8"
      },
      "source": [
        "#### 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98f33e17",
      "metadata": {
        "id": "98f33e17"
      },
      "outputs": [],
      "source": [
        "# Define list of parties to keep\n",
        "parties = ['S', 'DF', 'EL', 'V']\n",
        "df_filtered = df.loc[df['party'].isin(parties)].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19821266",
      "metadata": {
        "id": "19821266"
      },
      "source": [
        "#### 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "045f73e6",
      "metadata": {
        "id": "045f73e6"
      },
      "outputs": [],
      "source": [
        "# Compute length of each speech on the original dataframe\n",
        "# and discard speeches outside the span of the 25th-75th percentile.\n",
        "# Note that it is perfectly fine to compute the number of words on the filtered dataframe as well.\n",
        "# This makes no difference at all.\n",
        "df['n_words'] = df.text.apply(lambda x: len(x))\n",
        "df_filtered['n_words'] = df_filtered.text.apply(lambda x: len(x))\n",
        "p25, p75 = np.quantile(df['n_words'], q=.25), np.quantile(df['n_words'], q=.75)\n",
        "df_filtered = df_filtered.loc[(df_filtered['n_words'] <= p75) & (df_filtered['n_words'] >= p25)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c7ce978",
      "metadata": {
        "id": "0c7ce978"
      },
      "outputs": [],
      "source": [
        "# Define names of legislators, parties, and context-specific stopwords that we want to remove\n",
        "names_to_remove = [x[:-1] + '[a-z]+' for x in list(df.speaker.unique())]\n",
        "\n",
        "procedural_to_remove = ['[Ll]ovforsla[a-z]+', 'ordfør[a-z]+', 'spørgsmå[a-z]+',\n",
        "                        'forsla[a-z]+', 'L', 'B', '[Hh]r', '[Ff]ru', '[Aa]fstemnin[a-z]+',\n",
        "                        '[Ff]orhandlin[a-z]+', '[Hh]r', '[Ff]ru']\n",
        "\n",
        "parties_to_remove = ['[Ll]iberal [Aa]llianc[a-z]+', 'LA', '[Dd]et [Kk]onservative [Ff]olkepar[a-z]+', 'KF',\n",
        "                   '[Dd]e [Kk]onservati[a-z]+', 'Venst[a-z]+', '[Dd]ansk [Ff]olkepart[a-z]+',\n",
        "                   '[Nn]ye [Bb]orgerli[a-z]+', '[Dd]e [Rr]adikal[a-z]+', '[Ss]ocialdemokratie[a-z+]',\n",
        "                   '[Ss]ocialdemokra[a-z]+', '[Ss]ocialistis[a-z]+ [Ff]olkepart[a-z]+', 'SF',\n",
        "                   '[Aa]lternative[a-z]+', '[Ee]nhedslist[a-z]+', '[Rr]adika[a-z]+']\n",
        "\n",
        "removal_words = parties_to_remove + procedural_to_remove + names_to_remove\n",
        "\n",
        "removal_pattern = r'\\b(?:' + '|'.join(removal_words) + r')\\b'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81df18c7",
      "metadata": {
        "id": "81df18c7"
      },
      "outputs": [],
      "source": [
        "# Define list to contain our corpus\n",
        "corpus = list(df_filtered['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db0ae367",
      "metadata": {
        "scrolled": true,
        "id": "db0ae367"
      },
      "outputs": [],
      "source": [
        "# Conduct text cleaning, preprocessing, and tokenization.\n",
        "# Note that I use a lazy tokenization version here. This is way more imprecise\n",
        "# than using SpaCy, but also much faster. For our purpose here, it is sufficient,\n",
        "# but ideally you use SpaCy.\n",
        "# Note also that I do not remove stopwords. It is fine if you remove them.\n",
        "# but rule-of-thumb is to preprocess text *less* when using word embeddings.\n",
        "# The reason is that we want to learn representations of language *as it is*.\n",
        "# Removing text might disturb this goal.\n",
        "\n",
        "# Remove \\xa0 from text\n",
        "corpus = [re.sub('\\xa0', '', doc) for doc in tqdm(corpus)]\n",
        "\n",
        "# Remove double or more consecutive whitespaces\n",
        "corpus = [re.sub(' +', ' ', doc) for doc in tqdm(corpus)]\n",
        "\n",
        "# Remove names defined above\n",
        "corpus = [re.sub(removal_pattern, '', doc) for doc in tqdm(corpus)]\n",
        "\n",
        "# Convert to lowercase\n",
        "corpus = [doc.lower() for doc in tqdm(corpus)]\n",
        "\n",
        "# Lazy tokenization where I split on whitespace (.split() uses ' ' as the default value)\n",
        "corpus = [doc.split() for doc in corpus]\n",
        "\n",
        "# Keep only tokens that contain three or more characters\n",
        "corpus = [[x for x in doc if len(x) >= 3] for doc in corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da07b208",
      "metadata": {
        "id": "da07b208"
      },
      "source": [
        "#### 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6956a325",
      "metadata": {
        "id": "6956a325"
      },
      "outputs": [],
      "source": [
        "# Prepare data for training\n",
        "covariates = generate_tags(list(df_filtered.party.values))\n",
        "iterator = generate_iterator(words=corpus, metatags=covariates)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aafd864f",
      "metadata": {
        "id": "aafd864f"
      },
      "source": [
        "#### 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bf571f8",
      "metadata": {
        "id": "0bf571f8"
      },
      "outputs": [],
      "source": [
        "# Fit the Doc2Vec model with:\n",
        "#    - `vector_size=300`\n",
        "#    - `window=20`\n",
        "#    - `min_count=5`\n",
        "#    - `workers=8`\n",
        "#    - `epochs=10`\n",
        "#    - `sample=1e-3`\n",
        "\n",
        "d2v = Doc2Vec(iterator,\n",
        "              vector_size=300,\n",
        "              window=20,\n",
        "              min_count=5,\n",
        "              workers=8,\n",
        "              epochs=10,\n",
        "              sample = 1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a220c56",
      "metadata": {
        "id": "9a220c56"
      },
      "source": [
        "#### 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "549460a7",
      "metadata": {
        "id": "549460a7"
      },
      "outputs": [],
      "source": [
        "def dimensionality_reduction(embed_model, indicators, n_components=2, return_dataframe=True):\n",
        "    \"\"\"\n",
        "    Perform dimensionality reduction on high-dimensional embeddings using PCA.\n",
        "\n",
        "    This function takes an embedding model (embed_model) and a list of indicators, and it performs\n",
        "    dimensionality reduction on the embeddings associated with those indicators\n",
        "    using PCA.\n",
        "\n",
        "    Parameters:\n",
        "    embed_model: An embedding model such as Doc2Vec or Word2Vec from gensim\n",
        "    indicators (list): A list of indicators or tags.\n",
        "    n_components (int): The number of principal components to retain (default is 2).\n",
        "    return_pandas (bool): If True, the result is returned as a pandas DataFrame with\n",
        "        columns 'PC1' and 'PC2' and an 'indicator' column (default is False).\n",
        "\n",
        "    Returns:\n",
        "    Z (array or DataFrame): An array or DataFrame containing the reduced-dimensional\n",
        "        embeddings. If return_pandas is True, Z is a DataFrame.\n",
        "    dr (PCA): The PCA model used for dimensionality reduction.\n",
        "\n",
        "    Example:\n",
        "    >>> Z, dr = dimensionality_reduction(embed_model, ['tag1', 'tag2'], n_components=2)\n",
        "    >>> Z, dr = dimensionality_reduction(embed_model, ['tag1', 'tag2'], return_pandas=True)\n",
        "    \"\"\"\n",
        "    z = np.zeros((len(indicators), embed_model.vector_size))\n",
        "\n",
        "    for i in range(len(indicators)):\n",
        "        z[i,:] = embed_model.dv[indicators[i]]                       # changed from embed_model.docvecs[indicators[i]]\n",
        "\n",
        "    dr = PCA(n_components=n_components)\n",
        "\n",
        "    for i in range(len(indicators)):\n",
        "        z[i,:] = embed_model.dv[indicators[i]]                            # changed from embed_model.docvecs[indicators[i]]\n",
        "    Z = dr.fit_transform(z)\n",
        "\n",
        "    if return_dataframe:\n",
        "        Z = pd.DataFrame(Z)\n",
        "        Z.columns = ['PC1', 'PC2']\n",
        "        Z['indicator'] = indicators\n",
        "\n",
        "    return Z, dr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1891848f",
      "metadata": {
        "id": "1891848f"
      },
      "outputs": [],
      "source": [
        "# Use PCA to reduce the 300-dimensional party embeddings to two dimensions\n",
        "Z, pca_model = dimensionality_reduction(embed_model=d2v, indicators=parties, n_components=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e7ed8ad",
      "metadata": {
        "id": "7e7ed8ad"
      },
      "source": [
        "#### 7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4a2afc1",
      "metadata": {
        "id": "d4a2afc1"
      },
      "outputs": [],
      "source": [
        "def plot_pca(dataframe, metatags:list, color:list, show=True):\n",
        "\n",
        "    \"\"\"\n",
        "    Plot a PCA visualization of data points in a DataFrame.\n",
        "\n",
        "    This function takes a DataFrame with PCA-reduced data, a list of metatags,\n",
        "    and a colormap (cmap) to visualize data points in a two-dimensional PCA space.\n",
        "\n",
        "    Parameters:\n",
        "    dataframe (DataFrame): A DataFrame containing PCA-reduced data with 'PC1' and 'PC2' columns.\n",
        "    metatags (list): A list of tags corresponding to data points.\n",
        "    color (list): A colormap to assign colors to data points based on a category.\n",
        "    show (bool): If True, the plot is displayed (default is True).\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "\n",
        "    Example:\n",
        "    >>> plot_pca(dataframe=Z, metatags=['V', 'S'], color=['red', 'blue'], show=True)\n",
        "    \"\"\"\n",
        "\n",
        "    ##if len(dataframe) != metatags:\n",
        "     #   raise ValueError(f\"dataframe and metatags must have the same length\")\n",
        "\n",
        "    mpl.rcParams['axes.titlesize'] = 20\n",
        "    mpl.rcParams['axes.labelsize'] = 20\n",
        "    mpl.rcParams['font.size'] = 14\n",
        "\n",
        "    plt.figure(figsize=(22,15))\n",
        "    plt.scatter(dataframe.PC1, dataframe.PC2, color=color)\n",
        "    texts=[]\n",
        "    for label, x, y, c in zip(metatags, Z.PC1, Z.PC2, color):\n",
        "        plt.annotate(\n",
        "            label,\n",
        "            xy=(x, y), xytext=(-20, 20),\n",
        "            textcoords='offset points', ha='right', va='bottom',\n",
        "            bbox=dict(boxstyle='round,pad=0.5', fc=c, alpha=0.3),\n",
        "            arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
        "\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "\n",
        "    if show:\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "004e4c5d",
      "metadata": {
        "id": "004e4c5d"
      },
      "outputs": [],
      "source": [
        "# Plot the two PCAs with labels for each party position\n",
        "n_tags = int(len(np.unique(parties)) / len(parties))\n",
        "cmap = {'V': 'blue', 'DF': 'gold', 'S': 'red', 'EL': 'sandybrown'}\n",
        "cols = [cmap['EL']]*n_tags + [cmap['S']]*n_tags + [cmap['V']]*n_tags + [cmap['DF']]*n_tags\n",
        "plot_pca(dataframe=Z, metatags=parties, color=cols, show=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9669cabe",
      "metadata": {
        "id": "9669cabe"
      },
      "source": [
        "#### 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0319cd78",
      "metadata": {
        "id": "0319cd78"
      },
      "outputs": [],
      "source": [
        "class PCA_INTERPRET(object):\n",
        "\n",
        "    \"\"\"\n",
        "    Perform interpretation of PCA results for word embeddings.\n",
        "\n",
        "    This class is designed to interpret PCA results of word embeddings, particularly for\n",
        "    understanding the semantics of the principal components. It provides methods for\n",
        "    sorting words based on their association with different components and directions.\n",
        "\n",
        "    Parameters:\n",
        "    model (Doc2Vec): A Doc2Vec model.\n",
        "    parties (list): A list of party labels.\n",
        "    dr (PCA): A PCA model for dimensionality reduction.\n",
        "    Z (array): PCA-reduced data.\n",
        "    labels (list): Labels for data points.\n",
        "    rev1 (bool): Reverse the first component direction (default is False).\n",
        "    rev2 (bool): Reverse the second component direction (default is False).\n",
        "    min_count (int): Minimum word count threshold (default is 100).\n",
        "    max_count (int): Maximum word count threshold (default is 1000000).\n",
        "    max_features (int): Maximum number of features (words) to consider (default is 10000).\n",
        "\n",
        "    Attributes:\n",
        "    model (Doc2Vec): A Doc2Vec model.\n",
        "    parties (list): A list of party labels.\n",
        "    labels (list): Labels for data points.\n",
        "    P (int): Number of parties.\n",
        "    M (int): Size of the word vectors.\n",
        "    voc (list): Sorted vocabulary based on word counts.\n",
        "    V (int): Number of words in the vocabulary.\n",
        "    pca (PCA): A PCA model for dimensionality reduction.\n",
        "    max (array): Maximum values in the reduced data.\n",
        "    min (array): Minimum values in the reduced data.\n",
        "    sims (DataFrame): Similarities of words in reduced space.\n",
        "\n",
        "    Example:\n",
        "    >>> pca_interpreter = PCA_INTERPRET(d2v_model, parties, pca_model, reduced_data, labels, rev1=False, rev2=True)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, parties, dr, Z, labels, rev1=False, rev2=False, min_count=100, max_count = 1000000, max_features=10000):\n",
        "\n",
        "        self.model = model\n",
        "        self.parties = parties\n",
        "        self.labels = labels\n",
        "        self.P = len(self.parties)\n",
        "        self.M = self.model.vector_size\n",
        "        self.voc = self.sorted_vocab(min_count, max_count, max_features)\n",
        "        self.V = len(self.voc)\n",
        "        self.pca = dr\n",
        "        self.max = Z.max(axis=0)\n",
        "        self.min = Z.min(axis=0)\n",
        "        self.sims = self.compute_sims()\n",
        "        self.dim1 = rev1\n",
        "        self.dim2 = rev2\n",
        "\n",
        "    def sorted_vocab(self, min_count=100, max_count=10000, max_features=10000):\n",
        "        # OLD CODE\n",
        "        # wordlist=[]\n",
        "        # for word, vocab_obj in self.model.wv.vocab.items():\n",
        "        #     wordlist.append((word, vocab_obj.count))\n",
        "\n",
        "        # NEW CODE\n",
        "        wordlist = []\n",
        "        for v in vocab:\n",
        "          vcount = d2v.wv.get_vecattr(v, 'count')\n",
        "          wordlist.append((v, vcount))\n",
        "\n",
        "        # THIS IS UNCHANGED\n",
        "        wordlist = sorted(wordlist, key=lambda tup: tup[1], reverse=True)\n",
        "        return [w for w,c in wordlist if c>min_count and c<max_count and w.count('_')<3][0:max_features]\n",
        "\n",
        "    def compute_sims(self):\n",
        "\n",
        "        Z = np.zeros((self.V, 2))\n",
        "        for idx, w in enumerate(self.voc):\n",
        "            Z[idx, :] = self.pca.transform(self.model.wv[w].reshape(1,-1))\n",
        "        sims_right = euclidean_distances(Z, np.array([self.max[0],0]).reshape(1, -1))\n",
        "        sims_left = euclidean_distances(Z, np.array([self.min[0],0]).reshape(1, -1))\n",
        "        sims_up = euclidean_distances(Z, np.array([0,self.max[1]]).reshape(1, -1))\n",
        "        sims_down = euclidean_distances(Z, np.array([0,self.min[1]]).reshape(1, -1))\n",
        "        temp = pd.DataFrame({'word': self.voc, 'right': sims_right[:,0], 'left': sims_left[:,0], 'up': sims_up[:,0], 'down': sims_down[:,0]})\n",
        "        return temp\n",
        "\n",
        "    def top_words_list(self, topn=20):\n",
        "\n",
        "        if self.dim1:\n",
        "            ordering = ['left','right']\n",
        "        else:\n",
        "            ordering = ['right', 'left']\n",
        "        temp = self.sims.sort_values(by=ordering[0])\n",
        "        print(80*\"-\")\n",
        "        print(\"Words Associated with Positive Values (Right) on First Component:\")\n",
        "        print(80*\"-\")\n",
        "        self.top_positive_dim1 = temp.word.tolist()[0:topn]\n",
        "        self.top_positive_dim1 = ', '.join([w.replace('_',' ') for w in self.top_positive_dim1])\n",
        "        print(self.top_positive_dim1)\n",
        "        temp = self.sims.sort_values(by=ordering[1])\n",
        "        print(80*\"-\")\n",
        "        print(\"Words Associated with Negative Values (Left) on First Component:\")\n",
        "        print(80*\"-\")\n",
        "        self.top_negative_dim1 = temp.word.tolist()[0:topn]\n",
        "        self.top_negative_dim1 = ', '.join([w.replace('_',' ') for w in self.top_negative_dim1])\n",
        "        print(self.top_negative_dim1)\n",
        "\n",
        "        if self.dim2:\n",
        "            ordering = ['down','up']\n",
        "        else:\n",
        "            ordering = ['up', 'down']\n",
        "        temp = self.sims.sort_values(by=ordering[0])\n",
        "        print(80*\"-\")\n",
        "        print(\"Words Associated with Positive Values (North) on Second Component:\")\n",
        "        print(80*\"-\")\n",
        "        self.top_positive_dim2 = temp.word.tolist()[0:topn]\n",
        "        self.top_positive_dim2 = ', '.join([w.replace('_',' ') for w in self.top_positive_dim2])\n",
        "        print(self.top_positive_dim2)\n",
        "        temp = self.sims.sort_values(by=ordering[1])\n",
        "        print(80*\"-\")\n",
        "        print(\"Words Associated with Negative Values (South) on Second Component:\")\n",
        "        print(80*\"-\")\n",
        "        self.top_negative_dim2 = temp.word.tolist()[0:topn]\n",
        "        self.top_negative_dim2 = ', '.join([w.replace('_',' ') for w in self.top_negative_dim2])\n",
        "        print(self.top_negative_dim2)\n",
        "        print(80*\"-\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66680558",
      "metadata": {
        "id": "66680558"
      },
      "outputs": [],
      "source": [
        "# Use PCA_INTERPRET class to get words associated with each of the two dimensions and their two directons\n",
        "PCA_INTERPRET(d2v, parties, pca_model, Z, parties, rev1=False, rev2=False, min_count=100, max_count = 1000000, max_features = 50000).top_words_list(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9da0b818",
      "metadata": {
        "id": "9da0b818"
      },
      "source": [
        "## Exercise 2: Analogies\n",
        "\n",
        "We start by exploring whether the iconic word analogy can be solved usign pretrained models for English and Danish language.\n",
        "\n",
        "1) Load in a pretrained model for English of your choice from `gensim` (https://radimrehurek.com/gensim/models/word2vec.html)\n",
        "    - Available models: ['fasttext-wiki-news-subwords-300',\n",
        "     'conceptnet-numberbatch-17-06-300',\n",
        "     'word2vec-ruscorpora-300',\n",
        "     'word2vec-google-news-300',\n",
        "     'glove-wiki-gigaword-50',\n",
        "     'glove-wiki-gigaword-100',\n",
        "     'glove-wiki-gigaword-200',\n",
        "     'glove-wiki-gigaword-300',\n",
        "     'glove-twitter-25',\n",
        "     'glove-twitter-50',\n",
        "     'glove-twitter-100',\n",
        "     'glove-twitter-200']\n",
        "\n",
        "2) Solve the analogy *\"man is to woman as king is to _____\"*. Interpret the results.\n",
        "\n",
        "3) Load in the pretrained model for Danish ['conll17.da.wv'] using the `danlp` module.\n",
        "\n",
        "4) Repeat step 2). Can the Danish embeddings also solve the analogy? Interpret the result and differences if there are any.\n",
        "\n",
        "5) Explore whether the changing the wording (e.g. from singular to plural) matters for the Danish model\n",
        "\n",
        "6) Can you come up with one or two other analogies that word embeddings might solve? Repeat step 2 and 4 for your new analogy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "764fdb26",
      "metadata": {
        "id": "764fdb26"
      },
      "outputs": [],
      "source": [
        "# 1) Loading pretrained model 'word2vec-google-news-300' from gensim\n",
        "w2v_en = gensim.downloader.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b50fcc24",
      "metadata": {
        "id": "b50fcc24"
      },
      "outputs": [],
      "source": [
        "# 2) Solve the analogy \"man is to woman as king is to ____\"\n",
        "top_similarity_en = w2v_en.most_similar(positive=['king', 'woman'], negative=['man'])\n",
        "print(top_similarity_en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd89653e",
      "metadata": {
        "id": "fd89653e"
      },
      "outputs": [],
      "source": [
        "# 3) Loading Danish model 'conll17.da.wv'\n",
        " # w2v_da = load_wv_with_gensim('conll17.da.wv')\n",
        " # NOT POSSIBLE!! DANLP uses a previous version of Gensim. The rest of the exercises can not be solved in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e9c06e6",
      "metadata": {
        "id": "3e9c06e6"
      },
      "outputs": [],
      "source": [
        "# 4) Solve the analogy \"man is to woman as king is to ____\"\n",
        "#top_similarity_da = w2v_da.most_similar(positive=['konge', 'kvinde'], negative=['mand'])\n",
        "#print(top_similarity_da)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a1fb561",
      "metadata": {
        "id": "4a1fb561"
      },
      "outputs": [],
      "source": [
        "# # 5) Construct gender pairs to explore sensitivity to wording\n",
        "# gender_pairs = [\n",
        "#     (\"mand\", \"kvinde\"),\n",
        "#     (\"mænd\", \"kvinder\"),\n",
        "#     (\"han\", \"hun\"),\n",
        "#     (\"ham\", \"hende\"),\n",
        "#     (\"hans\", \"hendes\")]\n",
        "\n",
        "# # Compute similarity for each gender pair\n",
        "# sim_list = []\n",
        "# for gp in gender_pairs:\n",
        "#     sim_list += w2v_da.most_similar(positive=[gp[1], 'konge'], negative=[gp[0]], topn=20)\n",
        "\n",
        "# # a) Convert similarity list into dataframe\n",
        "# # b) Groupby identified words and compute the average cosine similarity\n",
        "# # c) Keep only identified words that appear in 3/5 or more pairs\n",
        "# # d) Sort values by mean\n",
        "# word_df = pd.DataFrame(sim_list, columns=['word', 'score'])\n",
        "# word_df = word_df.groupby('word')['score'].describe().reset_index()\n",
        "# word_df = word_df.loc[word_df['count'] >= 3]\n",
        "# word_df = word_df.sort_values(['mean'], ascending=False)\n",
        "# word_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e170d0d",
      "metadata": {
        "id": "2e170d0d"
      },
      "outputs": [],
      "source": [
        "# 2.6) Solving: \"Copenhagen is to Denmark what London is to ____\"\n",
        "# w2v_da.most_similar(positive=['danmark', 'london'], negative=['københavn'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a4d7dc8",
      "metadata": {
        "id": "8a4d7dc8"
      },
      "outputs": [],
      "source": [
        "# 2.6) Solving: \"Messi is to football what Karabatic is to ____\"\n",
        "# w2v_da.most_similar(positive=['fodbold', 'karabatic'], negative=['messi'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe18fc5d",
      "metadata": {
        "id": "fe18fc5d"
      },
      "source": [
        "## 3 Semantic Scaling\n",
        "\n",
        "NOTE: THIS CAN NOT BE SOLVED USING THE PRETRAINED MODEL FROM DANLP AS IT USES AN OLDER VERSION OF GENSIM, WHICH IS NOT SUPPORTED BY COLAB.\n",
        "\n",
        "With fairly little effort, we were able to obtain a descent representation of the ideological positions of four Danish parties. For the ideological scaling exercise, we *learned* so-called party embeddings, which we then reduced to two dimensions using PCA. That we reduces the space to two-dimensions is a pure modeling choice, which is often used because it is convenient for visualization, and it also makes quite a lot of sense (an economic dimension and a cultural dimension).\n",
        "\n",
        "Word embeddings can also learn *semantic scales* directly from the learned word representations. In a much-cited paper by Kozlowski et al. (2019) \"*The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings*\" from 2019, it is shown that word embeddings capture semantic relations between words that map onto a cultural dimension of class. The paper exploits the \"analogy-solving\" feature of word embeddings (e.g. *\"man is to woman as king is to _____\"*) to generate group-based dimensions in the vector space such as class, gender, and race.\n",
        "\n",
        "In this exercise, we investigate whether we can use pretrained embeddings on Danish language ('conll17.da.wv') to analyze the cultural dimensions of social class. To do so, we construct an *affluence* dimension which basically spans from \"rich\" to \"poor\". This is simply done using vector subtraction $\\mathbf{a} - \\mathbf{b}$ where subtracting vector $\\mathbf{b}$ from $\\mathbf{a}$ yields a semantic meaningful dimension. Hence, subtracting $affluence - poverty$, we construct an affluence dimension.\n",
        "\n",
        "The most iconic example of this logic is the *\"man is to woman as king is to _____\"* where the correct answer is *queen*. Using word embeddings, this can be approximated by first constructing the gender dimension $gender_{dim} = woman - man$ and then adding the vector for $king$, i.e. $woman - man + king$. This has the effect of starting at $king$ and then taking one step on the gender dimension scaled from $woman$ to $man$.\n",
        "\n",
        "Below, I have given you a set of word pairs from Kozlowski et al. (2019), which I have translated using ChatGPT. Each word pair corresponds to the *affluence dimension*. This list is called `aff_pairs` and consists of $32$ word pairs. I also provide you three other lists called `sports`, `foods`, and `drinks`, respectively. These consists of words that we *project* onto the *affluence dimension* using cosine similarity. Since the vectors 'conll17.da.wv' are normalized, this corresponds 1:1 to matrix multiplication.\n",
        "\n",
        "\n",
        "1) Why do we use more than one word pair to construct the *affluence dimension*?\n",
        "2) Write a function that computes the cosine similarity. Use NumPy's `np.dot()` and `np.linalg.norm()` for the individual parts.\n",
        "3) Check whether each pair of words in `aff_pairs` are present in the vocabulary used to train 'conll17.da.wv'. Filter away pairs where one or both words are missing (if any at all).\n",
        "4) Construct the semantic dimension for *affluence* (*hint*: All you need to do is to iterate over each word pair subtract the two word vectors, and finally average over the pairs. I suggest a list comprehension.)\n",
        "5. Project each of the words in `sports`, `foods`, and `drinks` onto the dimension from step 4 using the cosine similarity you wrote in step 2. Save the result in a list and convert to a pandas dataframe with two columns: one with the projected word (e.g. 'håndbold') and one for the projection score (i.e. the cosine similarity).\n",
        "6. Visualize the results using a barchart sorted from highest to lowest. Interpret and discuss the results.\n",
        "7. In the exercise, we have projected words onto an *affluence dimension*, but this idea generalizes to any semantic scale. For instance, if you construct a gender dimension for kids ranging from $girl - boy$ and project toys (e.g. dukker, biler, lego, etc.), you should also get meaningful results. Now it's your turn to be creative. Create a semantic scale and project a set of words onto your scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62739bbe",
      "metadata": {
        "id": "62739bbe"
      },
      "outputs": [],
      "source": [
        "# Affluence word pairs\n",
        "# aff_pairs = [\n",
        "#     (\"rig\", \"fattig\"),\n",
        "#     (\"rigere\", \"fattigere\"),\n",
        "#     (\"rigeste\", \"fattigste\"),\n",
        "#     (\"velstand\", \"fattigdom\"),\n",
        "#     (\"fordelagtige\", \"ulemper\"),\n",
        "#     (\"fordelagtigt\", \"ulempe\"),\n",
        "#     (\"fordelagtig\", \"ulempen\"),\n",
        "#     (\"velhavende\", \"hjælpeløs\"),\n",
        "#     (\"elegant\", \"uelegant\"),\n",
        "#     (\"dyr\", \"billig\"),\n",
        "#     (\"dyrt\", \"billigt\"),\n",
        "#     (\"overklasse\", \"underklasse\"),\n",
        "#     (\"eksklusiv\", \"normalt\"),\n",
        "#     (\"luksuriøs\", \"elendig\"),\n",
        "#     (\"luksus\", \"billig\"),\n",
        "#     (\"velhavende\", \"fattig\"),\n",
        "#     (\"velstående\", \"lavindkomst\"),\n",
        "#     (\"dyr\", \"enkel\"),\n",
        "#     (\"værdifuld\", \"værdiløs\"),\n",
        "#     (\"privilegeret\", \"underprivilegeret\"),\n",
        "#     (\"privilegeret\", \"uprivilegeret\"),\n",
        "#     (\"ejendom\", \"almenbolig\"),\n",
        "#     (\"udviklet\", \"underudviklet\"),\n",
        "#     (\"succesfuld\", \"usuccesfuld\"),\n",
        "#     (\"prangende\", \"simpel\"),\n",
        "#     (\"velhavende\", \"trængende\"),\n",
        "#     (\"rent\", \"beskidt\"),\n",
        "#     (\"velhavende\", \"forarmet\"),\n",
        "#     (\"luksuriøs\", \"faldefærdig\"),\n",
        "#     (\"velholdt\", \"faldefærdig\"),\n",
        "#     (\"velholdt\", \"faldefærdigt\"),\n",
        "#     (\"overflod\", \"nødlidende\")\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8bd28b3",
      "metadata": {
        "id": "e8bd28b3"
      },
      "outputs": [],
      "source": [
        "# # Projection words\n",
        "# sports = ['fodbold', 'håndbold', 'ridning', 'hestesport','golf', 'tennis',\n",
        "#           'boksning', 'badminton', 'svømning', 'ishockey', 'hockey', 'spejder']\n",
        "\n",
        "# foods = ['pizza', 'burger', 'steak', 'slik', 'chips', 'saftevand', 'grøntsager', 'kartofler',\n",
        "#          'sovs', 'sauce','frikadeller', 'flæskesteg', 'østers', 'muslinger',\n",
        "#          'skaldyr', 'fisk']\n",
        "\n",
        "# drinks = [ 'cola', 'danskvand', 'rødvin', 'rosé', 'fadøl', 'dåseøl', 'cocktails', 'drinks', 'juice']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a96b6fc9",
      "metadata": {
        "id": "a96b6fc9"
      },
      "outputs": [],
      "source": [
        "# class EmbeddingProjections:\n",
        "\n",
        "#     def __init__(self, model, dimension_words, projection_words):\n",
        "\n",
        "#         self.model = model\n",
        "#         self.vocab = list(self.model.vocab.keys())\n",
        "#         self.projection_words = projection_words\n",
        "#         self.dimension_words = [p for p in dimension_words if p[0] in self.vocab and p[1] in self.vocab]\n",
        "\n",
        "\n",
        "##     @staticmethod\n",
        "#     def cos_similarity(a, b):\n",
        "#         return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "\n",
        "##     @staticmethod\n",
        "#     def plot_projections(dataframe, color='skyblue'):\n",
        "#         dataframe = dataframe.sort_values(['score'], ascending=True)\n",
        "#         plt.figure(figsize=(12, 8))\n",
        "#         plt.barh(dataframe['word'], dataframe['score'], color=color)\n",
        "#         plt.xlabel('Cosine similarity')\n",
        "#         plt.title('Word Embedding Scores')\n",
        "#         plt.tight_layout()\n",
        "\n",
        "\n",
        "#     def semantic_dimension(self, return_vectors=True, return_mean=False):\n",
        "#         dim = np.array([self.model[x[0]] - self.model[x[1]] for x in self.dimension_words])\n",
        "\n",
        "#         if return_vectors and not return_mean:\n",
        "#             return dim\n",
        "#         elif return_mean and not return_vectors:\n",
        "#             return np.mean(dim, axis=0)\n",
        "#         else:\n",
        "#             return dim, np.mean(dim, axis=0)\n",
        "\n",
        "\n",
        "#     def dim_projection(self, return_dataframe=True, verbose=False):\n",
        "\n",
        "#         dimension_vectors, dimension_mean = self.semantic_dimension(return_mean=True)\n",
        "\n",
        "#         score_list = []\n",
        "#         for word in self.projection_words:\n",
        "#             if word in self.vocab:\n",
        "#                 score = self.cos_similarity(self.model[word], dimension_mean)\n",
        "#                 score_list += [score]\n",
        "#                 if verbose:\n",
        "#                     print(f\"{word}: {score}\")\n",
        "#             else:\n",
        "#                 if verbose:\n",
        "#                     print(f\"{word} not in vocab\")\n",
        "\n",
        "#         if return_dataframe:\n",
        "#             self.score_df = pd.DataFrame({'word': self.projection_words, 'score': score_list})\n",
        "#             return self.score_df\n",
        "#         else:\n",
        "#             return score_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c72d36b",
      "metadata": {
        "id": "6c72d36b"
      },
      "outputs": [],
      "source": [
        "# Initiate instance of the EmbeddingProjections class\n",
        "# projector = EmbeddingProjections(model=w2v_da, dimension_words=aff_pairs, projection_words=foods)\n",
        "\n",
        "# # Compute projections\n",
        "# sport_scores = projector.dim_projection()\n",
        "\n",
        "# # Plot projections\n",
        "# projector.plot_projections(dataframe=sport_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "751d90e9",
      "metadata": {
        "id": "751d90e9"
      },
      "outputs": [],
      "source": [
        "# Identify gendered language for children's toy\n",
        "# gender_pairs = [('drenge', 'piger', 'dreng', 'pige', 'drengene', 'pigerne')]\n",
        "# toys = ['dukke', 'dukker', 'lego', 'biler', 'krig', 'tegne']\n",
        "# projector = EmbeddingProjections(model=w2v_da, dimension_words=gender_pairs, projection_words=toys)\n",
        "# toy_scores = projector.dim_projection()\n",
        "# projector.plot_projections(dataframe=toy_scores)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}