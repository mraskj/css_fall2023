{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e3d2868",
   "metadata": {},
   "source": [
    "# Class 8: Local and Pretrained Embeddings Using Gensim - Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60202e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic Python modules\n",
    "import os\n",
    "import pickle\n",
    "import platform\n",
    "import multiprocessing\n",
    "\n",
    "# Regular expressions\n",
    "import re\n",
    "\n",
    "# Data management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple, Counter\n",
    "\n",
    "# Progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "# SpaCy\n",
    "import spacy\n",
    "\n",
    "# DANLP\n",
    "from danlp.models.embeddings import load_wv_with_gensim\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdcd274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # Working Directory # # # #\n",
    "\n",
    "if platform.system() == 'Linux':\n",
    "    wd = '/home/rask/'\n",
    "else:\n",
    "    wd = 'C:/Users/au535365/'\n",
    "\n",
    "wd = os.path.join(wd, 'Dropbox/teaching/css_fall2023')\n",
    "    \n",
    "# Change directory\n",
    "os.chdir(wd)\n",
    "\n",
    "# Confirm that the working directory is as intended \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d03ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate file ids\n",
    "files = ['20001', \n",
    "         '20011',\n",
    "         '20012',\n",
    "         '20021',\n",
    "         '20031',\n",
    "         '20041',\n",
    "         '20042',\n",
    "         '20051',\n",
    "         '20061',\n",
    "         '20071',\n",
    "         '20072',\n",
    "         '20081',\n",
    "         '20091',\n",
    "         '20101',\n",
    "         '20102',\n",
    "         '20111',\n",
    "         '20121',\n",
    "         '20131',\n",
    "         '20141',\n",
    "         '20142',\n",
    "         '20151',\n",
    "         '20161',\n",
    "         '20171',\n",
    "         '20181',\n",
    "         '20182',\n",
    "         '20191',\n",
    "         '20201',\n",
    "         '20211']\n",
    "\n",
    "# Specify base url\n",
    "base_url = 'https://raw.githubusercontent.com/mraskj/css_fall2023/master/data/ft-speeches/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ca318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sampling(dataframe, strata_col, group_col, n_size=100, return_dataframe=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform stratified sampling from a DataFrame based on specified strata.\n",
    "\n",
    "    This function performs stratified sampling from a DataFrame based on the values\n",
    "    of the strata column. It ensures that the sample size for each stratum (group) is\n",
    "    as close as possible to the specified sample size.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (DataFrame): The DataFrame to sample from.\n",
    "    strata_col (str): The name of the column used as the stratum for sampling.\n",
    "    group_col (str): The name of the column used to define stratum groups.\n",
    "    n_size (int): The desired sample size for each stratum (default is 500).\n",
    "    return_dataframe (bool): If True, return a DataFrame with the sampled data;\n",
    "        if False, return a list of indices (default is True).\n",
    "\n",
    "    Returns:\n",
    "    DataFrame or list: If return_dataframe is True, a DataFrame with the sampled\n",
    "        data is returned. If return_dataframe is False, a list of indices is returned.\n",
    "\n",
    "    Example:\n",
    "    >>> sampled_data = stratified_sampling(df, 'age', 'gender', n_size=200)\n",
    "    >>> sampled_indices = stratified_sampling(df, 'income', 'region', n_size=300, return_dataframe=False)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sed seet to be able to replicate\n",
    "    np.random.seed(10)\n",
    "    \n",
    "    # Define empty list to store samples\n",
    "    samples = []\n",
    "   \n",
    "    # Validity checks\n",
    "    if group_col not in dataframe.columns:\n",
    "        raise KeyError(f\"Dataframe must have column {group_col}\")\n",
    "    else:\n",
    "        groups = list(dataframe['party'].unique())\n",
    "\n",
    "    \n",
    "    if strata_col not in dataframe.columns:\n",
    "        if 'text' in dataframe.columns:\n",
    "            dataframe[strata_col] = dataframe.text.apply(lambda x: len(x))\n",
    "        else:\n",
    "            raise KeyError(f\"Dataframe must have column {strata_col}\")\n",
    "    \n",
    "    # Keep speeches within the 25th-75th percentile span in terms of number of words\n",
    "    p25, p75 = np.quantile(dataframe[strata_col], q=.25), np.quantile(dataframe[strata_col], q=.75)\n",
    "    dataframe = dataframe.loc[(dataframe[strata_col] <= p75) & (dataframe[strata_col] >= p25)]\n",
    "    \n",
    "    # Apply sampling\n",
    "    for group in groups:\n",
    "\n",
    "        indices = list(dataframe.loc[dataframe[group_col] == group].index)\n",
    "\n",
    "        if (len(indices)) < n_size:\n",
    "            n_samples = len(indices)\n",
    "        else:\n",
    "            n_samples = n_size\n",
    "\n",
    "        samples += list(np.random.choice(indices, size=n_samples, replace=False))\n",
    "    \n",
    "    if return_dataframe:\n",
    "        return dataframe.loc[dataframe.index.isin(samples)].reset_index(drop=False)\n",
    "    else:\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "df = pd.DataFrame()\n",
    "for file in tqdm(files):\n",
    "    df_term = pd.read_csv(base_url + file + '.csv')\n",
    "    #df_term = pd.read_csv('data/ft-speeches/' + file + '.csv')\n",
    "    if len(df_term) > 10000:\n",
    "        sample_df = stratified_sampling(dataframe=df_term, strata_col='n_words', group_col='party')\n",
    "        df = pd.concat([df, sample_df])\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get an error, you should download the model first. Uncomment the line below (remove the hashtag #) and run. \n",
    "# !python -m spacy download da_core_news_md\n",
    "# # Load the model \"da_core_news_md\"\n",
    "spacy_pipeline_da = spacy.load(\"da_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a4d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define list with our corpus\n",
    "corpus_raw = list(df['text'])\n",
    "\n",
    "# # Tokenize\n",
    "tokens_raw = [[d for d in spacy_pipeline_da(doc)] for doc in tqdm(corpus_raw, position=0, leave=True)]\n",
    "\n",
    "# IGNORE THIS\n",
    "# tokenized_docs = [x[0].doc for x in tokens_raw]\n",
    "# for p, ixs in df.groupby('period').groups.items():\n",
    "#     with open(f'data/ft-speeches-tokenized/spacy_tokens_{p}.pkl', 'wb') as f:\n",
    "#         pickle.dump([tokenized_docs[i] for i in list(ixs)], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE THIS\n",
    "# Read in tokens\n",
    "#tokens_raw = []\n",
    "#for p, ixs in df.groupby('period').groups.items():\n",
    "#    print(f\"Period {p}\")\n",
    "#    with open(f'data/ft-speeches-tokenized/spacy_tokens_{p}.pkl', 'rb') as f:\n",
    "#        tokens_raw += pickle.load(f)\n",
    "#        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19651fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove:\n",
    "# - stopwords\n",
    "# - punctuation\n",
    "# - digits\n",
    "# - tokens shorter than 3 characters\n",
    "# - spaces\n",
    "\n",
    "stop_words = sorted(list(spacy_pipeline_da.Defaults.stop_words))\n",
    "\n",
    "# Remove stopwords\n",
    "tokens_cleaned = [[x for x in token if x.text not in stop_words] for token in tqdm(tokens_raw)]\n",
    "\n",
    "# Remove punctuation\n",
    "tokens_cleaned = [[x for x in token if not x.is_punct] for token in tqdm(tokens_cleaned)]\n",
    "\n",
    "# Remove digits\n",
    "tokens_cleaned = [[x for x in token if not x.is_digit] for token in tqdm(tokens_cleaned)]\n",
    "\n",
    "# Remove tokens shorter than 3 characters\n",
    "tokens_cleaned = [[x for x in token if not x.is_space] for token in tqdm(tokens_cleaned)]\n",
    "\n",
    "# Remove tokens shorter than 3 characters\n",
    "tokens_cleaned = [[x for x in token if len(x) >= 3] for token in tqdm(tokens_cleaned)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of CPU-cores on your computer\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17efd538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tokens from speeches given by RV legislators (should be of length 21*100=2100 since we have 21 years and 100 speeches\n",
    "# in each year - see stratified sampling)\n",
    "tokens_subset = [[x.lower_ for x in tokens_cleaned[i]] for i in list(df.groupby('party').groups['RV'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b466a0f0",
   "metadata": {},
   "source": [
    "## Word2Vec Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44104bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Fit word2vec model for RV \n",
    "w2v_example = gensim.models.Word2Vec(tokens_subset,  \n",
    "               workers=cores,  \n",
    "               size=200,      \n",
    "               min_count=5,  \n",
    "               window = 10, \n",
    "               sample = 1e-2, \n",
    "               iter = 10 \n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute 10 most similar words for 'flygtninge'\n",
    "w2v_example.wv.most_similar('flygtninge', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027fe51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack words and similarity scores in two separate objects\n",
    "top_words, top_sims = zip(*w2v_example.wv.most_similar('flygtninge', topn=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a8fb3e",
   "metadata": {},
   "source": [
    "### Doc2Vec Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e39aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tags(*metatags):\n",
    "    \"\"\"\n",
    "    Generate indicators by combining multiple input tags.\n",
    "\n",
    "    This function takes multiple metatags and combines them by joining each element\n",
    "    with a hyphen ('-'). The result is a list of combined tags, which can be\n",
    "    used to label samples.\n",
    "\n",
    "    Parameters:\n",
    "    *metatags (tuple): A tuple of input tags.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of combined indicators.\n",
    "\n",
    "    Example:\n",
    "    >>> generate_tags(('A', 'B'), ('1', '2'))\n",
    "    ['A-1', 'B-2']\n",
    "    \"\"\"\n",
    "    tags = ['-'.join(map(str, t)) for t in zip(*metatags)]\n",
    "    return tags\n",
    "\n",
    "def generate_iterator(words, metatags):\n",
    "    \"\"\"\n",
    "    Generate an iterator of namedtuples containing words and tags.\n",
    "\n",
    "    This function creates an iterator that combines a list of words and a list\n",
    "    of metatags into namedtuples. Each namedtuple has two fields: 'words' and\n",
    "    'metatags', where 'words' is a list of tokenized words and 'metatags' is a list of associated tags.\n",
    "\n",
    "    Parameters:\n",
    "    words (nested list): A list of tokenized words.\n",
    "    metatags (list): A list of tags or indicators used to fit a Doc2Vec.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of namedtuples containing 'tokens' and 'tags'.\n",
    "\n",
    "    Example:\n",
    "    >>> generate_iterator(['apple', 'banana'], ['fruit', 'yellow'])\n",
    "    [speeches(words=['apple'], tags=['fruit']), speeches(words=['banana'], tags=['yellow'])]\n",
    "    \"\"\"\n",
    "    speech_iterator = namedtuple('docs', 'words tags')\n",
    "    iterator = [speech_iterator(x, [str(y)]) for x, y in zip(words, metatags)]\n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d502ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate tags\n",
    "tags = ['-'.join(map(str, t)) for t in zip(df.party)]\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f547326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate tags with function\n",
    "generate_tags(df.party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39163291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate iterator for doc2vec\n",
    "speech_iterator = namedtuple('docs', 'tokens tags')\n",
    "iterator = [speech_iterator(x, [y]) for x, y in zip(tokens_cleaned, tags)]\n",
    "iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da6d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate iterator with function \n",
    "iterator = generate_iterator(words=tokens_cleaned, metatags=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdd8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct iterator for RV \n",
    "period_tags = []\n",
    "tokens_subset = []\n",
    "for i in list(df.groupby('party').groups['RV']):\n",
    "    period_tags.append(df.iloc[i].period)\n",
    "    tokens_subset.append([x.lower_ for x in tokens_cleaned[i]])\n",
    "iterator = generate_iterator(words=tokens_subset, metatags=period_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ffb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Doc2Vec model with:\n",
    "#    - `vector_size=100`\n",
    "#    - `window=10`\n",
    "#    - `min_count=5`\n",
    "#    - `workers=24`\n",
    "#    - `epochs=10`\n",
    "#    - `sample=1e-3`\n",
    "\n",
    "d2v = Doc2Vec(iterator,\n",
    "              vector_size=100, \n",
    "              window=10, \n",
    "              min_count=5,\n",
    "              workers=cores,\n",
    "              epochs=10, \n",
    "              sample = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e27e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute most similar words for 'asyl' based on the trained d2v model\n",
    "d2v.wv.most_similar('asyl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ce671",
   "metadata": {},
   "source": [
    "## Party Differences In Sentiment on Immigration Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eee1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in AFINN\n",
    "afinn = pd.read_csv('data/AFINN-da-32.txt', sep='\\t', header=None, names=['word', 'score'])\n",
    "\n",
    "print(afinn.sample(10))\n",
    "\n",
    "# Keep only single words\n",
    "afinn = afinn.loc[afinn['word'].apply(lambda x: len(x.split())) == 1,].reset_index(drop=True)\n",
    "\n",
    "# Generate a boolean variable denoting if a word is positive or not\n",
    "afinn['positive'] = afinn['score'] > 0 \n",
    "\n",
    "# Define a function to compute the intersection between two lists\n",
    "def intersection(l1, l2):\n",
    "    return [v for v in l1 if v in l2]\n",
    "\n",
    "# Define function to compute the number of positive and negative words and their mean score\n",
    "def get_wordscores(df_, dict_, key_):\n",
    "    pdf = df_.loc[df_.word.isin(dict_['shared_words'])].groupby('positive').describe()\n",
    "    pdf = pdf.score.reset_index()\n",
    "    pdf = pdf[['positive','count', 'mean']]\n",
    "    pdf['party'] = [key_] * len(pdf)\n",
    "    pdf['vocab_size'] = [dict_['vocab_size']] * len(pdf)\n",
    "    return pdf\n",
    "\n",
    "# Define function to compute size of vocab\n",
    "def vocab_size(tokens):\n",
    "    word_freq = Counter()\n",
    "    for doc in tokens:\n",
    "        for word in doc:\n",
    "            word_freq[word] += 1\n",
    "    return word_freq, len(word_freq)\n",
    "\n",
    "afinn_words = list(afinn.word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ecca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of parties\n",
    "df_groups = df.groupby('party').groups\n",
    "parties = list(df_groups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb31807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "party_dict = {}\n",
    "party_score_df = pd.DataFrame()\n",
    "\n",
    "for party in tqdm(parties):\n",
    "    \n",
    "    party_dict[party] = {}\n",
    "    \n",
    "    tokens_subset = [[x.lower_ for x in tokens_cleaned[i]] for i in list(df_groups[party])]\n",
    "    \n",
    "    wordfreq, vsize = vocab_size(tokens=tokens_subset)\n",
    "    \n",
    "    w2v_party = gensim.models.Word2Vec(tokens_subset,  \n",
    "                   workers=cores,  \n",
    "                   size=100,      \n",
    "                   min_count=5,  \n",
    "                   window = 10, \n",
    "                   sample = 1e-3, \n",
    "                   iter = 10 \n",
    "                   )\n",
    "    \n",
    "    party_dict[party]['word2vec'] = w2v_party\n",
    "    \n",
    "    top_words, top_sims = zip(*party_dict[party]['word2vec'].wv.most_similar('asyl', topn=100))\n",
    "    \n",
    "    party_dict[party]['results'] = {'words': list(top_words), \n",
    "                                    'similarity': list(top_sims), \n",
    "                                    'shared_words': intersection(afinn_words, list(top_words)),\n",
    "                                    'vocab_size': vsize}\n",
    "    \n",
    "    party_score_df = pd.concat([party_score_df, get_wordscores(afinn,  party_dict[party]['results'], party)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f9a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "party_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ec9ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group the DataFrame by 'party', 'positive', and calculate the mean and sum of 'mean' and 'count'\n",
    "grouped = party_score_df.groupby(['party', 'positive']).agg({'mean': 'mean', 'count': 'sum'}).unstack().reset_index()\n",
    "\n",
    "# Create a dumbbell plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "index = range(len(grouped['party']))\n",
    "line_height = 0.2  # Adjust this value to control the spacing between points\n",
    "\n",
    "# Plot lines connecting points for 'Positive' True and False\n",
    "for i in index:\n",
    "    ax.plot([grouped['mean'][True][i], grouped['mean'][False][i]], [i, i], marker='o', markersize=10, label=grouped['party'][i])\n",
    "\n",
    "    # Add text labels above each dot\n",
    "    ax.text(grouped['mean'][True][i], i + line_height, f\"Pos: {int(grouped['count'][True][i])}\", ha='center')\n",
    "    ax.text(grouped['mean'][False][i], i + line_height, f\"Neg: {int(grouped['count'][False][i])}\", ha='center')\n",
    "\n",
    "\n",
    "# Add labels beneath the brackets\n",
    "ax.text(-1.75, -1.5, 'Negative words', ha='center', fontsize=12)\n",
    "ax.text(1.75, -1.5, 'Positive words', ha='center', fontsize=12)\n",
    "ax.set_xlim(left=-3.5, right=3.5)\n",
    "ax.set_yticks(index)\n",
    "ax.set_yticklabels(grouped['party'])\n",
    "#plt.xlabel('Mean Value')\n",
    "# plt.title('Dumbbell Plot of Mean Values by Party and Positivity')\n",
    "# plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a8bb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dumbbell plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "index = range(len(grouped['party']))\n",
    "line_height = 0.2  # Adjust this value to control the spacing between points\n",
    "\n",
    "# Plot lines connecting points for 'Positive' True and False\n",
    "for i in index:\n",
    "    ax.plot([grouped['count'][True][i], grouped['count'][False][i]], [i, i], marker='o', markersize=10, label=grouped['party'][i])\n",
    "    \n",
    "    # Add text labels above each dot\n",
    "    ax.text(grouped['count'][True][i], i + line_height, f\"Pos: {int(grouped['count'][True][i])}\", ha='center')\n",
    "    ax.text(grouped['count'][False][i], i + line_height, f\"Neg: {int(grouped['count'][False][i])}\", ha='center')\n",
    "\n",
    "# Add labels beneath the brackets\n",
    "ax.set_xlim(left=-0, right=20)\n",
    "ax.set_yticks(index)\n",
    "ax.set_xticks(range(0, 21))\n",
    "ax.set_yticklabels(grouped['party'])\n",
    "plt.xlabel('Number of Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c3aaa",
   "metadata": {},
   "source": [
    "## Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af85c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "from danlp.models.embeddings import load_wv_with_gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0fbaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pretrained model 'word2vec-google-news-300'\n",
    "w2v_en = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Danish model 'conll17.da.wv'\n",
    "w2v_da = load_wv_with_gensim('conll17.da.wv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d41f31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most similar words for 'israel' for the English model\n",
    "w2v_en.most_similar('israel', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136dd50c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Most similar words for 'israel' for the Danish model\n",
    "w2v_da.most_similar('israel', topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8323cf5b",
   "metadata": {},
   "source": [
    "## Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analogies: father is to son as mother is to X\n",
    "w2v_da.most_similar(positive=['moderen', 'sønnen'], negative=['faderen'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
