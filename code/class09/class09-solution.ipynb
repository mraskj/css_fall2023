{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4dU44+RJiBanDaf2nAZoP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mraskj/css_fall2023/blob/main/code/class09/class09-solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class 09: Audio Basics - Solution\n",
        "\n",
        "In this exercise, we explore how to play around with audio in Python. We'll learn how plot waveforms, convert audio from the time to the frequency domain, and how to convert audio into spectrograms.\n",
        "\n"
      ],
      "metadata": {
        "id": "zucTOZY880HP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "We start by:\n",
        "\n",
        "1. Cloning the course GitHub repo\n",
        "2. Importing necessary modules\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OHfXpOoHGxZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.1 Cloning GitHub Repository"
      ],
      "metadata": {
        "id": "tcesj3dKg8zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone GitHub directory into\n",
        "!git clone https://github.com/mraskj/css_fall2023.git"
      ],
      "metadata": {
        "id": "4QD5e4-0-DhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.2 Importing Modules"
      ],
      "metadata": {
        "id": "xHz89nDX-hUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MODULES\n",
        "\n",
        "# For file and directory management\n",
        "import os\n",
        "\n",
        "# For data handling\n",
        "import numpy as np\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For signal processing\n",
        "import scipy\n",
        "import librosa\n",
        "from scipy.io import wavfile"
      ],
      "metadata": {
        "id": "asmX0NwD_BTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Reading and Writing Audio Files"
      ],
      "metadata": {
        "id": "wEEz-bED-WmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1.0: Reading Audio\n",
        "\n",
        "1. Read in one of the audio files from *data/audio/class9/*. You decide which one.\n",
        "2. After reading the file, inspect the sampling rate and the characteristics of the audio signal (e.g. number of samples, sampling rate, duration, max and min values, and so on)\n",
        "\n",
        "[https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.read.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.read.html)"
      ],
      "metadata": {
        "id": "kcyoUcQFDGZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.0 Solution"
      ],
      "metadata": {
        "id": "20VYb04N5pvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function that prints varies characteristics of the audio signal.\n",
        "def describe_audio(signal, sr):\n",
        "  \"\"\"\n",
        "  Analyze and describe characteristics of an audio signal.\n",
        "\n",
        "  Parameters:\n",
        "  signal (numpy.ndarray): The input audio signal to be analyzed.\n",
        "  sr (int): The sample rate (in Hz) of the input signal.\n",
        "\n",
        "  Returns:\n",
        "  dict:\n",
        "      - 'n_samples': The length of the signal.\n",
        "      - 'sr': The sample rate.\n",
        "      - 'duration': The duration of the signal in seconds.\n",
        "      - 'bit_depth': The data type of the signal.\n",
        "      - 'max_signal': The maximum amplitude in the signal.\n",
        "      - 'min_signal': The minimum amplitude in the signal.\n",
        "      - 'mean_signal': The mean amplitude of the signal.\n",
        "      - 'std_signal': The standard deviation of the amplitudes in the signal.\n",
        "  \"\"\"\n",
        "\n",
        "  print(f\"Length of signal: {len(signal)}\")\n",
        "  print(f\"Sampling rate of the signal: {sr}\")\n",
        "  print(f\"Duration (s); {len(signal)/sr}\")\n",
        "  print(f\"Bit depth: {signal.dtype}\")\n",
        "  print(f\"Max amplitude: {np.max(signal)}\")\n",
        "  print(f\"Min amplitude: {np.min(signal)}\")\n",
        "  print(f\"Mean amplitude: {np.mean(signal)}\")\n",
        "  print(f\"Std amplitude: {np.std(signal)}\")\n",
        "\n",
        "  return {'n_samples': len(signal),\n",
        "          'sr': sr,\n",
        "          'duration': len(signal)/sr,\n",
        "          'bit_depth': signal.dtype,\n",
        "          'max_signal': np.max(signal),\n",
        "          'min_signal': np.min(signal),\n",
        "          'mean_signal': np.mean(signal),\n",
        "          'std_signal': np.std(signal)}"
      ],
      "metadata": {
        "id": "gGIGK8uS6T21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define base directory\n",
        "base_dir = os.path.join(os.getcwd(), 'css_fall2023/data/audio/class09')\n",
        "print(f\"Directory: {base_dir}\")\n",
        "\n",
        "# Define audio filepath\n",
        "fname = 'speaker0_q90'\n",
        "audio_fpath = os.path.join(base_dir, fname + '.wav')\n",
        "print(f\"Audio filepath: {audio_fpath}\")"
      ],
      "metadata": {
        "id": "xq9LlLn7_RZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read audio file using the read function from the wavfile class from scipy\n",
        "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.read.\n",
        "sr, signal = wavfile.read(audio_fpath)"
      ],
      "metadata": {
        "id": "FQwq5fYc5xUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the audio signal\n",
        "audio_info = describe_audio(signal, sr)"
      ],
      "metadata": {
        "id": "9ZiNNBKv5vA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1.1: Writing Audio\n",
        "\n",
        "Audio files can easily be saved as files using the `wavfile.write` function. Define an audio signal of your choice (e.g. a sine wave) with frequency $f$, length $l$ (i.e duration in seconds) and sampling rate $sr$. You decide the various values, but I recommend $f=[50, 100]$ and $l=[0.0, 5.0]$.  \n",
        "\n",
        "\n",
        "[https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.write.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.write.html)"
      ],
      "metadata": {
        "id": "ufYoMJ_XDD5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution 1.1"
      ],
      "metadata": {
        "id": "-pzg7e8Q8Auq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sampling rate\n",
        "sr = 16000\n",
        "\n",
        "# Define frequency\n",
        "f = 100\n",
        "\n",
        "# Define duration\n",
        "length = 1.0\n",
        "\n",
        "# Construct time samples\n",
        "t = np.linspace(0, length, sr)\n",
        "\n",
        "# Construct amplitude\n",
        "amplitude = np.iinfo(np.int16).max\n",
        "\n",
        "# Construct signal\n",
        "data = amplitude * np.sin(2 * np.pi * f * t)\n",
        "\n",
        "# Write audio file\n",
        "wavfile.write(\"/content/writing_example.wav\", sr, data.astype(np.int16))"
      ],
      "metadata": {
        "id": "cFMXxw2YQjbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Resampling\n",
        "\n",
        "We can always down- and upsample signals. We always want to work with the same sampling rate across all our files. If you use a pretrained model, you are typically required to preprocess your audio to the same sampling rate as the model. Why is that necessary you think?\n",
        "\n",
        "Whenver we want to change the sampling rate, we can *resample* our original audio file to a target rate. However, upsampling does not change the quality of your original audio. It only inter- and extrapolates to accomodate the target."
      ],
      "metadata": {
        "id": "pOlv2fGBRUDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to resample signal\n",
        "def resampling(signal, sr, target_sr):\n",
        "  \"\"\"\n",
        "  Resample an audio signal from the old sample rate to the new sample rate.\n",
        "\n",
        "  Parameters:\n",
        "  signal (numpy.ndarray): The input audio signal to be resampled.\n",
        "  sr (int): The original sample rate (in Hz) of the input signal.\n",
        "  target_sr (int): The target sample rate (in Hz) for the resampled signal.\n",
        "\n",
        "  Returns:\n",
        "  tuple: A tuple containing the new sample rate (target_sr) and the resampled signal (numpy.ndarray).\n",
        "  \"\"\"\n",
        "\n",
        "  # resample ratio\n",
        "  resample_ratio = target_sr / sr\n",
        "\n",
        "  # resample signal\n",
        "  resampled_signal = scipy.signal.resample(signal,\n",
        "                                           int(len(signal) * resample_ratio))\n",
        "\n",
        "  return target_sr, resampled_signal"
      ],
      "metadata": {
        "id": "sb5zZIkqHj9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.0: Naive Resampling\n",
        "\n",
        "We start by using only the `wavefile.write` function to conduct the resampling.\n",
        "\n",
        "1. Read in audio file we also used in *Exercise 1.0*\n",
        "2. Describe the audio in the same way as in *Exercise 1.0*\n",
        "3. Save the audio file using a sampling rate of of $44,100$ Hz by specyfing the the `rate` argument in the `write()` function. Call the file \"*naive_resampling_44100Hz.wav*\". Encode the signal as $16$-bit\n",
        "4. Repeat step 1 and 2 \"*naive_resampling_44100Hz.wav*\"\n",
        "5. Listen to \"*naive_resampling_44100Hz.wav*\" yourself. What's the problem?"
      ],
      "metadata": {
        "id": "eFe__Lg-9CzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution 2.0"
      ],
      "metadata": {
        "id": "JPg0W0n3_T4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Read in original audio file\n",
        "sr, signal = wavfile.read(audio_fpath)\n",
        "\n",
        "# Step 2: Describe the audio\n",
        "audio_info_original = describe_audio(signal=signal, sr=sr)\n",
        "\n",
        "# Step 3: Write audio to 44100 Hz\n",
        "target_sr = 44100\n",
        "wavfile.write(filename=f\"/content/naive_resampling_{target_sr}Hz.wav\",\n",
        "              rate=target_sr,\n",
        "              data=signal.astype(np.int16))"
      ],
      "metadata": {
        "id": "UljI0TljrxXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Read in resampled audio and describe\n",
        "sr, signal = wavfile.read(f\"/content/naive_resampling_{target_sr}Hz.wav\")\n",
        "audio_info_resampled = describe_audio(signal=signal, sr=sr)\n",
        "\n",
        "# Step 5: Manual listening or play from Python"
      ],
      "metadata": {
        "id": "M1MvFkeoATGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.1: Correct Resampling\n",
        "\n",
        "You probably figured the problem by listening to the audio and seeing the different durations between the original and resampled audio files. The goal of resampling is not to change the duration. The duration should fixed. What we want to change is the distance in time between each sample. Note that a sampling rate of $16,000$ Hz corresponds to a sample every $\\frac{1}{16}=0.0625 \\hspace{.1cm}\\text{ms}$ or $\\frac{1}{16000}=6.25e\\text{-}5 \\hspace{.1cm}\\text{s}$ (where $6.25 \\times 10^{-5}=0.0000625$)\n",
        "\n",
        "The trick is to scale the original signal by the ratio of the target rate and the original sampling rate. To conduct the resampling, you can use the `resample()` method from the `scipy.signal` module. Use a target rate of $44,100$ Hz\n",
        "\n",
        "1. Conduct step 1 and 2 from *Exercise 2.0*\n",
        "2. Compute the resample ratio\n",
        "3. Use `scipy.signal.resample` to resample the audio read in step 1 and assign it to an object called `resampled_signal`\n",
        "4. Write the audio to a file called \"*correct_resampling_44100Hz.wav*\". Make sure to encode the audio as $16$-bit\n",
        "5. Do step 1 for \"*correct_resampling_44100Hz.wav*\". Describe the results and compare them to your naive solution.\n",
        "6. Listen to \"*correct_resampling_44100Hz.wav*\" to verify your solution."
      ],
      "metadata": {
        "id": "DhH29Ey4A1c2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution 2.1"
      ],
      "metadata": {
        "id": "6dmHq269GSsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1+2: Read in original audio file and describe\n",
        "sr, signal = wavfile.read(audio_fpath)\n",
        "audio_info_original = describe_audio(signal=signal, sr=sr)\n",
        "\n",
        "# Step 2: Compute sampling ratio\n",
        "target_sr = 44100\n",
        "resample_ratio = target_sr / sr\n",
        "print(f\"Resample signal from {sr} to {target_sr} Hz\")\n",
        "\n",
        "# Step 3: Resample\n",
        "resampled_signal = scipy.signal.resample(signal, int(len(signal) * resample_ratio))"
      ],
      "metadata": {
        "id": "XfgK_wiGYOE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Write resampled signal\n",
        "wavfile.write(filename=f\"correct_resampling_{target_sr}Hz.wav\",\n",
        "              rate=target_sr,\n",
        "              data=resampled_signal.astype(np.int16))\n",
        "\n",
        "# Step 5: Read in resampled audio file and describe\n",
        "sr, resampled_signal = wavfile.read(f\"correct_resampling_{target_sr}Hz.wav\")\n",
        "audio_info_original = describe_audio(signal=resampled_signal, sr=sr)\n",
        "\n",
        "# Step 6: Manual listen"
      ],
      "metadata": {
        "id": "J9FiBo7OFSOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Speech Waveforms\n",
        "\n",
        "So far we have seen dummy waveforms with only one or two frequencies. In reality, audio signals contain a lot frequencies. This is also true for human speech. In the next four exercises, we will plot the speech waveform of four different audio files uttered by two different speakers, a male and a female speaker. Each speaker has two audio files with one being an examplar of a more subdue speaking style (*q10*) and the other being an examplar of an activated speaking style (*q90*)\n",
        "\n",
        "The files are in the *data/audio/class09/* folder:\n",
        "- *speaker0_q10.wav*\n",
        "- *speaker0_q90.wav*\n",
        "- *speaker1_q10.wav*\n",
        "- *speaker1_q90.wav*"
      ],
      "metadata": {
        "id": "xRdL4Xc4emsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.0: Reading and Normalizing Audio Files\n",
        "\n",
        "1. Read in each of the four audio files\n",
        "2. Write a function that normalizes a vector\n",
        "3. Normalize the amplitude of each audio file (*Hint*: you might get an error message when using your normalization function. Type cast the audio array as a float when you normalize to avoid the error)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v2rpRW5qf5jW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to normalize audio signal\n",
        "def normalization(x):\n",
        "  return (x - min(x)) / (max(x) - min(x))"
      ],
      "metadata": {
        "id": "T9l_OqxdhrEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define names of audio files in a list\n",
        "speaker_labels = [0, 1]\n",
        "quantiles = [10, 90]\n",
        "\n",
        "audio_files = []\n",
        "for s in speaker_labels:\n",
        "  for q in quantiles:\n",
        "    audio_files += [f'speaker{str(s)}_q{str(q)}']"
      ],
      "metadata": {
        "id": "eQFhIcaPhNMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path to directory containing audio files\n",
        "base_dir = os.path.join(os.getcwd(), 'css_fall2023/data/audio/class09')\n",
        "\n",
        "# Read in each audio file\n",
        "audio_signals = [wavfile.read(os.path.join(base_dir, f + '.wav')) for f in audio_files]\n",
        "\n",
        "# Unpack each tuple in audio_signals and keep only sampling rate\n",
        "sr, audio_raw_signals = zip(*audio_signals)\n",
        "sr = sr[0]"
      ],
      "metadata": {
        "id": "o-XmO8AshV_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize audio - note that we need to type cast each array to a float to avoid errors\n",
        "audio_normalized_signals = [normalization(a.astype(float)) for a in audio_raw_signals]"
      ],
      "metadata": {
        "id": "HDCaTA7WhtHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.1: Full Speech\n",
        "\n",
        "Plot the entire span of each speech for each audio file in a 2 by 2 grid with 2 rows and 2 columns. The first row should be *speaker0* and the second row should be *speaker1*. Give each speaker a unique color."
      ],
      "metadata": {
        "id": "BMBryNP_hYFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution 3.1"
      ],
      "metadata": {
        "id": "bgfH2Y1HntS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_subplots = len(audio_normalized_signals)\n",
        "num_rows = (num_subplots + 1) // 2\n",
        "num_cols = 2\n",
        "colors = ['#381a61', '#381a61', '#e78429', '#e78429']\n",
        "speakers = ['Claus Hjorth Frederiksen', 'Claus Hjorth Frederiksen', 'Özlem Cekic', 'Özlem Cekic']"
      ],
      "metadata": {
        "id": "ppnBbCbhmI8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = 0\n",
        "plt.figure(figsize=(20, 12))\n",
        "for j in range(num_cols):\n",
        "  for i in range(num_rows):\n",
        "    plt.subplot(num_rows, num_cols, c + 1)\n",
        "    t = np.linspace(0, len(audio_normalized_signals[c])/sr, len(audio_normalized_signals[c]), endpoint=False)\n",
        "    plt.plot(t, audio_normalized_signals[c], color=colors[c], alpha=0.7)\n",
        "\n",
        "    if c > 1:\n",
        "      plt.xlabel('Time (s)', size=16)\n",
        "\n",
        "    if c % 2 == 0:\n",
        "      plt.ylabel('Amplitude', size=16)\n",
        "\n",
        "    plt.title(f\"Speech {i}: {speakers[c]}\", size=14)\n",
        "    plt.grid(True)\n",
        "    c += 1\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UIOnNBIGmLHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.2: Five Seconds of Speech\n",
        "\n",
        "As you can see, it is difficult to extract any meaning from the full speech waveforms. Try to plot a subset of five seconds for each speech. I use start=$20$ and stop=$25$, but feel free to choose any other interval.\n",
        "\n",
        "Once again, plot the waveforms in a 2 by 2 grid with 2 rows and 2 columns. The first row should be *speaker0* and the second row should be *speaker1*. Give each speaker a unique color.\n",
        "\n",
        "Describe the results. Try to listen to the five seconds in each speech manually."
      ],
      "metadata": {
        "id": "10BOaTbQluA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution 3.2"
      ],
      "metadata": {
        "id": "JPlt4H1nnwwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_subplots = len(audio_normalized_signals)\n",
        "num_rows = (num_subplots + 1) // 2\n",
        "num_cols = 2\n",
        "start=20\n",
        "stop=25\n",
        "colors = ['#381a61', '#381a61', '#e78429', '#e78429']\n",
        "speakers = ['Claus Hjorth Frederiksen', 'Claus Hjorth Frederiksen', 'Özlem Cekic', 'Özlem Cekic']"
      ],
      "metadata": {
        "id": "LexYblYei327"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = 0\n",
        "plt.figure(figsize=(20, 12))\n",
        "for j in range(num_cols):\n",
        "  for i in range(num_rows):\n",
        "    plt.subplot(num_rows, num_cols, c + 1)\n",
        "    t = np.linspace(0, len(audio_normalized_signals[c])/sr, len(audio_normalized_signals[c]), endpoint=False)\n",
        "    plt.plot(t[start*sr:int(stop*sr)], audio_normalized_signals[c][start*sr:int(stop*sr)], color=colors[c], alpha=0.7)\n",
        "\n",
        "    if c > 1:\n",
        "      plt.xlabel('Time (s)', size=16)\n",
        "\n",
        "    if c % 2 == 0:\n",
        "      plt.ylabel('Amplitude', size=16)\n",
        "\n",
        "    plt.title(f\"Speech {i}: {speakers[c]}\", size=14)\n",
        "    plt.grid(True)\n",
        "    c += 1\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XUH-pgNXixT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.3: 25 milliseconds (0.025 seconds)\n",
        "\n",
        "This is clearly more informative. We can go even further to see what's going on. Try to plot $25$ ms of each speech in a 2 by 2 grid with 2 rows and 2 columns. The first row should be *speaker0* and the second row should be *speaker1*. Give each speaker a unique color.\n",
        "\n",
        "I use start=$11$ and stop=$11.025$, but you can choose any interval you like."
      ],
      "metadata": {
        "id": "b82GbUWjmcVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution 3.3"
      ],
      "metadata": {
        "id": "HHLsoS7-nyuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_subplots = len(audio_normalized_signals)\n",
        "num_rows = (num_subplots + 1) // 2\n",
        "num_cols = 2\n",
        "start=11\n",
        "stop=11.025\n",
        "colors = ['#381a61', '#381a61', '#e78429', '#e78429']\n",
        "speakers = ['Claus Hjorth Frederiksen', 'Claus Hjorth Frederiksen', 'Özlem Cekic', 'Özlem Cekic']"
      ],
      "metadata": {
        "id": "atUHNaY9mg_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = 0\n",
        "plt.figure(figsize=(20, 12))\n",
        "for j in range(num_cols):\n",
        "  for i in range(num_rows):\n",
        "    plt.subplot(num_rows, num_cols, c + 1)\n",
        "    t = np.linspace(0, len(audio_normalized_signals[c])/sr, len(audio_normalized_signals[c]), endpoint=False)\n",
        "    plt.plot(t[start*sr:int(stop*sr)], audio_normalized_signals[c][start*sr:int(stop*sr)], color=colors[c], alpha=0.7)\n",
        "\n",
        "    if c > 1:\n",
        "      plt.xlabel('Time (s)', size=16)\n",
        "\n",
        "    if c % 2 == 0:\n",
        "      plt.ylabel('Amplitude', size=16)\n",
        "\n",
        "    plt.title(f\"Speech {i}: {speakers[c]}\", size=14)\n",
        "    plt.grid(True)\n",
        "    c += 1\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_iWzcrb9mkFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4 Spectrograms\n",
        "\n",
        "Iterate the exact same procedure as in *Exercise 3* but plot the speeches as spectrograms instead of waveforms. I use the ranges:\n",
        "\n",
        "* Exercise 4.0: Full speech\n",
        "* Exercise 4.1: 5-20 seconds (15 seconds in total)\n",
        "* Exercise 4.2: 11-12.5 seconds (1.5 seconds in total)\n",
        "\n",
        "You decide if you use mel-spectrograms or the standard spectrogram. For the former, you should use the `librosa` module. For the latter, you can use `plt.specgram`."
      ],
      "metadata": {
        "id": "m-uWgeAzoPQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_subplots = len(audio_normalized_signals)\n",
        "num_rows = (num_subplots + 1) // 2\n",
        "num_cols = 2\n",
        "colors = ['viridis', 'viridis', 'inferno', 'inferno']\n",
        "speakers = ['Claus Hjorth Frederiksen', 'Claus Hjorth Frederiksen', 'Özlem Cekic', 'Özlem Cekic']"
      ],
      "metadata": {
        "id": "XvKXJgEwsm0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4.0: Full Speech\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I6dQr5iqGkkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution 4.0"
      ],
      "metadata": {
        "id": "q9Kj-m8bqqiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c = 0\n",
        "plt.figure(figsize=(20, 12))\n",
        "for j in range(num_cols):\n",
        "  for i in range(num_rows):\n",
        "    plt.subplot(num_rows, num_cols, c + 1)\n",
        "    t = np.linspace(0, len(audio_normalized_signals[c])/sr, len(audio_normalized_signals[c]), endpoint=False)\n",
        "    Pxx, freqs, spectimes, cax = plt.specgram(audio_normalized_signals[c],\n",
        "                                              Fs=sr,\n",
        "                                              scale='dB',\n",
        "                                              cmap=colors[c])\n",
        "\n",
        "    if c > 1:\n",
        "      plt.xlabel('Time (s)', size=16)\n",
        "\n",
        "    if c % 2 == 0:\n",
        "      plt.ylabel('Frequency (Hz)', size=16)\n",
        "\n",
        "    plt.title(f\"Speech {i}: {speakers[c]}\", size=14)\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.grid(True)\n",
        "    c += 1\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IyLLDrLyp8f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4.1: 5-20 seconds"
      ],
      "metadata": {
        "id": "9x0OmK5MGs15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution 4.1"
      ],
      "metadata": {
        "id": "YW6d15TDqyQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start=5\n",
        "stop=20\n",
        "c = 0\n",
        "plt.figure(figsize=(20, 12))\n",
        "for j in range(num_cols):\n",
        "  for i in range(num_rows):\n",
        "    plt.subplot(num_rows, num_cols, c + 1)\n",
        "    Pxx, freqs, spectimes, cax = plt.specgram(audio_normalized_signals[c][start*sr:stop*sr],\n",
        "                                              Fs=sr,\n",
        "                                              scale='dB',\n",
        "                                              cmap=colors[c])\n",
        "\n",
        "    if c > 1:\n",
        "      plt.xlabel('Time (s)', size=16)\n",
        "\n",
        "    if c % 2 == 0:\n",
        "      plt.ylabel('Frequency (Hz)', size=16)\n",
        "\n",
        "    plt.title(f\"Speech {i}: {speakers[c]}\", size=14)\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.grid(True)\n",
        "    c += 1\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TsYAPNM0qwQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4.2: 11-12.5 seconds"
      ],
      "metadata": {
        "id": "YkklEZCvGyCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution 4.2"
      ],
      "metadata": {
        "id": "Qsze25T5saQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start=11\n",
        "stop=12.5\n",
        "c = 0\n",
        "plt.figure(figsize=(20, 12))\n",
        "for j in range(num_cols):\n",
        "  for i in range(num_rows):\n",
        "    plt.subplot(num_rows, num_cols, c + 1)\n",
        "    Pxx, freqs, spectimes, cax = plt.specgram(audio_normalized_signals[c][start*sr:int(stop*sr)],\n",
        "                                              Fs=sr,\n",
        "                                              scale='dB',\n",
        "                                              cmap=colors[c])\n",
        "\n",
        "    if c > 1:\n",
        "      plt.xlabel('Time (s)', size=16)\n",
        "\n",
        "    if c % 2 == 0:\n",
        "      plt.ylabel('Frequency (Hz)', size=16)\n",
        "\n",
        "    plt.title(f\"Speech {i}: {speakers[c]}\", size=14)\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.grid(True)\n",
        "    c += 1\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "shaalAROrph5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}