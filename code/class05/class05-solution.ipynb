{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97b3e5b1",
   "metadata": {},
   "source": [
    "# Class 5: ML Lab - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f126617a",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "Partisanship and polarization are important features of democratic politics. Partisanship influences how citizens perceive real‐world conditions. For example, citizens tend to view the state of the national economy more positively if their party holds office. Polarization, on other hand, refers to ideological differences. While often used interchangeably, partisanship and polarization are different concepts, but the former often contains valuable information about the latter. If politics are becoming partisan, it is likely that it is getting more polarized as well. \n",
    "\n",
    "In this lab session, we will investigate whether we can develop computational large-scale measures of both partisanship and polarization. For the former, we use supervised learning. For the latter, we use unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def0c1ef",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before we start, we need to have the correct setup. We start by importing all modules we need in the exercise. we then change the working directory to the project folder you are working with in the class using `os.chdir()` where `os` refers to the inbuilt module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e650ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from string import punctuation\n",
    "from gensim.models import Doc2Vec\n",
    "from spacy.lang.da.stop_words import STOP_WORDS as stopwords\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8752d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory\n",
    "wd = '/home/rask/Dropbox/teaching/css_fall2023'                              # write directory here\n",
    "wd = 'C:/Users/au535365/Dropbox/teaching/css_fall2023'\n",
    "os.chdir(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b66f75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\au535365\\\\Dropbox\\\\teaching\\\\css_fall2023'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm that the working directory is as intended \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f2350a",
   "metadata": {},
   "source": [
    "## 1.0 Supervised Learning: Classification of Partisanship\n",
    "\n",
    "\n",
    "In their 2018 article, Peterson and Spirling (2018) develop and validate a measure of partisanship based on word choices by individual legislators. The logic is simple: The easier it is for a classifier to recognize legislators by their words, the higher is the amount of partisanship. This, in turn, means that polarization is higher. Peterson and Spirling (2018) develop a time-varying measure. We refrain for this in a second and for now consider only static partisanship.\n",
    "\n",
    "\n",
    "Link to article: https://www.cambridge.org/core/journals/political-analysis/article/classification-accuracy-as-a-substantive-quantity-of-interest-measuring-polarization-in-westminster-systems/45746D999CFCD1CB43E362392D7B2FB4\n",
    "\n",
    "We work with speeches from Folketinget from 2000-2021 spanning two decades of parliamentary debates. The data can be found on the GitHub repo under `data/ft-speeches` where each term is saved in a separate `.csv` file, e.g. `20001.csv`. Download the files to your local computer and place the files somewhere in your project folder. Do that before you continue.\n",
    "\n",
    "When you've downloaded the files, we can move on to the first exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23c76b6",
   "metadata": {},
   "source": [
    "### Reading and Cleaning Data\n",
    "\n",
    "The first section of the supervised learning exercise is reading and cleaning data. This the boring step of any empirical project, but is always time-consuming and important to master!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7aadb6",
   "metadata": {},
   "source": [
    "#### Exercise 1.0: Reading in Data I\n",
    "\n",
    "Read in each dataset you downloaded from GitHub. To see all files in a folder, you can write `os.listdir()`, which returns a list of files. Save the output from `os.listdir()` in an object called `files`. \n",
    "\n",
    "If you've done it correct, the list should have a length of $28$. Validate the result.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06888f9c",
   "metadata": {},
   "source": [
    "#### Solution 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970a4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of of files\n",
    "files = os.listdir('data/ft-speeches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b80b4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute length - validate that it is 28\n",
    "len(files) == 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aae5fb",
   "metadata": {},
   "source": [
    "#### Exercise 1.1: Reading in Data II\n",
    "\n",
    "We now use our list `files` to iteratively read in each of the $28$ datasets. This can be done using a for-loop where we loop over each file in the list. \n",
    "\n",
    "*Hints*: Declare an empty dataframe before the loop called called `df`. Within the loop, read in each dataset and concatenate it with the pre-declared dataframe. This can be using the `pd.concat([DF0, DF1])` (`pd` is imported as the namespace for Pandas). Note that `DF0` and `DF1` are just random names. Replace with your own. When all data is loaded, remember to reset indices.\n",
    "\n",
    "If you have done it correct, the length of the dataset will be $411886$. Validate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2f4c9c",
   "metadata": {},
   "source": [
    "#### Solution 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1d25935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 28/28 [00:26<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read in data\n",
    "df = pd.DataFrame()\n",
    "for file in tqdm(files):\n",
    "    df_term = pd.read_csv('data/ft-speeches/' + file)\n",
    "    df = pd.concat([df, df_term])\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50dfd6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute length - validate that it is 411886\n",
    "len(df) == 411886"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fda0c6",
   "metadata": {},
   "source": [
    "#### Exercise 1.2: Data Cleaning I\n",
    "\n",
    "Before you move on, try print the dataframe in various ways to see the content of the data. \n",
    "\n",
    "We start by removing speeches in terms where each bloc has $2000$ or less speeches.  \n",
    "\n",
    "I provide you the code for that. You can try play around with it to see what's going on.\n",
    "\n",
    "When you have executed the code, we want to keep only speeches given by legislators from Socialdemokratiet (S) or Venstre (V). \n",
    "\n",
    "*Hint*: Use `.loc` and a `.isin()` to filter speeches given by legislators from S or V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9162ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = df.groupby(['partycolor', 'period']).count().reset_index()[['partycolor', 'period', 'date']]\n",
    "count_df = count_df.rename(columns={'date': 'count'})\n",
    "\n",
    "df = pd.merge(df, count_df, on=['partycolor', 'period'], how='left')\n",
    "\n",
    "df = df.loc[df['count'] >= 2000,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348eadbc",
   "metadata": {},
   "source": [
    "#### Solution 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16638aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only Socialdemokratiet and Venstre\n",
    "df = df.loc[df['party'].isin(['S', 'V'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac24caa7",
   "metadata": {},
   "source": [
    "#### Exercise 1.3: Data Cleaning II\n",
    "\n",
    "If you haven't done already, remember to reset indices after filtering. \n",
    "\n",
    "The last thing we do is to make a binary indicator showing whether a speech is given by a legislator from S or not. You can achieve this in multiple ways. Remember that the output should be 0-1 column and NOT a 0.0 and 1.0 column. Note that this makes it a binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9713f9d6",
   "metadata": {},
   "source": [
    "#### Solution 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3a25d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make binary indicator\n",
    "df['sd'] = df['party'] == 'S'\n",
    "df['y_binary'] = [int(x) for x in df['sd']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdbac74",
   "metadata": {},
   "source": [
    "### Static Partisanship\n",
    "\n",
    "We now move on to the classification tasks. \n",
    "\n",
    "At this point in the course, we have not talked about how we numerically represent text. Therefore, I provide you with code to do that now using the `CountVectorizer` class from `sklearn` (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). By next class, you will understand what we're doing. I assume that you have called your dataset `df`. If now, go back and change it or replace `df` with your naming of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2037e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this code - the df['text'] refer to the text column in the dataframe\n",
    "vectorizer = CountVectorizer(decode_error='ignore', min_df = 50, max_df=0.10)\n",
    "vectorizer.fit(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114eb8eb",
   "metadata": {},
   "source": [
    "#### Exercise 1.4: Intuition\n",
    "\n",
    "Before we continue, describe in your own words: \n",
    "\n",
    "1) Which assumptions we necessarily must make to assume that partisanship is conveyed in word choices\n",
    "\n",
    "2) What it implies for the clarity of a voter's vote choice on election day. \n",
    "\n",
    "3) Do need high accuracy for us to get a good model for this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5886d68",
   "metadata": {},
   "source": [
    "#### Solution 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084c4e43",
   "metadata": {},
   "source": [
    "#### Exercise 1.5: Test Set and Model Selection\n",
    "\n",
    "To train a model to classify partisanship based on words, we must split our data into train and test sets to avoid bias in the model selection. This is the standard procedure for all machine learning tasks where we train a model from scratch (and also when fine-tuning a model from transfer learning. \n",
    "\n",
    "Why is the train-test necessary to obtain an unbiased estimate of the generalization error?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4572fd",
   "metadata": {},
   "source": [
    "#### Solution 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff80fe9",
   "metadata": {},
   "source": [
    "#### Exercise 1.6: Data Splitting\n",
    "\n",
    "Use the `train_test_split()` function from the `sklearn` module (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). It is already imported in the Setup section. Set `test_size=0.2` and `shuffle=True`. What does it mean to have a test_size of $0.2$ and why is it necessary to shuffle the data?\n",
    "\n",
    "Save it as `X_train`, `X_test`, `y_train`, and `y_test`. Inspect the content of the four sets. Is the length of `X_train` larger than `X_test`? Are `X_train` and `y_train` of equal length? What about `X_test` and `y_test`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9303986a",
   "metadata": {},
   "source": [
    "#### Solution 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33055dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['y_binary'], test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f85cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if length is larger for X_train than X_test\n",
    "len(X_train) > len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95972cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train) == len(y_train), len(X_test) == len(y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c3214",
   "metadata": {},
   "source": [
    "#### Exercise 1.7: Data Formatting\n",
    "\n",
    "If you check the `type()` of these four sets, you will see that they are `pandas.core.series.Series`. This is not universal, but happens since our data is stored in dataframes. \n",
    "\n",
    "Typically, however, scikit-learn wants the input data in numpy format. This is not a general rule, but I always use arrays or eventually lists to make the workflow the same every time.\n",
    "\n",
    "Convert the `y_train` and `y_test` to numpy arrays. This can be done very simply using the `np.array()` method.\n",
    "\n",
    "It is more tricky for `X_train` and `X_test` since we need to use the `vectorizer` we defined earlier based on the `CountVectorizer` class. Without further ado, execute the code. We will come back to the meaning of it in class 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2146a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply vectorizer on input features and convert to numpy arrays\n",
    "X_train = vectorizer.transform(X_train).toarray()\n",
    "X_test = vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3bee4",
   "metadata": {},
   "source": [
    "#### Solution 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d65395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numpy arrays\n",
    "y_train, y_test = np.array(y_train), np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acaf2da",
   "metadata": {},
   "source": [
    "#### Exercise 1.8: Random Sampling\n",
    "\n",
    "Sometimes we do not have the time to train a model on all our dataset. An effective work-around is to randomy sample a subset of your training and test set to reduce training time. This approach can even be extended to generate bootstrapping estimates (i.e. nonparametric uncertainty estimates) if we sample with replacement $N$ times. We do not implement the bootstrap here, but if you feel for it, go nuts!\n",
    "\n",
    "Your task is to randomly select a subset of samples from the training and test sets, respectively. We can achieve this with the `np.random.choice` method from the numpy module. We have already imported numpy as np. Set a seed like this `np.random.seed(10)` such that our results can be replicated. \n",
    "\n",
    "When you have done so, you can randomly select indices like this `np.random.choice(a, size=None, replace=False)` where `a` is a $1$d-array with the elements to be sampled and where `size` is the sample size. For this, we use $20000$ for the training set and $4000$ for the test set, which preserves the $0.2$ split. Think carefully about what you give to the parameter `a`. We need to select the *same* indices in the both `X_train` and `y_train` and the same for `X_test` and `y_test`. Save the results in two objects called `train_indices` and `test_indices` respectively.\n",
    "\n",
    "After randomly selecting $20000$ and $4000$ samples, use `train_indices` and `test_indices` to subset the training and test sets. Call the sets:\n",
    "* `X_train_subset`\n",
    "* `y_train_subset`\n",
    "* `X_test_subset`\n",
    "* `y_test_subset`\n",
    "\n",
    "Validate that the size of the resulting sets are $20000$ for training and $4000$ for test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e56c19f",
   "metadata": {},
   "source": [
    "#### Solution 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b93b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "N_train, N_test = 20000, 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ixs = np.random.choice(len(y_train), size=N_train, replace=False)\n",
    "test_ixs = np.random.choice(len(y_test), size=N_test, replace=False)\n",
    "\n",
    "y_train_subset, y_test_subset = y_train[train_ixs], y_test[test_ixs]\n",
    "X_train_subset, X_test_subset = X_train[train_ixs], X_test[test_ixs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d09ed4",
   "metadata": {},
   "source": [
    "#### Exercise 1.9: Logistic Regression\n",
    "\n",
    "We are now ready to train our first model. We use the logistic regression, which is actually a classifier and not a regression model. Since our input features are fairly large, we do not have the time and computer power to fine-tune eventual hyperparameters here. \n",
    "\n",
    "Start by declaring the logistic regression model using the model from `sklearn` (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Go through the documentation to see which options you have. For this, exercise we simply rely on the default options, but set `tol=1e-1` ($1e-1 = 0.01$). Note that the model is imported as `from sklearn.linear_model import LogisticRegression`\n",
    "\n",
    "Now we do:\n",
    "1) Initiate the model to an object called `logistic_clf`\n",
    "2) Fit the data to the model using the `.fit()` model where we give as input our training data. Note that this step might take 10-15 minutes. \n",
    "3) Predict on the test set using the `.predict()` method. Save the predictions to an object called `log_preds`\n",
    "4) Print the classification report using the `classification_report()` function which we have imported as `from sklearn.metrics import classification_report`. As input, it takes the `log_preds` and the labels for the test set.\n",
    "5) Describe the results. What is overall accuracy? What is the precision and recall for the two labels? Are their any differences?\n",
    "6) Interpret the results substantially. What does it mean that we are able/not able to classify partisanship?\n",
    "\n",
    "**NOTE**: Remember to fit the model using the subset sets!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f15d99",
   "metadata": {},
   "source": [
    "#### Solution 1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f281ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Initiate logistic classifier\n",
    "n_cores = multiprocessing.cpu_count()                                \n",
    "logistic_clf = LogisticRegression(solver='sag', \n",
    "                                  n_jobs=n_cores, \n",
    "                                  max_iter=150,\n",
    "                                  tol=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec1b456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Fit the model\n",
    "start_time = time.time()\n",
    "logistic_clf.fit(X_train_subset, y_train_subset)\n",
    "end_time = time.time()\n",
    "print(f\"Logistic classifier fitted in {end_time - start_time} seconds ({round((end_time - start_time) / 60, 3)} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Predict on test set\n",
    "log_preds = logistic_clf.predict(X_test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2691258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4 Classification report\n",
    "print(classification_report(log_preds, y_test_subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1936a8d",
   "metadata": {},
   "source": [
    "5 Describe the results ...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35e79ec",
   "metadata": {},
   "source": [
    "6 Interpret the results substantially ...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08dd14f",
   "metadata": {},
   "source": [
    "#### Exercise 1.10: Naive Bayes\n",
    "\n",
    "We see that the logistic regression performs fairly well. Without much work, we are able to accurately classify speeches as given by legislators from S or V. However, we are not interested in the accuracy in itself. That is, even if we could not classify partisanship based on speeches, our results would still be interesting. This is a huge difference compared to how computer scientists' use machine learning.\n",
    "\n",
    "We now check whether our results are robust to our choice of algorithm or whether other methods work better. To compare the results, we use the simple but powerful Naive Bayes (NB) method There are multiple variants of NB methods, but they are all a family of supervissed learning algorithms, which use the famous Bayes' theorem assuming conditional independence between every feature in our input data (given the label/class). This is obviously an unrealistic assumption. Words are not chosen randomly when we condition on whether the speech is from a S or V legislator. Still, NB algorithms are powerful baselines to compare results against.\n",
    "\n",
    "Start by declaring the multinominal NB model using sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html). Go through the documentation to see which options you have. For this, exercise we simply rely on the default options. Note that the model is imported as `from sklearn.naive_bayes import MultinomialNB`\n",
    "\n",
    "\n",
    "\n",
    "Now we do:\n",
    "1) Initiate the model to an object called `naive_clf`\n",
    "2) Fit the data to the model using the `.fit()` model where we give as input our training data. This takes a while, but should be faster than for logistic classifier. \n",
    "3) Predict on the test set using the `.predict()` method. Save the predictions to an object called `naive_preds`. To the get probabilities, you can use the `.predict_proba()` method. Do that as well and save it to `naive_probs`\n",
    "4) Print the classification report using the `classification_report()` function which we have imported as `from sklearn.metrics import classification_report`. As input, it takes the `naive_preds` and the labels for the test set.\n",
    "5) Describe the results. What is overall accuracy? What is the precision and recall for the two labels? Are their any differences? How they compare to the results from the logistic classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc5915",
   "metadata": {},
   "source": [
    "#### Solution 1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f566ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Initiate model\n",
    "naive_clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Fit model\n",
    "start_time = time.time()\n",
    "naive_clf.fit(X_train_subset, y_train_subset)\n",
    "end_time = time.time()\n",
    "print(f\"Naive classifier fitted in {end_time - start_time} seconds ({round((end_time - start_time) / 60, 3)} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd27a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Predictions and probabilities\n",
    "naive_preds = naive_clf.predict(X_test_subset)\n",
    "naive_probs = naive_clf.predict_proba(X_test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Classification report\n",
    "print(classification_report(naive_preds, y_test_subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329008f8",
   "metadata": {},
   "source": [
    "5 Describe the results ...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a71fc4",
   "metadata": {},
   "source": [
    "### Dynamic Partisanship\n",
    "\n",
    "Peterson and Spirling (2018) do not study static partisanship, but how it varies over time, higher accuracy as evidence or more polarization and lower accuracy as the latter. \n",
    "\n",
    "They test their approach in the UK House of Commons comparing yearly classification accuracy of a labour vs. conservative classifier. We now test this approach in the context of the Danish parliament and the historical big left and right-wing parties: Socialdemokratiet (S) and Venstre (V). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b963189f",
   "metadata": {},
   "source": [
    "#### Exercise 1.11: Data Formatting\n",
    "\n",
    "Since we want to investigate how classification accuracy varies over time, we need to divide our speeches into different periods. For this, we use the parliamentary sessions, which run from October in year $t$ to June in year $t+1$ --- unless when elections interrupt the term.\n",
    "\n",
    "The first trick is to make sure we have the same amount of input features in each term. For this, I define a `CountVectorizer()` below using a fixed vocabulary. Download the `vocab_min200.pkl` from the GitHub repo and execute the code without further ado. Once again --- don't worry about the code, we'll get back to it next week.\n",
    "\n",
    "Now, we want construct training and test sets again, but this time we do it for each parliamentary term. The column `period` denotes the term for a given speech. To prepare the data for this task, we want to transform the text based on the `CountVectorizer()`'s fixed vocabuluary for each tern.\n",
    "\n",
    "Do the following:\n",
    "1) Declare empty dictionaries called `X_dict` and `y_dict`\n",
    "2) Loop over each parliamentary term (you can define a list beforehand and then loop over it. Remeber to sort the list!!!)\n",
    "3) Within each iteration, subset the dataframe to the given term (remember to reset indices). Call the subset `df_term`\n",
    "4) Make an object called `y` based on the `y_binary` column in `df_term` \n",
    "5) Check whether there is any variance in the object `y`, and whether there any rows in `df_term`. If not, use the `continue` argument\n",
    "6) Apply the `vectorizer` object defined by the fixed vocab in the `CountVectorizer` object to the `text` column in `df_term`. Save the result to an object called `X`\n",
    "7) Store `X` and `y` in the `X_dict` and `y_dict` respectively. As keywords, use the parliamentary term like this `X_dict[str(term)] = X` and the same for `y` (note that we need `str(term)` since `term` is an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119774cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_vocab = pickle.load(open('data/vocab_min200.pkl', 'rb'))\n",
    "vectorizer = CountVectorizer(decode_error='ignore', vocabulary=fixed_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e37a4",
   "metadata": {},
   "source": [
    "#### Solution 1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e044e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of parliamentary terms\n",
    "parlterms = sorted(df['period'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c3a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dict, y_dict = {}, {}\n",
    "\n",
    "for term in parlterms:\n",
    "    \n",
    "    df_term = df.loc[df['period'] == term].reset_index(drop=True)\n",
    "    y = df_term['y_binary']\n",
    "    \n",
    "    if (np.mean(y)==0 or np.mean(y)==1 or len(df_term)==0):\n",
    "        continue\n",
    "    \n",
    "    X = vectorizer.fit_transform(df_term['text'])\n",
    "    \n",
    "    X_dict[str(term)] = X\n",
    "    y_dict[str(term)] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b955e6c",
   "metadata": {},
   "source": [
    "#### Exercise 1.12 Stratified k-fold Cross Validation\n",
    "\n",
    "Now that we have our data processed and saved in `X_dict` and `y_dict`, we can move on to the classification. Since we train a classifier on speeches on each term, our dataset is fairly small. To tackle this, we average \n",
    "the accuracy over a stratified 10-fold cross-validation. The stratified version of cross-validation makes sure that the balance between the labels are equal in each fold. Start by defining the straified k fold based on sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html). Use the specifications:\n",
    "* `n_splits=10`\n",
    "* `shuffle=True`\n",
    "* `random_state=10`\n",
    "\n",
    "and assign the object to `skf`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0ae81c",
   "metadata": {},
   "source": [
    "#### Solution 1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f4837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db06e715",
   "metadata": {},
   "source": [
    "#### Exercise 1.13: Fitting and Prediction with Logistic Regression \n",
    "\n",
    "Our data for each term is stored in `X_dict` and `y_dict` and we have defined a $10$-fold stratified cross validation scheme. Train a logistic classifier for each parliamentary term and compute the accuracy for each of the $10$ folds. I have provided you with a function that you can use to fit and predict partisanship for each term.\n",
    "\n",
    "You should loop over each term, access the data in the `X_dict` and `y_dict`, and then loop over each fold. This can be done using like this `for train_ix, test_ix in skf.split(X, y)` where `skf` is the object you made in the previous exercise.\n",
    "\n",
    "I don't provide you any more information here. See if you can figure it out yourself. You should end up with an object, for instance a dictionary, with each term having 10 accuracy estimates, one for each fold. \n",
    "\n",
    "**Note:** This task is difficult and requires you to master Python. Check the solution if you need help or ask the student next to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b813b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_classifier(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    logistic_clf = LogisticRegression(solver='sag', n_jobs=-1 ,tol=1e-1, C=1.e4 / 10000)        \n",
    "    logistic_clf.fit(X_train, y_train)\n",
    "    \n",
    "    return logistic_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204c5781",
   "metadata": {},
   "source": [
    "#### Solution 1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf8ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_stats = {}\n",
    "for term in tqdm(parlterms):\n",
    "    X, y = X_dict[str(term)], y_dict[str(term)]\n",
    "    cls_stats = {}\n",
    "    foldid = 0\n",
    "    for train_ix, test_ix in skf.split(X, y):\n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        cls_stats[f\"k-{foldid}\"] = logistic_classifier(X_train, y_train, X_test, y_test)\n",
    "        foldid += 1\n",
    "    term_stats[str(term)] = cls_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e33fa5",
   "metadata": {},
   "source": [
    "#### Exercise 1.14: Average Accuracy \n",
    "\n",
    "You should have a dictionary with keys being each term and then $10$ accuracy estimates for each fold. Compute the average accuray for each term. \n",
    "\n",
    "*Hint*: Use a dictionary comprehension approach using the `np.mean()` method on the values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034187ed",
   "metadata": {},
   "source": [
    "#### Solution 1.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5837a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dict = {str(key): np.mean(list(val.values())) for key, val in term_stats.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e72684",
   "metadata": {},
   "source": [
    "#### Exercise 1.15 Combine Into a Dataframe\n",
    "\n",
    "Now you should have a dictionary with terms as the keys and the average accuracy as the values. Combine this into a pandas dataframe called `stats_df`. You might need to transpose the data to get a proper representation. This can be done using `.transpose()` method.\n",
    "\n",
    "When you have done this, do the following:\n",
    "1) Rename the column called `0` to `mean`\n",
    "2) Assign the row indices as a column of its own to a column called `term` (can be done using `.index`)\n",
    "3) Reset indices\n",
    "\n",
    "You should now have a dataframe with 23 rows and two columns, `mean` and `term`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76659cc1",
   "metadata": {},
   "source": [
    "#### Solution 1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c5a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(stats_dict, index=[0]).transpose()\n",
    "stats_df = stats_df.rename(columns={0: 'mean'})\n",
    "stats_df['term'] = stats_df.index\n",
    "stats_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b060c2b",
   "metadata": {},
   "source": [
    "#### Exercise 1.16: Inspect and Interpret\n",
    "\n",
    "Inspect the results. Can we see any interesting variation over time? Does it match our intuitive understanding of Danish politics? Maybe, maybe not...\n",
    "\n",
    "I have provided you with code to plot the results over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e7d430",
   "metadata": {},
   "source": [
    "#### Solution 1.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e5a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the mean accuracy over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(stats_df['term'], stats_df['mean'], marker='o', linestyle='-', color='b')\n",
    "plt.title('Accuracy of Classifier For Each Term', size=16)\n",
    "plt.xlabel('Term', size=14)\n",
    "plt.ylabel('Accuracy', size=14)\n",
    "plt.xticks(rotation=45, size=10)\n",
    "plt.ylim(0.6, 0.8)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8379776",
   "metadata": {},
   "source": [
    "## 2.0 Unsupervised Learning: Principal Component Analysis (PCA)\n",
    "\n",
    "We now want to explore how we can use PCA to identify positions of political parties in a two-dimensional latent space. \n",
    "\n",
    "It is a common thing in political science to think of parties in spatial terms. The classical left-right distinction is an example of this. This typically refers to the positions in an economic sense. Another dimension is values (i.e. værdipolitik), commonly referred to as the cultural dimension. We investigate this two-dimensional space in the context of Denmark.\n",
    "\n",
    "To explore this, we are working with word embeddings trained on a corpus of parliamentary speeches in the Danish parliament from 2000-2021. We will get to word embeddings, what they are, what they tell us, and so on later in the course. \n",
    "\n",
    "In brief, a word embedding is an $M$ dimensional vector where $M$ is typically large $(128, 256, 300, 512, \\dots)$, and sometimes higher. Each word has an embedding which encodes the semantics of the word. The basic idea is that words that are similar are close to each other and word that are dissimilar are distant from each other. The embeddings are computed using a neural network called `Doc2Vec` (https://radimrehurek.com/gensim/models/doc2vec.html), which enables to include \"covariates\" into the estimation such as party indicators or year indicators. This results in one embedding for each \"covariate\". We will exploit this in the exercise. \n",
    "\n",
    "Our task is to figure out whether `Doc2Vec` outputs partisan embeddings that can be used to accurately locate parties in a two-dimensional space using PCA. We will try two different approaches. One where we investigate whether the variation happens at the bloc-level (left or right) and one where we investigate whether it happens at the party-level. \n",
    "\n",
    "The two models we will work with can be downloaded from the GitHub repo:\n",
    "* Bloc-term: d2v_bloc-term_size300_window20_epochs10_count50.pkl\n",
    "* Party: d2v_party_size200_window20_epochs5_count50.pkl\n",
    "\n",
    "The first uses bloc-term indicators (e.g. 'blue-20081') where the latter uses party indicators (e.g. 'RV-party' or 'DF-party'). \n",
    "\n",
    "Before the actual exercise, we will load in the first model and get a bit familiar with the model. We have already imported `Doc2Vec` from the `gensim` module (https://radimrehurek.com/gensim/models/doc2vec.html) in the top of the notebook. A trained model can be loaded by the `Doc2Vec.load()` method. I do this below and save it to an object called `d2v`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e6367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model name\n",
    "model_name = 'd2v_bloc-term_size300_window20_epochs10_count50.pkl'\n",
    "d2v = Doc2Vec.load('models/' + model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3228b",
   "metadata": {},
   "source": [
    "The `d2v` object inherits from `gensim.models.doc2vec.Doc2Vec`, which make it a module-specific data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563985cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of the data\n",
    "type(d2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac174e3d",
   "metadata": {},
   "source": [
    "To see the vocabulary you can write `d2v.wv.vocab` which returns a dictionary with key-value pairs of words (keys) and embeddings (values). Try print the vocabulary before you move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408f020",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d3380f",
   "metadata": {},
   "source": [
    "Each word embedding can be accessed by `d2v.wv[WORD]` where `WORD` for instance is 'indvandrer'. Try to access the embeddding of the word 'indvandrer'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5b609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the embedding for the word 'indvandrer'\n",
    "d2v.wv['indvandrer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2154208b",
   "metadata": {},
   "source": [
    "The output is a numpy array with dimension `(300, )`. Verify it using the `.shape method`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff31d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "d2v.wv['indvandrer'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd05ce7",
   "metadata": {},
   "source": [
    "All embeddings are of dimension `(300, )`. To get this, you can write `d2v.vector_size`. This is not a global solution, but varies from model to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b31691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dimensionality of the embeddings and save it as M\n",
    "M = d2v.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04189a79",
   "metadata": {},
   "source": [
    "As mentioned, the `Doc2Vec` permits including \"covariates\", which is party-term indicators in this case. We can see the list of indicators used to fit the model using `d2v.docvecs.offset2doctag`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8972e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check indicators used to fit the model\n",
    "d2v.docvecs.offset2doctag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826fbcd",
   "metadata": {},
   "source": [
    "This returns a list of indicators. To access the embedding associated with each indicator, we can write `d2v.docvecs[INDICATOR]` where `INDICATOR` is the name of the indicator e.g. 'blue-20081'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3bf4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v.docvecs['blue-20081']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f6c09",
   "metadata": {},
   "source": [
    "### Bloc-Term Embeddings\n",
    "\n",
    "We start by considering bloc-term embeddings using the model we have already loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e9ff6",
   "metadata": {},
   "source": [
    "#### Exercise 2.0: Generate List with Indicators \n",
    "\n",
    "Generate two lists with bloc-term indicators using list comprehensions. \n",
    "\n",
    "Call the first list `leftwing` which contains all indicators that contains `red` in the list returned by `d2v_model.docvecs.offset2doctag`\n",
    "\n",
    "Call the second list `rightwing` which contains all indicators that contains `blue` in the list returned by `d2v_model.docvecs.offset2doctag`\n",
    "\n",
    "*Hints:* You can check if a string starts with 'r' or 'b' using the method `.startswith()`. There are other possible solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943fd30b",
   "metadata": {},
   "source": [
    "#### Solution 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337a974",
   "metadata": {},
   "outputs": [],
   "source": [
    "leftwing = [d for d in d2v.docvecs.offset2doctag if d.startswith('r')]\n",
    "rightwing = [d for d in d2v.docvecs.offset2doctag if d.startswith('b')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a460cfb0",
   "metadata": {},
   "source": [
    "#### Exercise 2.1: Combining Lists\n",
    "\n",
    "Now that we have the two lists `leftwing` and `rightwing`, we want to append them together. Store the result in a list called `pt_list` which is short for **p**arty **t**erm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf785ea",
   "metadata": {},
   "source": [
    "#### Solution 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd5004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_list = leftwing + rightwing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123622e5",
   "metadata": {},
   "source": [
    "#### Exercise 2.2: Defining an Array\n",
    "\n",
    "We are interested in the total of $46$ bloc-term embeddings. Hence, we want to extract them and save them in a new matrix of dimension `(46, 300)` where $46$ refers to the number of bloc-term embeddings and $300$ to the dimensions of the embeddings. \n",
    "\n",
    "Generate an empty numpy array with dimension `(X, M)` where `X` is the number of party-term indicators and `M` is the size of the embeddings. Call the array `z`. Verify the shape of the array.\n",
    "\n",
    "*Hints*: Use the `np.ones` or `np.zeros` methods to construct the array. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77444180",
   "metadata": {},
   "source": [
    "#### Solution 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31054377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the (X, M) array\n",
    "z = np.zeros((len(pt_list), M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45376e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the shape is as intended\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af5ecf",
   "metadata": {},
   "source": [
    "#### Exercise 2.3: Populating `z` \n",
    "\n",
    "Now that we've created the empty matrix `z`, we want to extract the bloc-term embeddings and assign it to each row in `z`. \n",
    "\n",
    "Recall that you can access the embeddings like this `d2v.docvecs['blue-20081']` \n",
    "\n",
    "*Hints:* Loop over each indicator in `pt_list`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f93f3",
   "metadata": {},
   "source": [
    "#### Solution 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2521265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the list pt_list and assign each party-term embedding to a row in z\n",
    "for i in range(len(pt_list)):\n",
    "    z[i,:] = d2v.docvecs[pt_list[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf85af",
   "metadata": {},
   "source": [
    "#### Exercise 2.4: PCA\n",
    "\n",
    "We want to specify a PCA with two components, `n_components = 2`. \n",
    "\n",
    "Note that the PCA is already imported as `from sklearn.decomposition import PCA`. Inititate the model as `PCA(n_components=2)` and assign to an object called `pca_model`.\n",
    "\n",
    "Finally, you the `.fit_transform()` method from `pca_model` on the matrix `z`. Save the results to a new matrix called `Z`. \n",
    "\n",
    "Verify that your output `Z` is as intended. It should have a dimension of `(46, 2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fd5d6f",
   "metadata": {},
   "source": [
    "#### Solution 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2398453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca_model = PCA(n_components=2)\n",
    "Z = pca_model.fit_transform(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622fcd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify shape \n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1cae96",
   "metadata": {},
   "source": [
    "#### Exercise 2.5: Explained Variance\n",
    "\n",
    "Recall that PCA finds the principal components by maximizing variance. PC1 is projected onto an axis that explains most of the variance in the data. PC2 is then projected onto another axis that explains most of the leftover variation when we already accounted for PC1. \n",
    "\n",
    "To see the amount of explained variance, you can write `pca_model.explained_variance_ratio_`. How much variance is explained by PC1 and PC2 respectively? How much variation is captured in total by PC1 and PC2?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab00c4e",
   "metadata": {},
   "source": [
    "#### Solution 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca_model.explained_variance_ratio_\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffeaad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total amount of explained variance\n",
    "sum(explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e503c47",
   "metadata": {},
   "source": [
    "#### Exercise 2.6: Plot PCA\n",
    "\n",
    "To plot the PCs I have created a function that you can straightaway. Before using it, I convert `Z` to a dataframe and define a color scale to properly label the PCs. \n",
    "\n",
    "Try and place around with the code if you don't understand what's going on.\n",
    "\n",
    "Interpret and describe the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a900cf",
   "metadata": {},
   "source": [
    "#### Solution 2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2355e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Z to a dataframe\n",
    "Z_df = pd.DataFrame(Z)\n",
    "Z_df.columns = ['PC1', 'PC2']\n",
    "Z_df['party_label'] = pt_list\n",
    "\n",
    "# Define color scale\n",
    "color_scale = {'blue': '#3333FF', 'red': '#E91D0E'}\n",
    "\n",
    "cols = [color_scale['red']]*len(leftwing) + [color_scale['blue']]*len(rightwing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9213c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot PCs\n",
    "def plot_pca(dataframe, indicators, cmap, show=True):\n",
    "    \n",
    "    mpl.rcParams['axes.titlesize'] = 20\n",
    "    mpl.rcParams['axes.labelsize'] = 20\n",
    "    mpl.rcParams['font.size'] = 14\n",
    "\n",
    "    plt.figure(figsize=(22,15))\n",
    "    plt.scatter(dataframe.PC1, dataframe.PC2, color=cmap)\n",
    "    texts=[]\n",
    "    for label, x, y, c in zip(indicators, dataframe.PC1, dataframe.PC2, cmap):\n",
    "        plt.annotate(\n",
    "            label,\n",
    "            xy=(x, y), xytext=(-20, 20),\n",
    "            textcoords='offset points', ha='right', va='bottom',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', fc=c, alpha=0.3),\n",
    "            arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c75d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plot_pca(dataframe=Z_df, indicators=pt_list, cmap=cols, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1919628",
   "metadata": {},
   "source": [
    "#### Exercise 2.7: Interpret PCA\n",
    "\n",
    "We can also investigate which words that drives the placement of each bloc-term by looking at the positive and negative similarities for each pole (left-right, north-south). In have written a Python class for this purpose below. Define the class `PCA_INTERPRET` and describe the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9fba77",
   "metadata": {},
   "source": [
    "#### Solution 2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d46b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA_INTERPRET(object):\n",
    "    \n",
    "    def __init__(self, model, parties, dr, Z, labels, rev1=False, rev2=False, min_count=100, max_count = 1000000, max_features=10000):\n",
    "\n",
    "        self.model = model\n",
    "        self.parties = parties\n",
    "        self.labels = labels\n",
    "        self.P = len(self.parties)\n",
    "        self.M = self.model.vector_size   \n",
    "        self.voc = self.sorted_vocab(min_count, max_count, max_features)\n",
    "        self.V = len(self.voc)   \n",
    "        self.pca = dr\n",
    "        self.max = Z.max(axis=0)\n",
    "        self.min = Z.min(axis=0)\n",
    "        self.sims = self.compute_sims()\n",
    "        self.dim1 = rev1\n",
    "        self.dim2 = rev2\n",
    "        \n",
    "    def sorted_vocab(self, min_count=100, max_count=10000, max_features=10000):\n",
    "        wordlist=[]\n",
    "        for word, vocab_obj in self.model.wv.vocab.items():\n",
    "            wordlist.append((word, vocab_obj.count))\n",
    "        wordlist = sorted(wordlist, key=lambda tup: tup[1], reverse=True)\n",
    "        return [w for w,c in wordlist if c>min_count and c<max_count and w.count('_')<3][0:max_features]\n",
    "    \n",
    "    def compute_sims(self):\n",
    "\n",
    "        Z = np.zeros((self.V, 2))\n",
    "        for idx, w in enumerate(self.voc):\n",
    "            Z[idx, :] = self.pca.transform(self.model.wv[w].reshape(1,-1))\n",
    "        sims_right = euclidean_distances(Z, np.array([self.max[0],0]).reshape(1, -1))\n",
    "        sims_left = euclidean_distances(Z, np.array([self.min[0],0]).reshape(1, -1))\n",
    "        sims_up = euclidean_distances(Z, np.array([0,self.max[1]]).reshape(1, -1))\n",
    "        sims_down = euclidean_distances(Z, np.array([0,self.min[1]]).reshape(1, -1))\n",
    "        temp = pd.DataFrame({'word': self.voc, 'right': sims_right[:,0], 'left': sims_left[:,0], 'up': sims_up[:,0], 'down': sims_down[:,0]})\n",
    "        return temp\n",
    "\n",
    "    def top_words_list(self, topn=20):\n",
    "\n",
    "        if self.dim1:\n",
    "            ordering = ['left','right']\n",
    "        else:\n",
    "            ordering = ['right', 'left']\n",
    "        temp = self.sims.sort_values(by=ordering[0])\n",
    "        print(80*\"-\")\n",
    "        print(\"Words Associated with Positive Values (Right) on First Component:\")\n",
    "        print(80*\"-\")\n",
    "        self.top_positive_dim1 = temp.word.tolist()[0:topn]\n",
    "        self.top_positive_dim1 = ', '.join([w.replace('_',' ') for w in self.top_positive_dim1])\n",
    "        print(self.top_positive_dim1)\n",
    "        temp = self.sims.sort_values(by=ordering[1])\n",
    "        print(80*\"-\")\n",
    "        print(\"Words Associated with Negative Values (Left) on First Component:\")\n",
    "        print(80*\"-\")\n",
    "        self.top_negative_dim1 = temp.word.tolist()[0:topn]\n",
    "        self.top_negative_dim1 = ', '.join([w.replace('_',' ') for w in self.top_negative_dim1])\n",
    "        print(self.top_negative_dim1)\n",
    "\n",
    "        if self.dim2:\n",
    "            ordering = ['down','up']\n",
    "        else:\n",
    "            ordering = ['up', 'down']\n",
    "        temp = self.sims.sort_values(by=ordering[0])\n",
    "        print(80*\"-\")\n",
    "        print(\"Words Associated with Positive Values (North) on Second Component:\")\n",
    "        print(80*\"-\")\n",
    "        self.top_positive_dim2 = temp.word.tolist()[0:topn]\n",
    "        self.top_positive_dim2 = ', '.join([w.replace('_',' ') for w in self.top_positive_dim2])\n",
    "        print(self.top_positive_dim2)\n",
    "        temp = self.sims.sort_values(by=ordering[1])\n",
    "        print(80*\"-\")\n",
    "        print(\"Words Associated with Negative Values (South) on Second Component:\")\n",
    "        print(80*\"-\")\n",
    "        self.top_negative_dim2 = temp.word.tolist()[0:topn]\n",
    "        self.top_negative_dim2 = ', '.join([w.replace('_',' ') for w in self.top_negative_dim2])\n",
    "        print(self.top_negative_dim2)\n",
    "        print(80*\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e6da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply class\n",
    "PCA_INTERPRET(d2v, pt_list, pca_model, Z, pt_list, rev1=False, rev2=False, min_count=100, max_count = 1000000, max_features = 50000).top_words_list(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abca731b",
   "metadata": {},
   "source": [
    "### Party Embeddings\n",
    "\n",
    "We now investigate whether we can locate positions of individual parties and not just at the bloc level. \n",
    "\n",
    "Load the second model *d2v_party_size200_window20_epochs5_count50.pkl*. \n",
    "\n",
    "Note that this model has embeddings of size $200$ and not $300$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'd2v_party_size200_window20_epochs5_count50.pkl'\n",
    "d2v = Doc2Vec.load('models/' + model_name)\n",
    "M = d2v.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8043d79b",
   "metadata": {},
   "source": [
    "#### Exercise 2.8: Generate List with Indicators\n",
    "\n",
    "As for the bloc-term embeddings, we need to generate lists with the indicators used to fit the model. \n",
    "\n",
    "Recall that the indicators can be accessed with `d2v.docvecs.offset2doctag`. This time, it is a little more complicated since we have individual parties rather than blocs since we need to do it for each party. Extract the indcator for each party and save it in an object with the name of the party (e.g. `RV`). \n",
    "\n",
    "Combine into two lists called `rightwing` and `leftwing` based on the bloc affiliation of each party. Finally, combine the lists into one called `pt_list`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6876423",
   "metadata": {},
   "source": [
    "#### Solution 2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45fbad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = [x for x in d2v.docvecs.offset2doctag if x.startswith('V-')]\n",
    "KF = [x for x in d2v.docvecs.offset2doctag if x.startswith('KF-')]\n",
    "DF = [x for x in d2v.docvecs.offset2doctag if x.startswith('DF-')]\n",
    "NB = [x for x in d2v.docvecs.offset2doctag if x.startswith('NB-')]\n",
    "LA = [x for x in d2v.docvecs.offset2doctag if x.startswith('LA-')]\n",
    "\n",
    "RV = [x for x in d2v.docvecs.offset2doctag if x.startswith('RV-')]\n",
    "S = [x for x in d2v.docvecs.offset2doctag if x.startswith('S-')]\n",
    "SF = [x for x in d2v.docvecs.offset2doctag if x.startswith('SF-')]\n",
    "ALT = [x for x in d2v.docvecs.offset2doctag if x.startswith('ALT-')]\n",
    "EL = [x for x in d2v.docvecs.offset2doctag if x.startswith('EL-')]\n",
    "\n",
    "rightwing = V + KF + DF + NB + LA\n",
    "leftwing = RV + S + SF + ALT + EL\n",
    "pt_list = leftwing + rightwing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee71007",
   "metadata": {},
   "source": [
    "#### Exercise 2.9: PCA with Party Embeddings\n",
    "\n",
    "We need to do exactly the same steps as before. \n",
    "\n",
    "1) Define empty array\n",
    "2) Populate array with party embeddings\n",
    "3) Apply PCA\n",
    "4) Explore how much variance is explained\n",
    "\n",
    "Use the code you already have to do it using the new model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eeac93",
   "metadata": {},
   "source": [
    "#### Solution 2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1901bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the (X, M) array\n",
    "z = np.zeros((len(pt_list), M))\n",
    "\n",
    "# Loop through the list pt_list and assign each party-term embedding to a row in z\n",
    "for i in range(len(pt_list)):\n",
    "    z[i,:] = d2v.docvecs[pt_list[i]]\n",
    "\n",
    "# Apply PCA\n",
    "pca_model = PCA(n_components=2)\n",
    "Z = pca_model.fit_transform(z)\n",
    "\n",
    "# Explained variance\n",
    "explained_variance = pca_model.explained_variance_ratio_\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6281e513",
   "metadata": {},
   "source": [
    "#### Exercise 2.10: Plot PCA\n",
    "\n",
    "We are now ready to plot the PCs again. Use the function `plot_pca` again. \n",
    "\n",
    "Once again, I convert `Z` to a dataframe before using it and define a color scale to properly label the PCs. \n",
    "\n",
    "Interpret and describe the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b184bc39",
   "metadata": {},
   "source": [
    "#### Solution 2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b0fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Z to a dataframe\n",
    "Z_df = pd.DataFrame(Z)\n",
    "Z_df.columns = ['PC1', 'PC2']\n",
    "Z_df['party_label'] = pt_list\n",
    "\n",
    "color_scale = {'V': 'royalblue', 'KF': 'forestgreen',\n",
    "         'DF': 'gold', 'NB': 'darkslategrey', 'LA': 'mediumturquoise',\n",
    "         'RV': 'darkviolet', 'S': 'red', 'SF': 'sienna', 'EL': 'sandybrown', 'ALT': 'lawngreen'}\n",
    "\n",
    "cols_right = [color_scale['V']]*len(V) + [color_scale['KF']]*len(KF) + [color_scale['DF']]*len(DF) + [color_scale['NB']]*len(NB) + [color_scale['LA']]*len(LA)\n",
    "\n",
    "cols_left = [color_scale['RV']]*len(RV) + [color_scale['S']]*len(S) + [color_scale['SF']]*len(SF) + [color_scale['EL']]*len(EL) + [color_scale['ALT']]*len(ALT)\n",
    "\n",
    "cols = cols_right + cols_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a85dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plot_pca(dataframe=Z_df, indicators=pt_list, cmap=cols, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f8dea",
   "metadata": {},
   "source": [
    "#### Exercise 2.11: \n",
    "\n",
    "Use the class `PCA_INTERPRET` to investigate which words that drives the placement of each party. \n",
    "\n",
    "Describe the results. Do they make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06811138",
   "metadata": {},
   "source": [
    "#### Solution 2.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c863b0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply class\n",
    "PCA_INTERPRET(d2v, pt_list, pca_model, Z, pt_list, rev1=False, rev2=False, min_count=100, max_count = 1000000, max_features = 50000).top_words_list(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
