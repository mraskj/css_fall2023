{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29fbab49",
   "metadata": {},
   "source": [
    "# Class 6: Text Basics - Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f32945a",
   "metadata": {},
   "source": [
    "https://spacy.io/models/da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d908492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.da.examples import sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e74c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download da_core_news_sm\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216da192",
   "metadata": {},
   "source": [
    "## Spacy Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302cd60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_pipeline = spacy.load(\"da_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833af628",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(spacy_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a74c4",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "We pass whatever text we want to process to `spacy_pipeline`, which returns a `Doc` container object (https://spacy.io/api/doc) containing the tokenized text and a number of annotations for each token. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b56b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample text\n",
    "sample_text = sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54275e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SpaCy pipeline\n",
    "doc = spacy_pipeline(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa2cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print doc\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd238e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Looks like a standard string, but it's not - Check type\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1249ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can iterate over the Doc object to access the tokens - note that we access the token by the .text attribute\n",
    "tokens0 = [t.text for t in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be41a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative tokenizer: .split()\n",
    "tokens1 = sample_text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens0), len(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0d198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fcc854",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e44af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can view an individual token by indexing into the Doc object\n",
    "print(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6a0c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also looks like a string, but it's not -- Check type\n",
    "print(type(doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9547b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing a Doc object returns a Span object.\n",
    "print(doc[0:3])\n",
    "print(type(doc[0:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50339858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a token's index in a sentence\n",
    "print([(t.text, t.i) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b12c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy's tokenization is _non-destructive_, which means the original input can be reconstructed from the tokens.\n",
    "# You can view the original input like so:\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c6c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And by reconstructing, we also now have a string object\n",
    "print(type(doc.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02800628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is even non-destructive from each individual token as well\n",
    "print(doc[0].doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0073aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(doc[0].doc.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea4e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It also possible to tokenize multiple sentences at once - but spacy requires a string input\n",
    "s = sentences[0] + ' ' + sentences[1]\n",
    "# s = [sentences[0]] + [sentences[1]]\n",
    "doc = spacy_pipeline(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1169363",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a26a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at individual sentences (there should be two 'Span' objects).\n",
    "print([sent for sent in doc.sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e58289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also access individual tokens, but where the sentence structure is hidden\n",
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d35c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a nested list comprehension to maintain the sentence structure while looking at individual tokens\n",
    "[[t.text for t in sent] for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b021f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why choose a pretrained pipeline over the .split() method?\n",
    "\n",
    "s = 'Toronto ligger 159km fra Buffalo.'\n",
    "\n",
    "doc = spacy_pipeline(s)\n",
    "\n",
    "# Consider the spacy result:\n",
    "tokens0 = [t.text for t in doc]\n",
    "print(tokens0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the .split() result\n",
    "tokens1 = s.split()\n",
    "print(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45933201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far we have tokenized sentences or at most two sentences. Imagine we have a corpus. \n",
    "# tokens = [spacy_pipeline(x) for x in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369d827",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "* Stopwords/digits\n",
    "* Casing\n",
    "* Word reduction (stemming and lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ecd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list with Danish stopwords\n",
    "stop_words = sorted(list(spacy_pipeline.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb45fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print stopwords\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166ce8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute length of stopwords\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f8b604",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f548d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of stopwords using list comprehension\n",
    "[x for x in tokens0 if x not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654774aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of digits\n",
    "[x for x in tokens0 if not x.isdigit()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66035d1e",
   "metadata": {},
   "source": [
    "### Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d887d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case-folding using the builtin .lower() function\n",
    "s.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7da456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case-folding using the .lower_ attribute\n",
    "print([t.lower_ for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15eff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional lowering\n",
    "print([t.lower_ if not t.is_sent_start else t for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91888c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpaCy performs advanced preprocessing steps under the hood such as NER, POS, and Parsing\n",
    "s = 'Toronto ligger 159km fra Buffalo.'\n",
    "[(t.text, t.ent_type_) for t in spacy_pipeline(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6e75bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results are not always as we want. Try replace 'Toronto' with 'København'\n",
    "s = 'København ligger 159km fra Buffalo.'\n",
    "[(t.text, t.ent_type_) for t in spacy_pipeline(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f4ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load english pipeline\n",
    "spacy_pipeline_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb120c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Toronto ligger 159km fra Buffalo.'\n",
    "[(t.text, t.ent_type_) for t in spacy_pipeline_en(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f28938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get SpaCy to explain its abbreviations\n",
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a0766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional lowering using NER as exceptions\n",
    "print([t.lower_ if t.ent_type_ not in ['GPE', 'LOC'] else t for t in spacy_pipeline_en(s)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4993e63",
   "metadata": {},
   "source": [
    "### Word Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a0e537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import DanishStemmer\n",
    "stemmer = DanishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a7970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = 'Udlændinge kommer herop og begår kriminalitet'\n",
    "s = 'Toronto ligger 159km fra Buffalo.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4baf99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming using NLTK\n",
    "[stemmer.stem(t.text) for t in spacy_pipeline(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713211f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization using SpaCy\n",
    "[t.lemma_ for t in spacy_pipeline(s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78daf8dc",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "* Binary and count\n",
    "* TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a48a529",
   "metadata": {},
   "source": [
    "### Binary and Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c38dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a corpus of sentences.\n",
    "corpus = [\n",
    "    \"Red Bull drops hint on F1 engine.\",\n",
    "    \"Honda exits F1, leaving F1 partner Red Bull.\",\n",
    "    \"Hamilton eyes record eighth F1 title.\",\n",
    "    \"Aston Martin announces sponsor.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7cc4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classes and functions from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf2289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a BoW vectorizer\n",
    "vectorizer = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd63056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be71d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See vocab\n",
    "print(vectorizer.get_feature_names_out())\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3969df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort vocab\n",
    "dict(sorted(vectorizer.vocabulary_.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb16b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the vocab to the corpus\n",
    "bow = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd4368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix to np array\n",
    "bow_array = bow.toarray()\n",
    "print(bow_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97c67e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom tokenizer (more steps can easily be added)\n",
    "def spacy_tokenizer(doc):\n",
    "    toks = [t for t in spacy_pipeline_en(doc) if not t.is_punct]\n",
    "    return [t.text for t in toks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407763d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CountVectorizer and apply fit_transform\n",
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=False, decode_error='ignore', token_pattern=None)\n",
    "bow = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f134a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_array = bow.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b59691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise cosine similarity\n",
    "cosine_similarity(bow_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f2554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual computation of cosine similarity\n",
    "np.dot(bow_array[0], bow_array[1]) / (np.linalg.norm(bow_array[0]) * np.linalg.norm(bow_array[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43286ca0",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e4c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a02319f",
   "metadata": {},
   "source": [
    "We'll use the **20 newsgroups** dataset, which is a collection of 18,000 newsgroup posts across 20 topics.<br>\n",
    "https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset\n",
    "<br><br>\n",
    "List of datasets available:<br>\n",
    "https://scikit-learn.org/stable/datasets.html#datasets\n",
    "\n",
    "The **datasets** module includes fetchers for each dataset in scikit-learn. For our purposes, we'll fetch only the posts from the *sci.space* topic, and skip on headers, footers, and quoting of other posts.<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups\n",
    "<br><br>\n",
    "By default, the fetcher retrieves the *training* subset of the data only. If you don't know what that means, it'll become clear later in the course when we discuss modelling. For now, it doesn't matter for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5e5a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = fetch_20newsgroups(categories=['sci.space'],\n",
    "                            remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55389143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need named-entity recognition nor dependency parsing for\n",
    "# this so these components are disabled. This will speed up the\n",
    "# pipeline. We do need part-of-speech tagging however.\n",
    "unwanted_pipes = [\"ner\", \"parser\"]\n",
    "\n",
    "# For this exercise, we'll remove punctuation and spaces (which\n",
    "# includes newlines), filter for tokens consisting of alphabetic\n",
    "# characters, and return the lemma (which require POS tagging).\n",
    "def spacy_tokenizer(doc):\n",
    "    with nlp.disable_pipes(*unwanted_pipes):\n",
    "        return [t.lemma_ for t in nlp(doc) if \\\n",
    "                not t.is_punct and \\\n",
    "                not t.is_space and \\\n",
    "                t.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8756ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Use the default settings of TfidfVectorizer.\n",
    "vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer, token_pattern=None)\n",
    "features = vectorizer.fit_transform(corpus.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ac76f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of unique tokens.\n",
    "print(len(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d933d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dimensions of our feature matrix. X rows (documents) by Y columns (tokens).\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7825562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first two posts.\n",
    "corpus.data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6531f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "vectorizer.vocabulary_['satellite']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db883f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the encoding of the first document looks like in sparse format.\n",
    "print(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780586bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the query into a TF-IDF vector.\n",
    "query = [\"lunar orbit\"]\n",
    "query_tfidf = vectorizer.transform(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10521009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cosine similarities between the query and each document.\n",
    "# We're calling flatten() here becaue cosine_similarity returns a list\n",
    "# of lists and we just want a single list.\n",
    "cosine_similarities = cosine_similarity(features, query_tfidf).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c0bddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k(arr, k):\n",
    "    kth_largest = (k + 1) * -1\n",
    "    return np.argsort(arr)[:kth_largest:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10333822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So for our query above, these are the top five documents.\n",
    "top_related_indices = top_k(cosine_similarities, 5)\n",
    "print(top_related_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7637cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at their respective cosine similarities.\n",
    "print(cosine_similarities[top_related_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7f4f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top match.\n",
    "print(corpus.data[top_related_indices[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second-best match.\n",
    "print(corpus.data[top_related_indices[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19fce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a different query\n",
    "query = [\"satellite\"]\n",
    "query_tfidf = vectorizer.transform(query)\n",
    "\n",
    "cosine_similarities = cosine_similarity(features, query_tfidf).flatten()\n",
    "top_related_indices = top_k(cosine_similarities, 5)\n",
    "\n",
    "print(top_related_indices)\n",
    "print(cosine_similarities[top_related_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a3613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.data[top_related_indices[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
