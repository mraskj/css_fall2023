{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99316132",
   "metadata": {},
   "source": [
    "# Class 6: Text Basics - Solution\n",
    "\n",
    "## 1.0 Word Presence Vectorizer\n",
    "\n",
    "In this lab session, we will work practice the pipeline from:\n",
    "\n",
    "0) reading and cleaning data\n",
    "1) tokenization\n",
    "2) preprocessing\n",
    "3) vectorization\n",
    "4) model analysis and inference\n",
    "\n",
    "We will work with speeches in the Danish parliament and use a binary vectorization with a set of preprocessing steps. We will not work through all possible preprocessing steps or ways of vectorizing. However, you should be able to adapt the pipeline to new applications after today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e2abce",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "* Modules\n",
    "* Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1724771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # Import modules # # # #\n",
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c231fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # Working Directory # # # #\n",
    "\n",
    "if platform.system() == 'Linux':\n",
    "    wd = '/home/rask/'\n",
    "else:\n",
    "    wd = 'C:/Users/au535365/'\n",
    "\n",
    "wd = os.path.join(wd, 'Dropbox/teaching/css_fall2023')\n",
    "    \n",
    "# Change directory\n",
    "os.chdir(wd)\n",
    "\n",
    "# Confirm that the working directory is as intended \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0137855b",
   "metadata": {},
   "source": [
    "#### Exercise 1.0: Reading in Data\n",
    "\n",
    "We start by reading in data. We work the same data as in class05. \n",
    "\n",
    "Instead of reading the data in from our local directories, we read the data directly from GitHub. See the notebook `class05-filereading.ipynb` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f976f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate file ids\n",
    "files = ['20001', \n",
    "         '20011',\n",
    "         '20012',\n",
    "         '20021',\n",
    "         '20031',\n",
    "         '20041',\n",
    "         '20042',\n",
    "         '20051',\n",
    "         '20061',\n",
    "         '20071',\n",
    "         '20072',\n",
    "         '20081',\n",
    "         '20091',\n",
    "         '20101',\n",
    "         '20102',\n",
    "         '20111',\n",
    "         '20121',\n",
    "         '20131',\n",
    "         '20141',\n",
    "         '20142',\n",
    "         '20151',\n",
    "         '20161',\n",
    "         '20171',\n",
    "         '20181',\n",
    "         '20182',\n",
    "         '20191',\n",
    "         '20201',\n",
    "         '20211']\n",
    "\n",
    "# Specify base url\n",
    "base_url = 'https://raw.githubusercontent.com/mraskj/css_fall2023/master/data/ft-speeches/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb3706",
   "metadata": {},
   "source": [
    "#### Solution 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data. Solution here:\n",
    "df = pd.DataFrame()\n",
    "for file in tqdm(files):\n",
    "    df_term = pd.read_csv(base_url + file + '.csv')\n",
    "    df = pd.concat([df, df_term])\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02d5ef7",
   "metadata": {},
   "source": [
    "#### Exercise 1.1: Random Sampling\n",
    "\n",
    "Since this is just an exercise, we don't need to work with all the data we have available. \n",
    "\n",
    "Use the code you used in `class05-exercise` to randomly sample $N=500$ speeches from the dataframe `df` from _exercise 1.0_. \n",
    "\n",
    "Filter the dataframe `df` based on the sampled indices and save the new dataframe in an objected called `sample_df`. Remember to reset_indices using `.reset_index()` but this time specify `drop=False`. This creates a new column called `index`, which allows us to locate the sampled speeches in the original dataframe `df`.\n",
    "\n",
    "Remember to seed a seed to be able to replicate your results: `np.random.seed(10)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3f495",
   "metadata": {},
   "source": [
    "#### Solution 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3531cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution here:\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(10)\n",
    "\n",
    "# Define number of samples\n",
    "n_samples = 500\n",
    "\n",
    "# Random sampling\n",
    "sample_indices = np.random.choice(len(df), size=n_samples, replace=False)\n",
    "\n",
    "# Filter df and save to sample_df\n",
    "sample_df = df.loc[sample_indices].reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77540021",
   "metadata": {},
   "source": [
    "#### Exercise 1.2: Load spaCy Model\n",
    "\n",
    "We'll work with the SpaCy library (https://spacy.io/) when tokenizing and preprocessing our text. \n",
    "\n",
    "Since we use Danish text, we need to load a Danish model (https://spacy.io/models/da): `da_core_news_sm`. Load the model with:\n",
    "\n",
    "    spacy.load('da_core_news_sm')\n",
    "\n",
    "and save it to an object called `spacy_pipeline_da`.\n",
    "\n",
    "If you can not load the model, you must download the model first with:\n",
    "\n",
    "    !python -m spacy download da_core_news_sm\n",
    "    \n",
    "   \n",
    "After you have downloaded/loaded the model, define a list object called `texts`, which is the `text` column from the `sampled_df`. Remember to convert the object to a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddc4bb2",
   "metadata": {},
   "source": [
    "#### Solution 1.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12030c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution here:\n",
    "\n",
    "# Load the model \"da_core_news_sm\"\n",
    "spacy_pipeline_da = spacy.load(\"da_core_news_sm\")\n",
    "\n",
    "# Define a list called `texts' based on the 'text' column in the sample_df dataframe\n",
    "texts = list(sample_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03617cf7",
   "metadata": {},
   "source": [
    "#### Exercise 1.3: Define a customized spaCy tokenizer\n",
    "\n",
    "We want to create a custom spacy tokenizer that takes a string as input and returns a list of token (each token's text) with punctuation filtered out.\n",
    "\n",
    "To do this, define a function called `spacy_tokenizer`. See `class06-tutorial` for the syntax of creating your own functions in Python. The function should return the text of each token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf38ac2",
   "metadata": {},
   "source": [
    "#### Solution 1.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bca081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution here:\n",
    "\n",
    "# Define custom tokenizer that removes punctuation\n",
    "def spacy_tokenizer(doc):\n",
    "    toks = [t for t in spacy_pipeline_da(doc) if not t.is_punct]\n",
    "    return [t.text for t in toks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b2c78f",
   "metadata": {},
   "source": [
    "#### Exercise 1.4: Binary Vectorizer\n",
    "\n",
    "When you have written the function, we now instantiate an instance of the `CountVectorizer` class from sklearn to an object called `vectorizer`. \n",
    "\n",
    "Pass your tokenizer `spacy_tokenizer` to the `tokenizer` parameter.\n",
    "\n",
    "Besides passing your tokenizer, use the following arguments:\n",
    "\n",
    "   - `binary=True`\n",
    "   - `decode_error='ignore'`\n",
    "   - `token_pattern=None`\n",
    "\n",
    "Try read the documentation to figure out the functioning of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a7dc9",
   "metadata": {},
   "source": [
    "#### Solution 1.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af88c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution here:\n",
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, binary=True, decode_error='ignore', token_pattern=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10144e3e",
   "metadata": {},
   "source": [
    "#### Exercise 1.5: Fit Vectorizer\n",
    "\n",
    "When you have instantiated the binary BoW vectorizer to the object `vectorizer`, we then fit the vectorizer to our texts in the list `texts`. Write:\n",
    "\n",
    "    `vectorizer.fit(texts)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc26f3a",
   "metadata": {},
   "source": [
    "#### Solution 1.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a745b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the vectorizer\n",
    "vectorizer.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b5b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When you do it, you should get an error saying: \n",
    "#    `AttributeError: 'list' object has no attribute 'lower'`\n",
    "   \n",
    "#Can you figure out what's wrong? Try to solve the problem. Use the hints if you can not solve it. Save the result to a new list called `processed_text`.#\n",
    "\n",
    "#*Hints:* The problem is that the `tokens` object contains nested lists. This creates a problem as you see from the error since the tokenizer you provided to `CountVectorizer` expects a string input. Since the list is nested, we are given a list to the `vectorizer` object every time. The solution is to flatten the list, that is we want to join each nested list to a single string. Right now, each nested list has a token in each element. We join these together using:\n",
    "#    \n",
    "#    `' '.join()`\n",
    "    \n",
    "#Note that you must wrap the it inside a list comprehension! \n",
    "\n",
    "# Join together tokens for each document\n",
    "# processed_text = [' '.join(x) for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f3dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can apply the binary vectorizer to the list `processed_text`\n",
    "#vectorizer.fit(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9800de29",
   "metadata": {},
   "source": [
    "#### Exercise 1.6: Inspect the Vocabulary\n",
    "\n",
    "Now that we've fitted the speeches to the vectorizer (i.e. we have created our vocabulary), we can inspect the result.\n",
    "\n",
    "\n",
    "1) Print our the vocabulary from the `vectorizer` object using the `vocabulary_` attribute.\n",
    "2) Compute the length of the vocab\n",
    "3) Sort the vocab (since the vocab is a dictionary, you need to pass a lambda function to the key parameter inside the `sorted()` function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60714a98",
   "metadata": {},
   "source": [
    "#### Solution 1.6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d5f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write solutions for three bullets in three cells below here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea43443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Print vocab\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f3bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Compute number of unique tokens\n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Sort vocabulary \n",
    "sorted(vectorizer.vocabulary_.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c25f2f1",
   "metadata": {},
   "source": [
    "#### Exercise 1.7: Transform the Documents\n",
    "\n",
    "We have now created and inspected the vocabulary. The next step is to transform it to create a document-term-matrix. \n",
    "\n",
    "Use the `.transform()` method on the `processed_text` list you generated in exercise 1.5 and save the result to an object called `binary_bow`.\n",
    "\n",
    "When this is done convert the matrix to a numpy array using to attribute `.toarray()` of your `binary_bow` object. The array should be saved to an object called `binary_bow_array`. \n",
    "\n",
    "Compute the shape of `binary_bow_array`. What's the expected dimension? Does it match your expectation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d619ae",
   "metadata": {},
   "source": [
    "#### Solution 1.7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bcef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution here:\n",
    "\n",
    "# Transform \n",
    "binary_bow = vectorizer.transform(processed_text)\n",
    "\n",
    "# Numpy array\n",
    "binary_bow_array = binary_bow.toarray()\n",
    "\n",
    "# Compute shape of the array - what's the expected dimension?\n",
    "binary_bow_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a95e40",
   "metadata": {},
   "source": [
    "#### Exercise 1.8: Query Search\n",
    "\n",
    "We have now tokenized, preprocessed, and vectorized our corpus. The final thing is analysis and inference. In this exercise, we restrict ourselves to simple, but a powerful applications. In this exercise 1.8, we define a search engine where we compute the similarity between documents (i.e. speeches) and a search query. \n",
    "\n",
    "To do this, we define a query word and save it in a list called `query`. You can choose whatever word you like, but I have used \"dagpenge\". The reason why we use a list despite having only one word is because of the required format of our `vectorizer` object.\n",
    "\n",
    "When you have done this, we need to transform our `query` list into a vector to be able to numerically compare the similarity between the query and our speeches. Use the existing `vectorizer` you have and use the `.transform()` method and save it to an object called `query_vector`.\n",
    "\n",
    "Why do we transform the query using the existing vectorizer object and not a new one?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b6079",
   "metadata": {},
   "source": [
    "#### Solution 1.8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534e2327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution here:\n",
    "\n",
    "# Transform the query to a binary vector using the `vectorizer` and the `.transform()` method\n",
    "query = [\"dagpenge\"]\n",
    "query_vector = vectorizer.transform(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f288adc4",
   "metadata": {},
   "source": [
    "#### Exercise 1.9: Query Similarity\n",
    "\n",
    "We can now compute the cosine similarity between the `binary_bow_array` object and the `query_vector`. Use the `cosine_similarity` function imported from sklearn in top of the notebook. Flatten the results using `.flatten()`. We use this to convert the array from $(500,1)$ to $(500, )$. Save the results to an object called `cos_sim`. \n",
    "\n",
    "\n",
    "1. Verify the shape of `cos_sim` after flattening\n",
    "2. Return $k=5$ indices of speeches with highest cosine similarity\n",
    "3. Inspect the cosine similarity for top $k=5$ speeches\n",
    "4. Print the text of the speech with the highest similarity by subetting the top index using the `texts` list\n",
    "\n",
    "\n",
    "I have provided you a function that help you to do step 2 below: `top_k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db034263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that returns the top k indices\n",
    "def top_k(arr, k):\n",
    "    kth_largest = (k + 1) * -1\n",
    "    return np.argsort(arr)[:kth_largest:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15057cff",
   "metadata": {},
   "source": [
    "#### Solution 1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb9af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution here: \n",
    "\n",
    "# Compute cosine similarity\n",
    "cos_sim = cosine_similarity(binary_bow_array, query_vector).flatten()\n",
    "\n",
    "# 1) Verify shape\n",
    "print(cos_sim.shape)\n",
    "\n",
    "# 2) Top k=5 speeches\n",
    "top_related_indices = top_k(cos_sim, 5)\n",
    "print(top_related_indices)\n",
    "\n",
    "# 3) Cosine similarity for k=5\n",
    "print(cos_sim[top_related_indices])\n",
    "\n",
    "# 4) Top match\n",
    "print(texts[top_related_indices[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e334ae4",
   "metadata": {},
   "source": [
    "#### Exercise 1.10: Advanced Preprocessing\n",
    "\n",
    "The customized spaCy tokenizer we provided was very simple. We need to do more in actual applications.\n",
    "\n",
    "In this exercise, you are encouraged to play around with possible preprocessing steps and see how it affects the query similarity exercise. \n",
    "\n",
    "Adapt the `spacy_tokenizer` you wrote in exercise 1.3 .\n",
    "\n",
    "Do the following:\n",
    "\n",
    "- Remove punctuation (as already done in exercise 1.3)\n",
    "- Lemmatize (can be done by returning the `lemma_` attribute of each token)\n",
    "- Lower-casing\n",
    "- Removal of stopwords (define a list of stopwords from `spacy_pipeline_da.Defaults.stop_words`)\n",
    "\n",
    "You are free to use more preprocessing steps if you like. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f751242",
   "metadata": {},
   "source": [
    "#### Solution 1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21cbcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution in the cells below here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5dfaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list with Danish stopwords\n",
    "stop_words = sorted(list(spacy_pipeline_da.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e6d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize tokenizer\n",
    "def spacy_tokenizer(doc):\n",
    "    toks = [t for t in spacy_pipeline_da(doc) if not t.is_punct]\n",
    "    toks = [t.lemma_ for t in toks]\n",
    "    toks = [t.lower() for t in toks]\n",
    "    toks = [t for t in toks if t not in stop_words]\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89889353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CountVectorizer\n",
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, binary=True, decode_error='ignore', token_pattern=None)\n",
    "\n",
    "\n",
    "# Transform the documents using .transform()\n",
    "binary_bow = vectorizer.transform(texts)\n",
    "\n",
    "# Convert to np array\n",
    "binary_bow_array = binary_bow.toarray()\n",
    "\n",
    "# Transform the query to a binary vector using the `vectorizer` and the `.transform()` method\n",
    "query = [\"dagpenge\"]\n",
    "query_vector = vectorizer.transform(query)\n",
    "\n",
    "# Compute cosine similarity\n",
    "cos_sim = cosine_similarity(binary_bow_array, query_vector).flatten()\n",
    "\n",
    "# Verify shape\n",
    "print(cos_sim.shape)\n",
    "\n",
    "# Top k=5 speeches\n",
    "top_related_indices = top_k(cos_sim, 5)\n",
    "print(top_related_indices)\n",
    "\n",
    "# Cosine similarity for k=5\n",
    "print(cos_sim[top_related_indices])\n",
    "\n",
    "# Top match\n",
    "print(texts[top_related_indices[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
