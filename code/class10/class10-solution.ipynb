{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiil7Dmf8YbmdZtf00yoy1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mraskj/css_fall2023/blob/main/code/class10/class10-solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class 10: Audio Measurement - Solution\n",
        "\n",
        "In this exercise, we explore how we can use a Python-binding of Praat, a speech analysis software designed to study phonetics (the sound of speeches) using computers, to study acoustic features of political speeches.\n",
        "\n"
      ],
      "metadata": {
        "id": "7Mwg1E0qXNid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0 Setup\n",
        "\n",
        "We start by:\n",
        "\n",
        "1. Cloning the course GitHub repo\n",
        "2. Install necessary packages\n",
        "3. Importing necessary modules\n",
        "4. Reading data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KmZlm8YkehcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.1 Cloning GitHub Repository"
      ],
      "metadata": {
        "id": "o7be1ctVg1xH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vf8LweEaX9ri"
      },
      "outputs": [],
      "source": [
        "# Clone GitHub directory into\n",
        "!git clone https://github.com/mraskj/css_fall2023.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.2 Install Packages"
      ],
      "metadata": {
        "id": "Mid5kt9yhENt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "!pip install -r /content/css_fall2023/requirements/requirements_topic4-class9.txt"
      ],
      "metadata": {
        "id": "G85KZhXHYo1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.3 Importing Modules"
      ],
      "metadata": {
        "id": "Nv-JBOVNhHwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For file and directory management\n",
        "import os\n",
        "\n",
        "# For regular expressions\n",
        "import re\n",
        "\n",
        "# For data handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# For plotting\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set() # Use seaborn's default style\n",
        "# Feature extraction\n",
        "import librosa\n",
        "\n",
        "# For Praat\n",
        "import parselmouth"
      ],
      "metadata": {
        "id": "_JYM5ag0YUPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.4 Reading Data\n",
        "\n",
        "In this exercise, we will work with 20 audio files each corresponding to a speech in the Danish parliament in the second assembly of the 2007 parliamentary term (*20072*). The files come from 10 different speakers with two speeches each. FIve of the speakers are men and Five are women.\n",
        "\n",
        "You find the files in *data/audio/class10/* folder in the cloned GitHub repo."
      ],
      "metadata": {
        "id": "GAmgPs20hL-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path to directory containing audio files\n",
        "base_dir = os.path.join(os.getcwd(), 'css_fall2023/data/audio/class10')\n",
        "\n",
        "male_files = os.listdir(os.path.join(base_dir, 'male'))\n",
        "female_files = os.listdir(os.path.join(base_dir, 'female'))"
      ],
      "metadata": {
        "id": "DJup7iKyZjOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we print `male_files` and `female_files`, we'll see that they are unsorted. To keep things in order, we sort the files by the first digits in the filenames. We define a custom function for this purpose and provide the function as parameter to the `key` argument in the `sorted()` function."
      ],
      "metadata": {
        "id": "akCPUP-ohzEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom sorting key function\n",
        "def digit_sort_key(v):\n",
        "    digits = re.search('\\d+', v)\n",
        "    return int(digits.group())\n",
        "\n",
        "# Sort the lists\n",
        "male_files = sorted(male_files, key=digit_sort_key)\n",
        "female_files = sorted(female_files, key=digit_sort_key)"
      ],
      "metadata": {
        "id": "yORBU9xjzTXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each audio file corresponds to speech given in a parliamentary debate in the Danish Parliament. In the GitHub repo, you find a dataset with metadata for each file: `class10_audio_df.csv`. Read in the data to an object called `speech_df`. It contains metadata for the audio files such as the name of the file, the speaker, and a bunch of other variables. Before we proceed, we divide `speech_df`, into two: `female_speech_df` and `male_speech_df`."
      ],
      "metadata": {
        "id": "zsAU22_sh8Y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in dataframes\n",
        "speech_df = pd.read_csv('/content/css_fall2023/data/class10_audio_df.csv')\n",
        "\n",
        "# Divide dataset into two - note that we can also filter the data based on the `gender` column\n",
        "female_speech_df = speech_df.iloc[:10]\n",
        "male_speech_df = speech_df.iloc[10:]"
      ],
      "metadata": {
        "id": "Ivib5wXDh1e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Visualization of Pitch\n",
        "\n",
        "While `parselmouth-praat` enables us to measure a wide range of acoustic features, we focus on the fundamental frequency ($F0$) here. Feel free to play around with others as well.\n",
        "\n",
        "Recall from the lecture that $F0$ refers to the number of times per second our vocal cords vibrate when making voiced sounds. As the name suggests, voiced sounds refer to sounds that make use of our vocal cords. All vowel sounds are voiced, but not all consonants are. Some consonant sounds, such as /p/, /f/ and /s/ are unvoiced. This means the vocal cords do not vibrate when producing these sounds.\n",
        "\n",
        "For our purpose, we are talking about $F0$ at a higher level than vowels and consonants, typically at the speech level, but other semantic units such as sentences are also likely or fixed temporal segments (e.g. a five second window). Regardless of the level of measurement, the fundamental frequency is called the *fundamental* since it's the dominant frequency in the given time period that we consider.\n",
        "\n",
        "As we saw on the slides, the $F0$ is a strong and robust indicator of emotional arousal. The idea is simple. When a speaker becomes emotional aroused, or if a speaker wants to *project* arousal, $F0$ increases. The link is not necessarily causal, but what matters to us is that arousal is reflected in $F0$. We will explore this idea in this exercise by estimating the pitch of speeches.\n",
        "\n",
        "The fundamental frequency of a sound is closely correlated with pitch. Although some may think that $F0$ is just a fancy word for pitch, others argue that pitch is a different concept. In fact, pitch is more to do with a person's perception of a $F0$. In other words, fundamental frequency refers to the actual physical properties of a sound wave signal, whereas pitch refers to how our ears and brains perceive the signal regarding the rate of frequency.\n",
        "\n",
        "In this exercise, we will investigate whether we can interpret a speaker's level of emotional arousal by computing $F0$ for the 20 audio files and projecting the estimates onto a spectrogram. We did something similar in the tutorial. Your task is to:\n",
        "\n",
        "\n",
        ">  **Plot each speech as a spectrogram, compute $F0$ and project it onto the spectrogram**.\n",
        "\n",
        "1. Make two plots, one of female speakers and one for male speakers. Arrange the plots in a $5\\times 2$ grid with each row being a single speaker.\n",
        "2. Describe your modeling choices.\n",
        "\n",
        "    a) How do you handle pitch estimates that are zero?\n",
        "\n",
        "    b) Which settings do you use to compute the pitch?\n",
        "\n",
        "    c) How do you control the time range? (see step 3)\n",
        "\n",
        "    d) Do you restrict the y-axis of the spectrogram? Can be done by explictly passing a value to the `maximum_frequency` argument in the `parselmouth.Sound.to_spectrogram()` method.\n",
        "3. Conduct step 1 for different time ranges and describe the differences.\n",
        "\n",
        "    - Range 1: Full speech\n",
        "    - Range 2: 15 seconds of speech\n",
        "    - Range 3: 5 seconds of speech\n",
        "    - Range 4: 2 seconds of speech\n",
        "\n",
        "\n",
        "\n",
        "4. Based on the results, can you visually identify the speech where the speaker is most aroused? Why, why not? Does the time range matters?"
      ],
      "metadata": {
        "id": "upie4FQ-i-Hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution 1"
      ],
      "metadata": {
        "id": "ufDIVRnYHABl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_spectrogram(spectrogram, start=0, stop=10000, dynamic_range=70, cmap='afmhot'):\n",
        "    \"\"\"\n",
        "    Draw a spectrogram using Matplotlib.\n",
        "\n",
        "    Parameters:\n",
        "    spectrogram (Spectrogram): The input spectrogram.\n",
        "    start (float, optional): The start time in milliseconds. Defaults to 0.\n",
        "    stop (float, optional): The stop time in milliseconds. Defaults to 10000.\n",
        "    dynamic_range (float, optional): The dynamic range in decibels. Defaults to 70.\n",
        "    cmap (str, optional): The colormap to use. Defaults to 'afmhot'.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    X, Y = spectrogram.x_grid(), spectrogram.y_grid()\n",
        "    indices = np.where((X >= start) & (X <= stop))[0]\n",
        "    X = X[indices]\n",
        "    sg_db = 10 * np.log10(spectrogram.values)\n",
        "    sg_db = sg_db[:,indices[:-1]]\n",
        "    plt.pcolormesh(X, Y, sg_db, vmin=sg_db.max() - dynamic_range, cmap=cmap)\n",
        "    plt.ylim([spectrogram.ymin, spectrogram.ymax])\n",
        "\n",
        "\n",
        "def draw_pitch(pitch):\n",
        "    \"\"\"\n",
        "    Draw a pitch contour using Matplotlib.\n",
        "\n",
        "    Parameters:\n",
        "    pitch (Pitch): The input pitch object.\n",
        "\n",
        "    Returns:\n",
        "    Tuple[float, float]: A tuple containing the mean F0 and standard deviation of F0.\n",
        "    \"\"\"\n",
        "    pitch_values = pitch.selected_array['frequency']\n",
        "    f0mean = np.mean(pitch_values[pitch_values > 0])\n",
        "    f0std = np.std(pitch_values[pitch_values > 0])\n",
        "    pitch_values[pitch_values==0] = np.nan\n",
        "\n",
        "    plt.plot(pitch.xs(), pitch_values, 'o', markersize=5, color='#381a61')\n",
        "    plt.plot(pitch.xs(), pitch_values, 'o', markersize=2, color='#e78429')\n",
        "\n",
        "    plt.grid(False)\n",
        "    plt.ylim(0, pitch.ceiling)\n",
        "\n",
        "    return np.round(f0mean, 2), np.round(f0std, 2)\n",
        "\n",
        "\n",
        "def draw_spectrogram_pitch(files:list,\n",
        "                           gender:str,\n",
        "                           dataframe,\n",
        "                           start=None,\n",
        "                           stop=None,\n",
        "                           max_freq = 6000,\n",
        "                           cmap='afmhot',\n",
        "                           show=True):\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Draw spectrograms and pitch contours for a list of audio files.\n",
        "\n",
        "    Parameters:\n",
        "    files (list): List of audio files.\n",
        "    gender (str): Gender of the speaker.\n",
        "    dataframe (DataFrame): Dataframe containing speaker information.\n",
        "    start (float, optional): The start time in seconds. If not provided, it's determined from the audio data. Defaults to None.\n",
        "    stop (float, optional): The stop time in seconds. If not provided, it's determined from the audio data. Defaults to None.\n",
        "    max_freq (float, optional): The maximum frequency for the spectrogram. Defaults to 6000.\n",
        "    cmap (str, optional): The colormap to use. Defaults to 'afmhot'.\n",
        "    show (bool, optional): Whether to display the plots. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    n_plots = len(files)\n",
        "    num_rows = (n_plots + 1) // 2\n",
        "    num_cols = n_plots // num_rows\n",
        "\n",
        "    plt.figure(figsize=(30, 22))\n",
        "\n",
        "    for c, f in enumerate(files):\n",
        "        snd = parselmouth.Sound(os.path.join(base_dir, gender, files[c]))\n",
        "\n",
        "        if not start and not stop:\n",
        "          start, stop = int(snd.xmin), int(snd.xmax)\n",
        "\n",
        "        pitch = snd.to_pitch()\n",
        "        spectrogram = snd.to_spectrogram(window_length=0.025, maximum_frequency=max_freq)\n",
        "\n",
        "        plt.subplot(num_rows, num_cols, c + 1)\n",
        "\n",
        "        draw_spectrogram(spectrogram, start=start, stop=stop, cmap=cmap)\n",
        "\n",
        "        if c % 2 == 0:\n",
        "          plt.ylabel('Frequency (Hz)')\n",
        "\n",
        "        if c % 2 == 1:\n",
        "          plt.yticks([])\n",
        "\n",
        "        if np.abs(c - n_plots) <= 2:\n",
        "          plt.xlabel('Time (s)')\n",
        "\n",
        "        plt.twinx()\n",
        "        f0mean, f0std = draw_pitch(pitch)\n",
        "\n",
        "        if c % 2 == 0:\n",
        "          plt.yticks([])\n",
        "\n",
        "        if c % 2 == 1:\n",
        "          plt.ylabel('F0 (Hz)')\n",
        "\n",
        "        if c < 8:\n",
        "          plt.xticks([])\n",
        "\n",
        "        plt.xlim([start, stop])\n",
        "        plt.title(f\"Speaker: {dataframe.iloc[c]['sppid']} (F0 mean={np.round(f0mean, 3)}, F0 std={np.round(f0std, 3)})\")\n",
        "\n",
        "    if show:\n",
        "      plt.show()\n"
      ],
      "metadata": {
        "id": "Nu5kkIeCaazx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw plot for male speakers\n",
        "draw_spectrogram_pitch(files=male_files,\n",
        "                       gender='male',\n",
        "                       dataframe=male_speech_df,\n",
        "                       cmap='viridis',\n",
        "                       start=20,\n",
        "                       stop=25)"
      ],
      "metadata": {
        "id": "LFUt3Au-DjvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw plot for female speakers\n",
        "draw_spectrogram_pitch(files=female_files,\n",
        "                       gender='female',\n",
        "                       dataframe=female_speech_df,\n",
        "                       cmap='viridis',\n",
        "                       start=5,\n",
        "                       stop=20)"
      ],
      "metadata": {
        "id": "okU0_xqHEWfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Standardized vs Unstandardized Pitch\n",
        "\n",
        "In *Exercise 1*, we visualized pitch contours across the $20$ speeches comparing two speeches for each speaker at a time. This pairwise-comparison indirectly parses out any speaker heterogeneity since we compare speeches within the same speaker.\n",
        "\n",
        "To see why this is important, we will now compute the speech-level average of $F0$ for each speech to investigate the impact of speaker heterogeneity when interpreting results.  \n",
        "\n",
        "\n",
        "\n",
        "> **Task 1: Compute the speech-level average $F0$ for both male and female speakers. Add metadata from the `speech_df` and combine into a dataframe. Take the columns 'sppid', 'gender' and 'spitch'. The 'sppid' column is the speaker identifier containing the name of the speaker and the party. The 'gender' column is the gender of the speaker. And the `spitch' column contains standardized pitch estimates for that specific speech computed over speeches from 2000-2021. Hence, hundreds and even thousands of speeches contribute to the score. Recall that standardization scales data to mean zero and unit variance. Sort the computed F0 means from highest to lowest and describe the results. Pay attention to the difference between your F0 estimates and the standardized estimates.**\n",
        "\n",
        "> **Task 2: Plot the pitch estimates for the first speech for each speaker of your $20$ files. Make a total of three plots. The first plot should contain pitch distributions for both male and female speakers in one. The second plot should contain pitch distributions for female speakers only. And the third plot should contain pitch distributions for male speakers only. Arrange the plot in a grid where you have the second and third plot vertically stacked. Describe the results.**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "55S_qtLjxVBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution Task 1"
      ],
      "metadata": {
        "id": "CZnS570g8mh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the pitch for all 20 speeches.\n",
        "# I iterate over the files in `female_files` first and then `male_files`.\n",
        "# For each loop, I:\n",
        "#     - load the file using `parselmouth.Sound()`\n",
        "#     - convert the sound object to a pitch object using `.to_pitch()`\n",
        "#     - extract the values using `.selected_array['frequency']`\n",
        "#     - save all non-zero estimates in an empty list associated with each loop\n",
        "\n",
        "female_pitch_list = []\n",
        "for c, f in enumerate(female_files):\n",
        "    snd = parselmouth.Sound(os.path.join(base_dir, 'female', female_files[c]))\n",
        "    pitch = snd.to_pitch()\n",
        "    pitch_values = pitch.selected_array['frequency']\n",
        "    female_pitch_list.append(pitch_values[pitch_values > 0])\n",
        "\n",
        "male_pitch_list = []\n",
        "for c, f in enumerate(male_files):\n",
        "    snd = parselmouth.Sound(os.path.join(base_dir, 'male', male_files[c]))\n",
        "    pitch = snd.to_pitch()\n",
        "    pitch_values = pitch.selected_array['frequency']\n",
        "    male_pitch_list.append(pitch_values[pitch_values > 0])"
      ],
      "metadata": {
        "id": "cKF9QtUl4Se8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the average F0 for each file and store the results in two lists.\n",
        "# Note that I use a zip() with the filenames to make the link\n",
        "# between the values and the audio files explicit.\n",
        "male_f0mean = [(x, y) for x, y in zip(male_files, [np.mean(x) for x in male_pitch_list])]\n",
        "female_f0mean = [(x, y) for x, y in zip(female_files, [np.mean(x) for x in female_pitch_list])]\n",
        "\n",
        "# Unpack the list of tuples into two separate tuples\n",
        "file_list, f0_list = zip(*female_f0mean + male_f0mean)"
      ],
      "metadata": {
        "id": "7YsRgxVcj2NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a dataframe based on the `speech_df` and the two separate tuples.\n",
        "# Note that the tuples must be converted to a pd.Series or pd.DataFrame object\n",
        "# to be used in the pd.concat() function. I specify axis=1 to say that I want\n",
        "# to concatenate at the column-level.\n",
        "f0_df = pd.concat([speech_df[['sppid', 'gender', 'spitch']],\n",
        "           pd.Series(file_list, name='file'),\n",
        "           pd.Series(f0_list, name='f0')],\n",
        "          axis=1)"
      ],
      "metadata": {
        "id": "TyxoQxrzkb3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort values by the 'f0' column in descending order\n",
        "f0_df = f0_df.sort_values(by='f0', ascending=False)"
      ],
      "metadata": {
        "id": "QMxfXYFGluR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlate spitch and f0 - correlation is 0.305 indicating only a weak association\n",
        "f0_df[['spitch', 'f0']].corr()"
      ],
      "metadata": {
        "id": "30zUkj3n2MTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution Task 2"
      ],
      "metadata": {
        "id": "pTlVFoRR8pUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute mean pitch for men and women -- use [::2] indexing to extract every second speech\n",
        "male_mean = np.mean([np.mean(x) for x in male_pitch_list[::2]])\n",
        "female_mean = np.mean([np.mean(x) for x in female_pitch_list[::2]])"
      ],
      "metadata": {
        "id": "0AjsCVVJ-fgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot distribution of pitch for men and women arranged in a grid.\n",
        "plt.figure(figsize=(14,8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "ax = sns.kdeplot(male_pitch_list[::2] + female_pitch_list[::2], legend=False)\n",
        "ax.text(male_mean + 10, 0.00125, '$\\u2642$')\n",
        "ax.text(female_mean + 10, 0.00125, '$\\u2640$')\n",
        "plt.axvline(male_mean, 0, 5, linestyle='--', color='black')\n",
        "plt.axvline(female_mean, 0, 5, linestyle=':', color='black')\n",
        "plt.ylabel('')\n",
        "plt.yticks([])\n",
        "plt.xlabel('Hz')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "sns.kdeplot(female_pitch_list[::2])\n",
        "plt.legend(labels=list(female_speech_df.iloc[::2].sppid.values), frameon=False,fontsize=10)\n",
        "plt.ylabel('')\n",
        "plt.yticks([])\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "sns.kdeplot(male_pitch_list[::2])\n",
        "plt.legend(labels=list(male_speech_df.iloc[::2].sppid.values), frameon=False, fontsize=10)\n",
        "plt.ylabel('')\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nKygQB2H77Yv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}