{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO3+TdY6lhe6uYlS5m/dzGW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mraskj/css_fall2023/blob/main/code/class11/class11-solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class 11: Speaker Diarization and Recognition - Solution"
      ],
      "metadata": {
        "id": "G2F540X2lqh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, we investigate the validity of using pretrained models to conduct speaker diarization and speaker recognition. For the former, we compare acoustic features computed with groundtruth and automated timestamps. For the latter, we investigate how embeddings can be used to discriminate between speakers.  \n",
        "\n",
        "As we always do in Colab, you should start by cloning the GitHub repo and by constructing an output folder to save files you create in Colab."
      ],
      "metadata": {
        "id": "cmMzwffYk3Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone GitHub directory into\n",
        "!git clone https://github.com/mraskj/css_fall2023.git"
      ],
      "metadata": {
        "id": "V9oE7XE8EfdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define an output directory where we save all output created in Colab\n",
        "output_dir = os.path.join(os.getcwd(), 'output')\n",
        "\n",
        "# Make directory if it not already exists.\n",
        "if not os.path.exists(output_dir):\n",
        "  os.mkdir(output_dir)"
      ],
      "metadata": {
        "id": "LIMvaQXFFOLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Audio Measurement and Annotations\n",
        "\n",
        "In the first exercise, we investigate the sensitivity of annotation errors to the estimation of acoustic features. I have provided you with a RTTM groundtruth annotations for the same debate snippet as we worked with in the tutorial. The RTTM file can be found here `/content/css_fall2023/data/audio/class11/57-17-groundtruth-withoutspeaker.rttm` (assuming you have cloned the GitHub repo). Note that we work with the version without speaker labels here. You can also work with version with speaker labels if you want - the results should be similar.\n",
        "\n",
        "The recording of the debate snippet is found here: `/content/css_fall2023/data/audio/class11/57-71-class.mp4`.\n",
        "\n",
        "1. Convert the video file to audio\n",
        "\n",
        "2. Apply diarization using the `pyannote/speaker-diarization@2.1` model from `pyannote.audio`. Describe each step you take (e.g. do you keep all diarized segments or do you discard some? Do you merge back-to-back segments from the same speaker label).\n",
        "\n",
        "3. Write the diarization result to a RTTM file.\n",
        "\n",
        "\n",
        "4. Compute the DER for three different error margins: 0.0, 0.5, and 1.0. Describe the results. Based on this, do you expect substantial differences in the estimation of acoustic features such as pitch, loudness, or MFCCs when using groundtruth annotations compared to the automated annotations?\n",
        "\n",
        "\n",
        "5. Split the diarized segments to separate audio files\n",
        "\n",
        "\n",
        "6. Split the groundtruth segments to separate audio files\n",
        "\n",
        "\n",
        "7. Compute the first 10 MFCCs, pitch, and intensity for each of the segments from step 4 and 5. For pitch, compute also the standard deviation, minimum, and maximum value.\n",
        "\n",
        "8. Compare the measure for the diarized and groundtruth segments. Note that you must link each diarization segment to a groundtruth segment to be able to do the comparison. There must be exactly the same number of segments in both conditions. Describe and show the results.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fS2o_6yNFQ2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1.1\n",
        "\n",
        "I start by converting the video file to an audio file using FFMPEG. I use a sampling rate of $16,000$ and a single channel."
      ],
      "metadata": {
        "id": "BEfT-lqoEJZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# We specify the sampling rate and number of channels beforehand. This is not\n",
        "# strictly necessary, but is a useful approach when you wrap code in functions.\n",
        "sr = 16000\n",
        "channels = 1\n",
        "\n",
        "# Specify path to video and audio file file\n",
        "video_fpath = '/content/css_fall2023/data/audio/class11/57-71-class.mp4'\n",
        "audio_fpath = os.path.join(output_dir, '57-71-class.wav')\n",
        "\n",
        "# Write the ffmpeg code\n",
        "cmd = ['ffmpeg',\n",
        "       '-y',\n",
        "       '-i',\n",
        "       video_fpath,\n",
        "       '-vn',\n",
        "       '-ar',\n",
        "       str(sr),\n",
        "       '-ac',\n",
        "       str(channels),\n",
        "       '-acodec',\n",
        "       'pcm_s16le',\n",
        "       audio_fpath]\n",
        "\n",
        "# Execute the code in the shell\n",
        "subprocess.call(cmd)\n",
        "\n",
        "# -y specifies that the output file should be overwritten if already existing\n",
        "# -i specifies the input file\n",
        "# -vn omits the video stream\n",
        "# -ar specifies the sampling rate\n",
        "# -ac specifies the number of channels\n",
        "# -acodec specifies the type of encoding (in this case pcm_s16le)"
      ],
      "metadata": {
        "id": "rjJGxvEPC8gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1.2\n",
        "\n",
        "For the diarization, I first install the pyannote.audio library and then import the `Pipeline` class."
      ],
      "metadata": {
        "id": "Sqd78kzqEpPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyannote.audio"
      ],
      "metadata": {
        "id": "62C1y5esCR0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Pipeline from pyannote.audio\n",
        "from pyannote.audio import Pipeline"
      ],
      "metadata": {
        "id": "luT_nppVCa2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I then load in the pretrained pipeline `pyannote/speaker-diarization@2.1` and allocate the loaded pipeline to `cuda` (i.e. a GPU) if available. The audio file is then diarized."
      ],
      "metadata": {
        "id": "p03acTCFFXne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define access token and name of pipeline\n",
        "from google.colab import userdata\n",
        "access_token = userdata.get('huggingface')\n",
        "\n",
        "pipeline_name = \"pyannote/speaker-diarization@2.1\"\n",
        "\n",
        "# Load and initiate pipeline.\n",
        "pipeline = Pipeline.from_pretrained(pipeline_name,\n",
        "                                    use_auth_token=access_token)"
      ],
      "metadata": {
        "id": "_qRNby3MCLDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyTorch\n",
        "import torch\n",
        "\n",
        "# Send pipeline to GPU (if available)\n",
        "pipeline.to(torch.device(\"cuda\"  if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "print(f\"Device: {pipeline.device}\")"
      ],
      "metadata": {
        "id": "o6m1cDX3DEJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the diarization pipeline to our audio file\n",
        "diarization = pipeline(os.path.join(output_dir, '57-71-class.wav'))"
      ],
      "metadata": {
        "id": "8peNif61DG5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only segments that are five seconds or longer.\n",
        "# The choice of five seconds are fairly arbitrary, but is intended to\n",
        "# capture that I want to avoid segments that are not actual speech.\n",
        "threshold = 5.0\n",
        "dia_result = []\n",
        "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "    start = round(turn.start, 1)\n",
        "    end = round(turn.end, 1)\n",
        "\n",
        "    # Keep only segments that are five seconds or longer\n",
        "    if (end - start) >= threshold:\n",
        "      dia_result.append([speaker, start, end])"
      ],
      "metadata": {
        "id": "A9xWRmr111Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge consecutive segments\n",
        "diarized = []\n",
        "curr = dia_result[0][0]\n",
        "start = dia_result[0][1]\n",
        "for i in range(len(dia_result)-1):\n",
        "  if curr == dia_result[i+1][0]:\n",
        "    continue\n",
        "  else:\n",
        "    diarized.append([curr,start, dia_result[i][2]])\n",
        "    curr = dia_result[i+1][0]\n",
        "    start = dia_result[i+1][1]\n",
        "\n",
        "diarized.append([curr,start, dia_result[i+1][2]])"
      ],
      "metadata": {
        "id": "AIRjEeUn1xfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diarized"
      ],
      "metadata": {
        "id": "n6i5U4P459a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1.3"
      ],
      "metadata": {
        "id": "Kbp9E8yYHir2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I then loop through each diarized segment and write to an RTTM file. Before I do so, I manually remove the first and last diarized segment, which corresponds to chair speech. If you look at the groundtruth file, you will recognize that these segments are not included. This is not urgent at this stage, but when we make pairwise comparisons across acoustic features, we need to link *one* diarized segment to *one* groundtruth segment in a 1:1 match."
      ],
      "metadata": {
        "id": "rfJEyqFKG3pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write output to an RTTM-file by looping through each diarized segment\n",
        "dia_rttm = []\n",
        "for i, v  in enumerate(diarized[1:-1]):\n",
        "    start = round(float(v[1]), 2)\n",
        "    end = round(float(v[2]), 2)\n",
        "\n",
        "    # Join each line as a string\n",
        "    dia_rttm.append(' '.join(['SPEAKER ' +\n",
        "                      '57-17-class' +\n",
        "                      ' 1',\n",
        "                      str(start),\n",
        "                      str(round(end-start, 1)),\n",
        "                      '<NA> <NA>',\n",
        "                      v[0],\n",
        "                      '<NA> <NA>']))\n",
        "\n",
        "# Writing\n",
        "with open(os.path.join(output_dir, '57-17-prediction.rttm'), 'w') as f:\n",
        "    for d in dia_rttm:\n",
        "        f.write('%s\\n' % d)"
      ],
      "metadata": {
        "id": "kzMAd2OwGRUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1.4\n",
        "\n",
        "To compare the results of the automated annotations and a groundtruth, I use the Diarization Error Rate (DER), which is bounded between 0 and 1. The lower the DER, the better. I test the results using three different values of `collar`, which controls the allowed annotation error. I specify `collar`=[0.0, 0.5, 1.0]"
      ],
      "metadata": {
        "id": "OVOJdFSHHeCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in DER metric and function to load RTTM files\n",
        "from pyannote.database.util import load_rttm\n",
        "from pyannote.metrics.diarization import DiarizationErrorRate\n",
        "\n",
        "# Specify collar ranges\n",
        "collars = [0.0, 0.5, 1.0]"
      ],
      "metadata": {
        "id": "nOXK_ktBIVN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the RTTM-files\n",
        "prediction_withoutspeaker = load_rttm(os.path.join(output_dir, '57-17-prediction.rttm'))['57-17-class']\n",
        "groundtruth_withoutspeaker = load_rttm('/content/css_fall2023/data/audio/class11/57-17-groundtruth-withspeaker.rttm')['57-17-class']\n",
        "# Compute the DER\n",
        "der_dict = {}\n",
        "for collar in collars:\n",
        "  print(f\"Error margin: {collar} seconds\")\n",
        "  metric = DiarizationErrorRate(collar=collar, skip_overlap=True)\n",
        "  der = metric(groundtruth_withoutspeaker, prediction_withoutspeaker, detailed=True)\n",
        "  der_dict[str(collar)] = der"
      ],
      "metadata": {
        "id": "sRQWx1fXIoEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DER results to a pandas dataframe to represent it as a table\n",
        "import pandas as pd\n",
        "der_df = pd.DataFrame(der_dict)\n",
        "der_df"
      ],
      "metadata": {
        "id": "3tvPRPcS7bVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The overall diarization error is very low across each of the three error margins ranging from 6.53% to 2.99%f when using an error margin of $1.0$ seconds compared to no margin at all ($0.0$ seconds). The differences arises almost entirely due to differences in the false alarm. This means that the annotations assign portions of the signal as a speech while in fact is not.\n",
        "\n",
        "Based on the small differences, we expect minor differences in the estimation of acoustic features."
      ],
      "metadata": {
        "id": "0yE--GrAJfSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1.5\n",
        "\n",
        "I now split the diarized segments into individual audio files based on the generated timestamps.\n"
      ],
      "metadata": {
        "id": "SQt76JS6LKth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function that splits an audio file based on provided timestamps in a\n",
        "# RTTM file. Note that each segments must be formatted exactly as a RTTM file\n",
        "# work due to the indexing.\n",
        "def audio_split(segments:list, segment_dir:str, channels=1, sr=16000):\n",
        "\n",
        "  for i, d in enumerate(segments):\n",
        "\n",
        "    segment_fpath = os.path.join(segment_dir, f\"segment_{i}-{d[7]}.wav\")\n",
        "    cmd = ['ffmpeg',\n",
        "            '-y',\n",
        "            '-i',\n",
        "            audio_fpath,\n",
        "            '-vn',\n",
        "            '-ar',\n",
        "            str(sr),\n",
        "            '-ac',\n",
        "            str(channels),\n",
        "            '-acodec',\n",
        "            'pcm_s16le',\n",
        "            '-ss',\n",
        "            str(d[3]),\n",
        "            '-t',\n",
        "            str(d[4]),\n",
        "            segment_fpath]\n",
        "\n",
        "    subprocess.call(cmd)"
      ],
      "metadata": {
        "id": "APDolmWwVU7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess diarization list to be used to split audio file into its individual segments\n",
        "dia_rttm_split = [x.split() for x in dia_rttm]"
      ],
      "metadata": {
        "id": "y3rH5GxqSdAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct segments from diarization output\n",
        "segment_dir = os.path.join(output_dir, 'segments_diarized')\n",
        "if not os.path.exists(segment_dir):\n",
        "  os.mkdir(segment_dir)\n",
        "\n",
        "audio_split(segments=dia_rttm_split, segment_dir=segment_dir)"
      ],
      "metadata": {
        "id": "uzwN-y6WULNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1.6\n",
        "\n",
        "I now do the same for the groundtruth segments. I first read in the groundtruth RTTM file and preprocess to the same format."
      ],
      "metadata": {
        "id": "IapeivyXUd0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in groundtruth RTTM and preprocess it to match the diarization output used above\n",
        "with open('/content/css_fall2023/data/audio/class11/57-17-groundtruth-withspeaker.rttm') as f:\n",
        "  lines = f.read()\n",
        "  lines = lines.split('\\n')\n",
        "  lines = [line for line in lines if len(line) > 0]\n",
        "  groundtruth_rttm = [x.split() for x in lines]"
      ],
      "metadata": {
        "id": "w9WMzhxqUx_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct segments from groundtruth\n",
        "segment_dir = os.path.join(output_dir, 'segments_groundtruth')\n",
        "if not os.path.exists(segment_dir):\n",
        "  os.mkdir(segment_dir)\n",
        "\n",
        "audio_split(segments=groundtruth_rttm, segment_dir=segment_dir)"
      ],
      "metadata": {
        "id": "lfJFPuo8Uu_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groundtruth_rttm[11], dia_rttm[11]"
      ],
      "metadata": {
        "id": "cYa_RJvnH_On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1.7\n",
        "\n",
        "I now compute a range of acoustic features using `praat-parselmouth` for each of the diarized and groundtruth segments."
      ],
      "metadata": {
        "id": "1AXex1q3WsO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praat-parselmouth"
      ],
      "metadata": {
        "id": "eb5RuH1eXLZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diarized_dir = os.path.join(output_dir, 'segments_diarized')\n",
        "groundtruth_dir = os.path.join(output_dir, 'segments_groundtruth')\n",
        "segments_diarized = os.listdir(diarized_dir)\n",
        "segments_groundtruth = os.listdir(groundtruth_dir)"
      ],
      "metadata": {
        "id": "OvBwSzsHXgbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def digit_sort_key(v):\n",
        "\n",
        "    digits = re.search('\\d+', v)\n",
        "    return int(digits.group())\n",
        "\n",
        "# Sort segments by segment number\n",
        "segments_diarized = sorted(segments_diarized, key=digit_sort_key)\n",
        "segments_groundtruth = sorted(segments_groundtruth, key=digit_sort_key)"
      ],
      "metadata": {
        "id": "Hu7JKNmA899E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segments_groundtruth[11], segments_diarized[11]"
      ],
      "metadata": {
        "id": "-_i-pNyoHvoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify that we have an equal number of segments:\n",
        "assert len(segments_diarized) == len(segments_groundtruth)"
      ],
      "metadata": {
        "id": "L8_WrikA9dbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import parselmouth\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "PNKXjUSCgwku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def praat_acoustics(fname):\n",
        "    snd = parselmouth.Sound(fname)\n",
        "    pitch = parselmouth.praat.call(snd, \"To Pitch\", 0.0, 75, 500)\n",
        "    f0_mean = parselmouth.praat.call(pitch, \"Get mean\", 0, 0, 'Hertz')\n",
        "    f0_std = parselmouth.praat.call(pitch, \"Get standard deviation\", 0 ,0, 'Hertz')\n",
        "    f0_max = parselmouth.praat.call(pitch, \"Get maximum\", 0, 0, 'Hertz',\"None\")\n",
        "    f0_min = parselmouth.praat.call(pitch, \"Get minimum\", 0, 0, 'Hertz',\"None\")\n",
        "    mean_loudness = np.mean(snd.to_intensity().values)\n",
        "    mfccs = np.mean(snd.to_mfcc(10).to_array(), axis=1)\n",
        "\n",
        "    acoustics = {'f0_mean': f0_mean,\n",
        "                 'f0_std': f0_std,\n",
        "                 'f0_max': f0_max,\n",
        "                 'f0_min': f0_min,\n",
        "                 'loudness_mean': mean_loudness}\n",
        "\n",
        "    for i, mfcc in enumerate(mfccs[1:]):\n",
        "      acoustics[f'MFCC{i+1}'] = mfcc\n",
        "\n",
        "    return acoustics"
      ],
      "metadata": {
        "id": "604mjxzaAIrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diarized_acoustics = {}\n",
        "for s in [x for x in segments_diarized if x.endswith('wav')][:]:\n",
        "  ac = praat_acoustics(fname=os.path.join(diarized_dir, s))\n",
        "  diarized_acoustics[s] = ac"
      ],
      "metadata": {
        "id": "VLJrdnrAeR78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groundtruth_acoustics = {}\n",
        "for s in [x for x in segments_groundtruth if x.endswith('wav')][:]:\n",
        "  ac = praat_acoustics(fname=os.path.join(groundtruth_dir, s))\n",
        "  groundtruth_acoustics[s] = ac"
      ],
      "metadata": {
        "id": "J4AQwv-P_cJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1.8\n",
        "\n",
        "I have now computed the a bunch of acoustic features and saved them in two dictionaries, one for the diarized segments and one for the groundtruths. The next task is to compare the acoustic features.\n",
        "\n",
        "This needs to be done pairwise such that *one* diarized segment is matched to *one* groundtruth segment."
      ],
      "metadata": {
        "id": "7GpxYuI4EtDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate segment number\n",
        "segment_number = pd.Series(list(range(0, len(segments_diarized))), name='segment_number')\n",
        "\n",
        "# Construct dataframes with diarized acoustics and groundtruth acoustics\n",
        "diarized_df = pd.DataFrame(diarized_acoustics).transpose().reset_index(names='segment')\n",
        "diarized_df = pd.concat([segment_number, diarized_df], axis=1)\n",
        "\n",
        "groundtruth_df = pd.DataFrame(groundtruth_acoustics).transpose().reset_index(names='segment')\n",
        "groundtruth_df = pd.concat([segment_number, groundtruth_df], axis=1)"
      ],
      "metadata": {
        "id": "eJFXDX0k-XtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diarized_df.iloc[9:12]"
      ],
      "metadata": {
        "id": "DVhjlWzjHgWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groundtruth_df.iloc[9:12]"
      ],
      "metadata": {
        "id": "oIrZHEViISzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute pairwise differences for each segment and for each feature.\n",
        "# Note that we don't have to loop through each segment as Python\n",
        "# maps each index the two dataframes under the hood.\n",
        "features = diarized_df.columns[2:]\n",
        "acoustic_pairwise = {}\n",
        "for f in features:\n",
        "  pairwise_diff = np.abs(groundtruth_df[f] - diarized_df[f])\n",
        "  acoustic_pairwise[f] = pairwise_diff\n",
        "\n",
        "# Convert dict to dataframe\n",
        "acoustic_pairwise_df = pd.DataFrame(acoustic_pairwise)"
      ],
      "metadata": {
        "id": "Gnty9CUxFhcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect results\n",
        "acoustic_pairwise_df"
      ],
      "metadata": {
        "id": "lR7MHpEzacjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataframe `acoustic_pairwise_df` contains the pairwise differences in the acoustic features measured in natural units. That is, the differences in the mean $F0$ (`f0_mean`) is in Hertz, while the differences in mean loudness (`loudness_mean`) is in decibel (dB).\n",
        "\n",
        "The initial inspection reveals that the differences seems negligble suggesting that the automated annotations accurately recover the groundtruth results. There is one notable difference, which is caused for ther 12th segment (index 11). This corresponds to:\n",
        "- `/content/output/segments_diarized/segment_11-SPEAKER_03.wav`\n",
        "- `/content/output/segments_groundtruth/segment_11-TraceyCrouch.wav`\n",
        "\n",
        "To see why this difference occurs, I first manually listen to each of the segments and check the timestamps for the diarized and groundtruth annotations. Both checks suggest that the difference between the two segments is minor, yet we observe a substantial difference in the average $F0$. To see what's going on, I represent each of the two segments as a spectogram and project estimates of the $F0$ onto the plot."
      ],
      "metadata": {
        "id": "qJDDa01vamuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def draw_spectrogram(spectrogram, start=0, stop=10000, dynamic_range=70, cmap='afmhot'):\n",
        "    \"\"\"\n",
        "    Draw a spectrogram using Matplotlib.\n",
        "\n",
        "    Parameters:\n",
        "    spectrogram (Spectrogram): The input spectrogram.\n",
        "    start (float, optional): The start time in milliseconds. Defaults to 0.\n",
        "    stop (float, optional): The stop time in milliseconds. Defaults to 10000.\n",
        "    dynamic_range (float, optional): The dynamic range in decibels. Defaults to 70.\n",
        "    cmap (str, optional): The colormap to use. Defaults to 'afmhot'.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    X, Y = spectrogram.x_grid(), spectrogram.y_grid()\n",
        "    indices = np.where((X >= start) & (X <= stop))[0]\n",
        "    X = X[indices]\n",
        "    sg_db = 10 * np.log10(spectrogram.values)\n",
        "    sg_db = sg_db[:,indices[:-1]]\n",
        "    plt.pcolormesh(X, Y, sg_db, vmin=sg_db.max() - dynamic_range, cmap=cmap)\n",
        "    plt.ylim([spectrogram.ymin, spectrogram.ymax])\n",
        "\n",
        "\n",
        "def draw_pitch(pitch):\n",
        "    \"\"\"\n",
        "    Draw a pitch contour using Matplotlib.\n",
        "\n",
        "    Parameters:\n",
        "    pitch (Pitch): The input pitch object.\n",
        "\n",
        "    Returns:\n",
        "    Tuple[float, float]: A tuple containing the mean F0 and standard deviation of F0.\n",
        "    \"\"\"\n",
        "    pitch_values = pitch.selected_array['frequency']\n",
        "    f0mean = np.mean(pitch_values[pitch_values > 0])\n",
        "    f0std = np.std(pitch_values[pitch_values > 0])\n",
        "    pitch_values[pitch_values==0] = np.nan\n",
        "\n",
        "    plt.plot(pitch.xs(), pitch_values, 'o', markersize=5, color='#381a61')\n",
        "    plt.plot(pitch.xs(), pitch_values, 'o', markersize=2, color='#e78429')\n",
        "\n",
        "    plt.grid(False)\n",
        "    plt.ylim(0, pitch.ceiling)\n",
        "\n",
        "    return np.round(f0mean, 2), np.round(f0std, 2)\n",
        "\n",
        "\n",
        "def draw_spectrogram_pitch(files:list,\n",
        "                           start=None,\n",
        "                           stop=None,\n",
        "                           max_freq = 8000,\n",
        "                           cmap='afmhot',\n",
        "                           show=True):\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Draw spectrograms and pitch contours for a list of audio files.\n",
        "\n",
        "    Parameters:\n",
        "    files (list): List of audio files.\n",
        "    gender (str): Gender of the speaker.\n",
        "    dataframe (DataFrame): Dataframe containing speaker information.\n",
        "    start (float, optional): The start time in seconds. If not provided, it's determined from the audio data. Defaults to None.\n",
        "    stop (float, optional): The stop time in seconds. If not provided, it's determined from the audio data. Defaults to None.\n",
        "    max_freq (float, optional): The maximum frequency for the spectrogram. Defaults to 6000.\n",
        "    cmap (str, optional): The colormap to use. Defaults to 'afmhot'.\n",
        "    show (bool, optional): Whether to display the plots. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    n_plots = len(files)\n",
        "    num_rows = (n_plots + 1) // 2\n",
        "    num_cols = n_plots // num_rows\n",
        "\n",
        "    for c, f in enumerate(files):\n",
        "        snd = parselmouth.Sound(f)\n",
        "\n",
        "        if not start and not stop:\n",
        "          start, stop = snd.xmin, snd.xmax\n",
        "\n",
        "        pitch = snd.to_pitch(pitch_floor=75, pitch_ceiling=500)\n",
        "        spectrogram = snd.to_spectrogram(window_length=0.025, maximum_frequency=max_freq)\n",
        "\n",
        "        plt.subplot(num_rows, num_cols, c + 1)\n",
        "\n",
        "        draw_spectrogram(spectrogram, start=start, stop=stop, cmap=cmap)\n",
        "\n",
        "        if c % 2 == 0:\n",
        "          plt.ylabel('Frequency (Hz)')\n",
        "\n",
        "        if c % 2 == 1:\n",
        "          plt.yticks([])\n",
        "\n",
        "        plt.xlabel('Time (s)')\n",
        "        plt.xticks([start, stop])\n",
        "\n",
        "\n",
        "        plt.twinx()\n",
        "        f0mean, f0std = draw_pitch(pitch)\n",
        "\n",
        "        if c % 2 == 0:\n",
        "          plt.yticks([])\n",
        "\n",
        "        if c % 2 == 1:\n",
        "          plt.ylabel('F0 (Hz)')\n",
        "\n",
        "\n",
        "        plt.xlim([start, stop])\n",
        "        plt.title(f\"F0 mean={np.round(f0mean, 3)}, F0 std={np.round(f0std, 3)}\")\n",
        "\n",
        "    if show:\n",
        "      plt.show()\n"
      ],
      "metadata": {
        "id": "EHk31BDogu_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "draw_spectrogram_pitch(files=[os.path.join(diarized_dir, segments_diarized[11]),\n",
        "                              os.path.join(groundtruth_dir, segments_groundtruth[11])],\n",
        "                       cmap='viridis')"
      ],
      "metadata": {
        "id": "N2cZh224gx1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plots show that the differences arises due to high pitch estimates in the very start of the groundtruth segment. This can not be heard in the audio signal, but due to the short duration of the segment, they end up contributing with fairly high amount to the overall pitch estimate for the groundtruth segment.\n",
        "\n",
        "I now move on to the average differences for each feature. I compute both an unweighted and weighted average. The unweighted average pools the difference for each segment together without further ado while the weighted average take the varying durations into account. I construct the weights such that each segment's contribution is proportion to the segment's share of total duration."
      ],
      "metadata": {
        "id": "bzuhJZyRgpyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute unweighted means\n",
        "unweighted_mean = np.mean(acoustic_pairwise_df, axis=0)"
      ],
      "metadata": {
        "id": "eM9QPCuxOk-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weighted mean with weights corresponding to each segment's share of total duration\n",
        "dur = [float(x[4]) for x in groundtruth_rttm]\n",
        "dur_share = [x/sum(dur) for x in dur]\n",
        "weighted_mean = acoustic_pairwise_df.apply(lambda x: x * dur_share)\n",
        "weighted_mean = np.sum(weighted_mean, axis=0)"
      ],
      "metadata": {
        "id": "PCxwCiByOO_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weighted_df = pd.DataFrame(weighted_mean, columns=['weighted'])\n",
        "unweighted_df = pd.DataFrame(unweighted_mean, columns=['unweighted'])\n",
        "pairwise_df = pd.concat([unweighted_df, weighted_df], axis=1)\n",
        "# pairwise_df = pairwise_df[:4]"
      ],
      "metadata": {
        "id": "jHnJe41YcX_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect results\n",
        "pairwise_df"
      ],
      "metadata": {
        "id": "TLLn9qmmd7yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pairwise comparisons show very minor differences between the acoustic results obtained using automated annotations compared to the manually created groundtruth. This suggests that automated annotations can be efficiencely used to compute acoustic features with high validity.\n",
        "\n",
        "This holds both for the unweighted and weighted values. The latter is generally smaller than the former. This makes intuitive sense as shorter segments are more sensitive to annotation errors than longer segments since each estimate contributes more to the\n",
        "\n",
        "To illustrate the results, I plot the average differences for the $F0$ features below."
      ],
      "metadata": {
        "id": "OaTCo0O2eBUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pairwise_df = pairwise_df[:4]\n",
        "fig, ax = plt.subplots(figsize=(10,6), facecolor = \"white\")\n",
        "\n",
        "ax.grid(which=\"major\", axis='both', color='#758D99', alpha=0.6, zorder=1)\n",
        "\n",
        "ax.spines[['top','right','bottom']].set_visible(False)\n",
        "\n",
        "ax.hlines(y=pairwise_df.index, xmin=pairwise_df['weighted'], xmax=pairwise_df['unweighted'], color='#758D99', zorder=2, linewidth=2, label='_nolegend_', alpha=.8)\n",
        "ax.scatter(pairwise_df['unweighted'], pairwise_df.index, label='Unweighted', s=60, color='#DB444B', zorder=3, alpha=.7)\n",
        "ax.scatter(pairwise_df['weighted'], pairwise_df.index, label='Weighted', s=60, color='#006BA2', zorder=3, alpha=.7)\n",
        "\n",
        "ax.xaxis.set_tick_params(labeltop=True,\n",
        "                         labelbottom=False,\n",
        "                         bottom=False,\n",
        "                         labelsize=10,\n",
        "                         pad=-1)\n",
        "\n",
        "ax.text(0.5, 1.06, 'Hertz', transform=ax.transAxes, horizontalalignment='center', fontsize=10)\n",
        "\n",
        "ax.set_yticks(pairwise_df.index)\n",
        "ax.set_yticklabels(pairwise_df.index,     # Set labels again\n",
        "                   ha = 'left')           # Set horizontal alignment to left\n",
        "ax.yaxis.set_tick_params(pad=120,         # Pad tick labels so they don't go over y-axis\n",
        "                         labelsize=11)    # Set label size\n",
        "\n",
        "ax.legend(['Unweighted', 'Weighted'], loc=(0,1.05), ncol=2, frameon=False, handletextpad=-.1, handleheight=1)\n",
        "\n",
        "# Set xlim\n",
        "ax.set_xlim(0, 10)"
      ],
      "metadata": {
        "id": "bOKRFO-MRVAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Similarity of Speaker Embeddings\n",
        "\n",
        "In the tutorial, we saw how pretrained speaker embeddings can be used to construct speaker embeddings on a completely different set of audio files\n",
        "without any fine-tuning or adaption. While we did it visually in the tutorial, we'll exlore the similarity of embeddings using cosine similarity to test whether we can use these for speaker recognition.\n",
        "\n",
        "The audio we work with is the diarized segments from *Exercise 1*. Your task is to:\n",
        "\n",
        "1. Compute the pairwise cosine similarity between embeddings computed using a *sliding* window. You decide on the `duration` and `step` parameters. Compare the average for embeddings from same speakers and the average for embeddings from different speakers. Plot and describe your results. The plot should be a histogram colored by whether the similarity is computed on embeddings from the same or different speakers. I have provided you with a function below: `plot_histograms`\n",
        "\n",
        "2. Compute the pairwise cosine similarity between embeddings computed using a *fixed* window (specified with the `window=whole`). Plot and describe your results. The plot should be a similarity matrix (a heatmap). I have provided you with a function below: `plot_similarity_matrix`\n",
        "\n",
        "3. Discuss based on the results in 1+2 whether pretrained speaker embeddings can be exploited for speaker recognition.\n",
        "\n",
        "There are a bunch of resources that might help you in the exercise.\n",
        "\n",
        "- For plotting:\n",
        "    * https://github.com/resemble-ai/Resemblyzer/blob/master/demo_utils.py\n",
        "    * https://github.com/resemble-ai/Resemblyzer (see the cross-similarity plot)\n",
        "- For embeddings:\n",
        "    * https://huggingface.co/pyannote/embedding\n",
        "- For cosine similarity:\n",
        "    * Use the `cosine_similarity()` function from `sklearn.metrics.pairwise`\n",
        "\n",
        "Note that are multiple ways to achieve the results and yours might very well be smarter than mine. Take a look at the solution if you get stuck."
      ],
      "metadata": {
        "id": "_8_5qIEjIqOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tools for plotting\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "_default_colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
        "_my_colors = np.array([\n",
        "    [0, 127, 70],\n",
        "    [255, 0, 0],\n",
        "    [255, 217, 38],\n",
        "    [0, 135, 255],\n",
        "    [165, 0, 165],\n",
        "    [255, 167, 255],\n",
        "    [97, 142, 151],\n",
        "    [0, 255, 255],\n",
        "    [255, 96, 38],\n",
        "    [142, 76, 0],\n",
        "    [33, 0, 127],\n",
        "    [0, 0, 0],\n",
        "    [183, 183, 183],\n",
        "    [76, 255, 0],\n",
        "], dtype=float) / 255\n",
        "\n",
        "\n",
        "def plot_histograms(all_samples, names=None, title=\"\"):\n",
        "    \"\"\"\n",
        "    Plots (possibly) overlapping histograms and their median\n",
        "    \"\"\"\n",
        "\n",
        "    _, ax = plt.subplots()\n",
        "\n",
        "    for samples, color, name in zip(all_samples, _default_colors, names):\n",
        "      ax.hist(samples, density=True, color=color, label=name, alpha=0.5)\n",
        "    ax.legend(frameon=False, loc='upper right')\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_yticks([])\n",
        "    ax.set_title(title)\n",
        "\n",
        "    ylim = ax.get_ylim()\n",
        "    ax.set_ylim(*ylim)\n",
        "    for samples, color in zip(all_samples, _default_colors):\n",
        "        median = np.median(samples)\n",
        "        ax.vlines(median, *ylim, color, \"dashed\")\n",
        "        ax.text(median, ylim[1] * 0.15, \"median\", rotation=270, color=color)\n",
        "\n",
        "def plot_similarity_matrix(matrix, labels_a=None, labels_b=None, ax: plt.Axes=None, title=\"\"):\n",
        "    if ax is None:\n",
        "        _, ax = plt.subplots()\n",
        "    fig = plt.gcf()\n",
        "\n",
        "    img = ax.matshow(matrix, extent=(-0.5, matrix.shape[0] - 0.5,\n",
        "                                     -0.5, matrix.shape[1] - 0.5))\n",
        "\n",
        "    ax.xaxis.set_ticks_position(\"bottom\")\n",
        "    if labels_a is not None:\n",
        "        ax.set_xticks(range(len(labels_a)))\n",
        "        ax.set_xticklabels(labels_a, rotation=90, size=7)\n",
        "    if labels_b is not None:\n",
        "        ax.set_yticks(range(len(labels_b)))\n",
        "        ax.set_yticklabels(labels_b[::-1], size=7)  # Upper origin -> reverse y axis\n",
        "    ax.set_title(title)\n",
        "\n",
        "\n",
        "    cax = make_axes_locatable(ax).append_axes(\"right\", size=\"5%\", pad=0.15)\n",
        "    fig.colorbar(img, cax=cax, ticks=np.linspace(0.25, 1, 4))\n",
        "    img.set_clim(0.25, 1)\n",
        "    img.set_cmap(\"inferno\")"
      ],
      "metadata": {
        "id": "Wr3xzxYLkGPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping speaker labels to speaker names and gender\n",
        "# NOTE: It is not guaranteed that your labels corresponds to those below.\n",
        "#       You might need to make your own mapping. You can use the transcript\n",
        "#       '/content/css_fall2023/data/audio/class11/57-17-official_transcript.csv'\n",
        "#       for help if you don't know the speakers purely by voice.\n",
        "speaker_mapping = {'SPEAKER_09': 'Karen Bradley',\n",
        "                  'SPEAKER_06': 'David Hanson',\n",
        "                  'SPEAKER_07': 'John Whittingdale',\n",
        "                  'SPEAKER_01': 'Christine Jardine',\n",
        "                  'SPEAKER_03': 'Tracey Crouch',\n",
        "                  'SPEAKER_10': 'Wes Streeting',\n",
        "                  'SPEAKER_04': 'Amanda Milling',\n",
        "                  'SPEAKER_11': 'Chris Elmore',\n",
        "                  'SPEAKER_05': 'Nusrat Ghani',\n",
        "                  'SPEAKER_00': 'Jim Shannon',\n",
        "                  'SPEAKER_08': 'Tom Watson',\n",
        "                  'SPEAKER_12': 'John Glen',\n",
        "                  'SPEAKER_13': 'Luke Pollard',\n",
        "                  'SPEAKER_02': 'CHAIR'}\n",
        "\n",
        "gender_mapping = {'SPEAKER_09': 'Woman',\n",
        "                  'SPEAKER_06': 'Man',\n",
        "                  'SPEAKER_07': 'Man',\n",
        "                  'SPEAKER_01': 'Woman',\n",
        "                  'SPEAKER_03': 'Woman',\n",
        "                  'SPEAKER_10': 'Man',\n",
        "                  'SPEAKER_04': 'Woman',\n",
        "                  'SPEAKER_11': 'Man',\n",
        "                  'SPEAKER_05': 'Woman',\n",
        "                  'SPEAKER_00': 'Man',\n",
        "                  'SPEAKER_08': 'Man',\n",
        "                  'SPEAKER_12': 'Man',\n",
        "                  'SPEAKER_13': 'Man',\n",
        "                  'SPEAKER_02': 'Man'}"
      ],
      "metadata": {
        "id": "vPwvHGX8jqMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 2.1\n"
      ],
      "metadata": {
        "id": "kGPy1BualiOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained embedding model\n",
        "from pyannote.audio import Model\n",
        "embedding_model = Model.from_pretrained(\"pyannote/embedding\",\n",
        "                              use_auth_token=access_token)\n",
        "\n",
        "# Load Inference class\n",
        "from pyannote.audio import Inference"
      ],
      "metadata": {
        "id": "5id-MRf6jLyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We use a sliding window with a duration of 5 seconds with a 1s step.\n",
        "embedding_inference = Inference(embedding_model, window=\"sliding\", duration=5, step=1)\n",
        "\n",
        "# Define list of segment filepaths\n",
        "segment_fpaths = list(segments_diarized)\n",
        "\n",
        "# Define empty list to store the embeddings\n",
        "embeddings = []\n",
        "\n",
        "# Define empty lists to store speaker and gender labels\n",
        "speaker_labels, speaker_gender = [], []\n",
        "\n",
        "# Loop through each segment\n",
        "for ix, segment_fpath in enumerate(segment_fpaths):\n",
        "  # Compute embedding for each segment and convert to numpy array\n",
        "  embed = np.array(embedding_inference(os.path.join(diarized_dir, segment_fpath)))\n",
        "  # embed = embed / np.linalg.norm(embed)\n",
        "\n",
        "  # Concatenate with previous embeddings\n",
        "  if len(embeddings) > 0:\n",
        "    embeddings = np.concatenate([embeddings, embed])\n",
        "  else:\n",
        "    embeddings = np.concatenate([embed,])\n",
        "\n",
        "  # Generate speaker and gender labels\n",
        "  segment_speaker = segment_fpath.split('-')[-1].split('.')[0]\n",
        "  speaker_labels += [speaker_mapping[segment_speaker]] * embed.shape[0]\n",
        "  speaker_gender += [gender_mapping[segment_speaker]] * embed.shape[0]\n"
      ],
      "metadata": {
        "id": "2aFqeEypjGX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "embeddings0 = embeddings[:]\n",
        "embeddings1 = embeddings[:].T\n",
        "\n",
        "sim_matrix = cosine_similarity(embeddings0, embeddings1.T)\n",
        "\n",
        "# Create a dictionary to store the indices for each name\n",
        "speaker_group = {}\n",
        "\n",
        "for ix, sp in enumerate(speaker_labels):\n",
        "    if sp not in speaker_group:\n",
        "        speaker_group[sp] = [ix]\n",
        "    else:\n",
        "        speaker_group[sp].append(ix)\n",
        "\n",
        "# Print the indices for each name group\n",
        "for sp, ix in speaker_group.items():\n",
        "    print(f\"{sp}: {ix}\")"
      ],
      "metadata": {
        "id": "XaP288TikKtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices = list(range(0, len(sim_matrix)))\n",
        "sim_same_list = []\n",
        "sim_diff_list = []\n",
        "\n",
        "for ix, val in enumerate(speaker_group):\n",
        "  same_indices = speaker_group[val]\n",
        "  same_values = sim_matrix[np.min(same_indices)][same_indices]\n",
        "\n",
        "  diff_indices = list(set(indices).difference(same_indices))\n",
        "  diff_values = sim_matrix[np.min(same_indices)][diff_indices]\n",
        "\n",
        "  sim_same_list += [same_values]\n",
        "  sim_diff_list += [diff_values]\n",
        "\n",
        "sim_same_list = np.concatenate(sim_same_list)\n",
        "sim_diff_list = np.concatenate(sim_diff_list)"
      ],
      "metadata": {
        "id": "jriGB4I3kYzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_histograms((sim_same_list, sim_diff_list), [\"Same speaker\", \"Different speakers\"])"
      ],
      "metadata": {
        "id": "jmIw8VcIkbvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot shows that the cosine similarity between embeddings from the same speakers compared to different speakers, on average, are much more similar. The median is almost $0.6$ for same speakers while around $0.17$ for different speakers. While the distributions have a minor overlap, this is negligible. The embeddings are computed on 5 seconds windows meaning that noise and ``bad variation'' is expected."
      ],
      "metadata": {
        "id": "XdCSLpUynNxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 2.2"
      ],
      "metadata": {
        "id": "LA2YtcsnkrRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use a sliding window with a duration of 1.6 seconds with a 0.2s step.\n",
        "embedding_inference = Inference(embedding_model, window=\"whole\")\n",
        "\n",
        "# Define list of segment filepaths\n",
        "segment_fpaths = list(segments_diarized)\n",
        "\n",
        "# Define empty list to store the embeddings\n",
        "embeddings = []\n",
        "\n",
        "# Define empty lists to store speaker and gender labels\n",
        "speaker_labels, speaker_gender = [], []\n",
        "\n",
        "# Loop through each segment\n",
        "for ix, segment_fpath in enumerate(segment_fpaths[:]):\n",
        "\n",
        "  # Compute embedding for each segment and convert to numpy array\n",
        "  embed = np.array(embedding_inference(os.path.join(diarized_dir, segment_fpath)))\n",
        "\n",
        "  # Append\n",
        "  embeddings.append(embed)\n",
        "\n",
        "  # Generate speaker and gender labels\n",
        "  segment_speaker = segment_fpath.split('-')[-1].split('.')[0]\n",
        "  speaker_labels += [speaker_mapping[segment_speaker]]\n",
        "  speaker_gender += [gender_mapping[segment_speaker]]\n"
      ],
      "metadata": {
        "id": "C6XTY5Rakqb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_sim_matrix = cosine_similarity(np.array(embeddings), np.array(embeddings))\n",
        "plot_similarity_matrix(cross_sim_matrix, labels_a=speaker_labels, labels_b=speaker_labels)"
      ],
      "metadata": {
        "id": "9NMxPB0Cl_Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like the distributions in *Exercise 2.1*, the similarity matrix shows a similar pattern. Embeddings from same speakers are much more similar than embeddings from different speakers. This is visually presented with bright colors showing a higher cosine similarity between embeddings computed on the entire segments."
      ],
      "metadata": {
        "id": "KZrBVI6RoHU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 2.3\n",
        "\n",
        "Based on the results in *2.1* and *2.2*, pretrained speaker embeddings is a promising approach to speaker recognition. Despite that they are trained on a different population of speakers, the pretrained model are still able to generate embeddings that encode each speaker's unique voice characteristics. The results show 1) that embeddings from the same speaker have a high similarity and 2) that embeddings from different speakers have a substantially lower similarity. Taken together, this means the pretrained embeddings are able to recognize speakers while at the same time distinguishing between different speakers.\n"
      ],
      "metadata": {
        "id": "AiJV0uWzobXS"
      }
    }
  ]
}