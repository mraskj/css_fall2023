{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPDeWzDuK3IRcT3OsdJPbVM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mraskj/css_fall2023/blob/main/code/class11/class11-exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class 11: Speaker Diarization and Recognition - Exercise"
      ],
      "metadata": {
        "id": "G2F540X2lqh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, we investigate the validity of using pretrained models to conduct speaker diarization and speaker recognition. For the former, we compare acoustic features computed with groundtruth and automated timestamps. For the latter, we investigate how embeddings can be used to discriminate between speakers.  \n",
        "\n",
        "As we always do in Colab, you should start by cloning the GitHub repo and by constructing an output folder to save files you create in Colab."
      ],
      "metadata": {
        "id": "cmMzwffYk3Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone GitHub directory into\n",
        "!git clone https://github.com/mraskj/css_fall2023.git"
      ],
      "metadata": {
        "id": "V9oE7XE8EfdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define an output directory where we save all output created in Colab\n",
        "output_dir = os.path.join(os.getcwd(), 'output')\n",
        "\n",
        "# Make directory if it not already exists.\n",
        "if not os.path.exists(output_dir):\n",
        "  os.mkdir(output_dir)"
      ],
      "metadata": {
        "id": "LIMvaQXFFOLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Audio Measurement and Annotations\n",
        "\n",
        "In the first exercise, we investigate the sensitivity of annotation errors to the estimation of acoustic features. I have provided you with a RTTM groundtruth annotations for the same debate snippet as we worked with in the tutorial. The RTTM file can be found here `/content/css_fall2023/data/audio/class11/57-17-groundtruth-withoutspeaker.rttm` (assuming you have cloned the GitHub repo). Note that we work with the version without speaker labels here. You can also work with version with speaker labels if you want - the results should be similar.\n",
        "\n",
        "The recording of the debate snippet is found here: `/content/css_fall2023/data/audio/class11/57-71-class.mp4`.\n",
        "\n",
        "1. Convert the video file to audio\n",
        "\n",
        "2. Apply diarization using the `pyannote/speaker-diarization@2.1` model from `pyannote.audio`. Describe each step you take (e.g. do you keep all diarized segments or do you discard some? Do you merge back-to-back segments from the same speaker label).\n",
        "\n",
        "3. Write the diarization result to a RTTM file.\n",
        "\n",
        "\n",
        "4. Compute the DER for three different error margins: 0.0, 0.5, and 1.0. Describe the results. Based on this, do you expect substantial differences in the estimation of acoustic features such as pitch, loudness, or MFCCs when using groundtruth annotations compared to the automated annotations?\n",
        "\n",
        "\n",
        "5. Split the diarized segments to separate audio files\n",
        "\n",
        "\n",
        "6. Split the groundtruth segments to separate audio files\n",
        "\n",
        "\n",
        "7. Compute the first 10 MFCCs, pitch, and intensity for each of the segments from step 4 and 5. For pitch, compute also the standard deviation, minimum, and maximum value.\n",
        "\n",
        "8. Compare the measure for the diarized and groundtruth segments. Note that you must link each diarization segment to a groundtruth segment to be able to do the comparison. There must be exactly the same number of segments in both conditions. Describe and show the results.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fS2o_6yNFQ2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Similarity of Speaker Embeddings\n",
        "\n",
        "In the tutorial, we saw how pretrained speaker embeddings can be used to construct speaker embeddings on a completely different set of audio files\n",
        "without any fine-tuning or adaption. While we did it visually in the tutorial, we'll exlore the similarity of embeddings using cosine similarity to test whether we can use these for speaker recognition.\n",
        "\n",
        "The audio we work with is the diarized segments from *Exercise 1*. Your task is to:\n",
        "\n",
        "1. Compute the pairwise cosine similarity between embeddings computed using a *sliding* window. You decide on the `duration` and `step` parameters. Compare the average for embeddings from same speakers and the average for embeddings from different speakers. Plot and describe your results. The plot should be a histogram colored by whether the similarity is computed on embeddings from the same or different speakers. I have provided you with a function below: `plot_histograms`\n",
        "\n",
        "2. Compute the pairwise cosine similarity between embeddings computed using a *fixed* window (specified with the `window=whole`). Plot and describe your results. The plot should be a similarity matrix (a heatmap). I have provided you with a function below: `plot_similarity_matrix`\n",
        "\n",
        "3. Discuss based on the results in 1+2 whether pretrained speaker embeddings can be exploited for speaker recognition.\n",
        "\n",
        "There are a bunch of resources that might help you in the exercise.\n",
        "\n",
        "- For plotting:\n",
        "    * https://github.com/resemble-ai/Resemblyzer/blob/master/demo_utils.py\n",
        "    * https://github.com/resemble-ai/Resemblyzer (see the cross-similarity plot)\n",
        "- For embeddings:\n",
        "    * https://huggingface.co/pyannote/embedding\n",
        "- For cosine similarity:\n",
        "    * Use the `cosine_similarity()` function from `sklearn.metrics.pairwise`\n",
        "\n",
        "Note that are multiple ways to achieve the results and yours might very well be smarter than mine. Take a look at the solution if you get stuck."
      ],
      "metadata": {
        "id": "_8_5qIEjIqOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tools for plotting\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "_default_colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
        "_my_colors = np.array([\n",
        "    [0, 127, 70],\n",
        "    [255, 0, 0],\n",
        "    [255, 217, 38],\n",
        "    [0, 135, 255],\n",
        "    [165, 0, 165],\n",
        "    [255, 167, 255],\n",
        "    [97, 142, 151],\n",
        "    [0, 255, 255],\n",
        "    [255, 96, 38],\n",
        "    [142, 76, 0],\n",
        "    [33, 0, 127],\n",
        "    [0, 0, 0],\n",
        "    [183, 183, 183],\n",
        "    [76, 255, 0],\n",
        "], dtype=float) / 255\n",
        "\n",
        "\n",
        "def plot_histograms(all_samples, names=None, title=\"\"):\n",
        "    \"\"\"\n",
        "    Plots (possibly) overlapping histograms and their median\n",
        "    \"\"\"\n",
        "\n",
        "    _, ax = plt.subplots()\n",
        "\n",
        "    for samples, color, name in zip(all_samples, _default_colors, names):\n",
        "      ax.hist(samples, density=True, color=color, label=name, alpha=0.5)\n",
        "    ax.legend(frameon=False, loc='upper right')\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_yticks([])\n",
        "    ax.set_title(title)\n",
        "\n",
        "    ylim = ax.get_ylim()\n",
        "    ax.set_ylim(*ylim)\n",
        "    for samples, color in zip(all_samples, _default_colors):\n",
        "        median = np.median(samples)\n",
        "        ax.vlines(median, *ylim, color, \"dashed\")\n",
        "        ax.text(median, ylim[1] * 0.15, \"median\", rotation=270, color=color)\n",
        "\n",
        "def plot_similarity_matrix(matrix, labels_a=None, labels_b=None, ax: plt.Axes=None, title=\"\"):\n",
        "    if ax is None:\n",
        "        _, ax = plt.subplots()\n",
        "    fig = plt.gcf()\n",
        "\n",
        "    img = ax.matshow(matrix, extent=(-0.5, matrix.shape[0] - 0.5,\n",
        "                                     -0.5, matrix.shape[1] - 0.5))\n",
        "\n",
        "    ax.xaxis.set_ticks_position(\"bottom\")\n",
        "    if labels_a is not None:\n",
        "        ax.set_xticks(range(len(labels_a)))\n",
        "        ax.set_xticklabels(labels_a, rotation=90, size=7)\n",
        "    if labels_b is not None:\n",
        "        ax.set_yticks(range(len(labels_b)))\n",
        "        ax.set_yticklabels(labels_b[::-1], size=7)  # Upper origin -> reverse y axis\n",
        "    ax.set_title(title)\n",
        "\n",
        "\n",
        "    cax = make_axes_locatable(ax).append_axes(\"right\", size=\"5%\", pad=0.15)\n",
        "    fig.colorbar(img, cax=cax, ticks=np.linspace(0.25, 1, 4))\n",
        "    img.set_clim(0.25, 1)\n",
        "    img.set_cmap(\"inferno\")"
      ],
      "metadata": {
        "id": "Wr3xzxYLkGPi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}