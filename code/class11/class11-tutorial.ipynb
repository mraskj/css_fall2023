{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyPY21SdB1f3KDG238SsTfvE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mraskj/css_fall2023/blob/main/code/class11/class11-tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class 11: Speech and Speaker Recognition - Tutorial\n",
        "\n",
        "In the class tutorial, we'll cover how we can use pretrained models to conduct:\n",
        "\n",
        "- Speaker diarization\n",
        "- Speaker recognition\n",
        "- Speech recognition\n",
        "\n",
        "For the purpose, we'll rely on `pyannote.audio` (https://github.com/pyannote/pyannote-audio, https://huggingface.co/pyannote) and `faster-whisper` (https://github.com/guillaumekln/faster-whisper) which is open-source libraries that achives state-of-the-art performances. The latter is a faster (as the name suggest...) implementation of OpenAI's popular model `Whisper` (https://github.com/openai/whisper).\n",
        "\n"
      ],
      "metadata": {
        "id": "7Mwg1E0qXNid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0 Setup\n",
        "\n",
        "We start by:\n",
        "\n",
        "1. Cloning the course GitHub repo\n",
        "2. Importing necessary modules\n",
        "3. Video to audio conversion\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KmZlm8YkehcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.1 Cloning GitHub Repository"
      ],
      "metadata": {
        "id": "o7be1ctVg1xH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vf8LweEaX9ri"
      },
      "outputs": [],
      "source": [
        "# Clone GitHub directory into\n",
        "!git clone https://github.com/mraskj/css_fall2023.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.2 Importing Modules"
      ],
      "metadata": {
        "id": "Nv-JBOVNhHwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For file and directory management\n",
        "import os\n",
        "\n",
        "# For shell interaction\n",
        "import subprocess\n",
        "\n",
        "# For data handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style = \"darkgrid\")"
      ],
      "metadata": {
        "id": "_JYM5ag0YUPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.3 Video to audio conversion\n",
        "\n",
        "For the tutorial and the exercise, we work with a 10 minute snippet of a debate from the UK House of Commons in December 2017. The ID of the debate is 57-17 saying that it's the 17th debate in the 57 parliamentary session. I have randomly selected a 10 minute snippet from the debate. You can find the video in the `content/css_fall2023/data/audio/class11` folder and is called `57-71-class.mp4`.\n",
        "\n",
        "When we work with audio data, we need to discard the video stream from the recording. To that we use the open-source multimodal data handler library FFMPEG (https://www.ffmpeg.org/)\n",
        "\n",
        "FFMPEG is called from the terminal. We have already seen that the\n",
        "exclamation mark '!' can be used to specify that the code should be\n",
        "interpreted as a shell command. A better Pythonic way of doing it is to use\n",
        "the `subprocess` module, which allows you to interact with your operating\n",
        "system directly from Python using Python syntax.\n",
        "\n"
      ],
      "metadata": {
        "id": "GAmgPs20hL-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an output directory where we save all output created in Colab\n",
        "output_dir = os.path.join(os.getcwd(), 'output')\n",
        "\n",
        "# Make directory if it not already exists.\n",
        "if not os.path.exists(output_dir):\n",
        "  os.mkdir(output_dir)"
      ],
      "metadata": {
        "id": "b3a4rrUsGv4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We specify the sampling rate and number of channels beforehand. This is not\n",
        "# strictly necessary, but is a useful approach when you wrap code in functions.\n",
        "sr = 16000\n",
        "channels = 1\n",
        "\n",
        "# Specify path to video and audio file file\n",
        "video_fpath = '/content/css_fall2023/data/audio/class11/57-71-class.mp4'\n",
        "audio_fpath = os.path.join(output_dir, '57-71-class.wav')\n",
        "\n",
        "# Write the ffmpeg code\n",
        "cmd = ['ffmpeg',\n",
        "       '-y',\n",
        "       '-i',\n",
        "       video_fpath,\n",
        "       '-vn',\n",
        "       '-ar',\n",
        "       str(sr),\n",
        "       '-ac',\n",
        "       str(channels),\n",
        "       '-acodec',\n",
        "       'pcm_s16le',\n",
        "       audio_fpath]\n",
        "\n",
        "# Execute the code in the shell\n",
        "subprocess.call(cmd)\n",
        "\n",
        "# -y specifies that the output file should be overwritten if already existing\n",
        "# -i specifies the input file\n",
        "# -vn omits the video stream\n",
        "# -ar specifies the sampling rate\n",
        "# -ac specifies the number of channels\n",
        "# -acodec specifies the type of encoding (in this case pcm_s16le)"
      ],
      "metadata": {
        "id": "_wuxbGYJm3OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speaker Diarization"
      ],
      "metadata": {
        "id": "rMKWqpfEJK0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To access the open-source models from `pyannote.audio`, we need to create an access token and accept the terms and conditions for using the models.\n",
        "\n",
        "See the steps in the TL;DR in the link: https://github.com/pyannote/pyannote-audio/tree/develop\n",
        "\n",
        "There are two pipelines available:\n",
        "\n",
        "- `pyannote/speaker-diarization` (version 2.1)\n",
        "- `pyannote/speaker-diarization-3.0` (version 3.0)\n",
        "\n",
        "You need to accept terms for both models if you want to use both. We'll stick to `pyannote/speaker-diarization@2.1` here.\n",
        "\n",
        "Before we can access the model, we also need to download the `pyannote.audio` module. We do that by:\n",
        "\n",
        "\n",
        "```\n",
        "# !pip install pyannote.audio\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "pk8eJDlHLQQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyannote.audio"
      ],
      "metadata": {
        "id": "V3bCliA6LXy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Pipeline from pyannote.audio\n",
        "from pyannote.audio import Pipeline"
      ],
      "metadata": {
        "id": "ArYUbGipOalU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define access token and name of pipeline\n",
        "from google.colab import userdata\n",
        "access_token = userdata.get('huggingface')\n",
        "\n",
        "pipeline_name = \"pyannote/speaker-diarization@2.1\"\n",
        "\n",
        "# Load and initiate pipeline.\n",
        "pipeline = Pipeline.from_pretrained(pipeline_name,\n",
        "                                    use_auth_token=access_token)"
      ],
      "metadata": {
        "id": "4P4S9WAwJNZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyTorch\n",
        "import torch\n",
        "\n",
        "# Send pipeline to GPU (if available)\n",
        "pipeline.to(torch.device(\"cuda\"  if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "print(f\"Device: {pipeline.device}\")"
      ],
      "metadata": {
        "id": "gd7SiUs0Kj6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the diarization pipeline to our audio file\n",
        "diarization = pipeline(os.path.join(output_dir, '57-71-class.wav'))"
      ],
      "metadata": {
        "id": "-4G_9aG-NVjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the output\n",
        "diarization"
      ],
      "metadata": {
        "id": "fA3h_ZHXO1hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the diarization result\n",
        "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")"
      ],
      "metadata": {
        "id": "27fp9kyz3nlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating diarization results always uses so-called RTTM files.\n",
        "\n",
        "RTTM stands for Rich Transcription Time Marked (RTTM) files and are space-delimited text files containing one turn per line, each line containing ten fields:\n",
        "\n",
        "- ``Type``  --  segment type; should always by ``SPEAKER``\n",
        "- ``File ID``  --  file name; basename of the recording minus extension (e.g.,\n",
        "  ``rec1_a``)\n",
        "- ``Channel ID``  --  channel (1-indexed) that turn is on; should always be\n",
        "  ``1``\n",
        "- ``Turn Onset``  --  onset of turn in seconds from beginning of recording\n",
        "- ``Turn Duration``  -- duration of turn in seconds\n",
        "- ``Orthography Field`` --  should always by ``<NA>``\n",
        "- ``Speaker Type``  --  should always be ``<NA>``\n",
        "- ``Speaker Name``  --  name of speaker of turn; should be unique within scope\n",
        "  of each file\n",
        "- ``Confidence Score``  --  system confidence (probability) that information\n",
        "  is correct; should always be ``<NA>``\n",
        "- ``Signal Lookahead Time``  --  should always be ``<NA>``\n",
        "\n",
        "For instance:\n",
        "\n",
        "    SPEAKER CMU_20020319-1400_d01_NONE 1 130.430000 2.350 <NA> <NA> juliet <NA> <NA>\n",
        "    SPEAKER CMU_20020319-1400_d01_NONE 1 157.610000 3.060 <NA> <NA> tbc <NA> <NA>\n",
        "    SPEAKER CMU_20020319-1400_d01_NONE 1 130.490000 0.450 <NA> <NA> chek <NA> <NA>"
      ],
      "metadata": {
        "id": "yoYZa6nwUjAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write output to an RTTM-file by looping through each diarized segment\n",
        "dia_result_withspeaker = []\n",
        "dia_result_withoutspeaker = []\n",
        "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "    start = round(turn.start, 1)\n",
        "    end = round(turn.end, 1)\n",
        "\n",
        "    # Keep only segments that are five seconds or longer\n",
        "    if (end - start) >= 5.0:\n",
        "\n",
        "      # Join each line as a string\n",
        "      dia_result_withspeaker.append(' '.join(['SPEAKER ' +\n",
        "                                '57-17-class' +\n",
        "                                ' 1',\n",
        "                                str(start),\n",
        "                                str(round(end-start, 1)),\n",
        "                                '<NA> <NA>',\n",
        "                                speaker,\n",
        "                                '<NA> <NA>']))\n",
        "\n",
        "      dia_result_withoutspeaker.append(' '.join(['SPEAKER ' +\n",
        "                                '57-17-class' +\n",
        "                                ' 1',\n",
        "                                str(start),\n",
        "                                str(round(end-start, 1)),\n",
        "                                '<NA> <NA>',\n",
        "                                'SPEAKER-00',\n",
        "                                '<NA> <NA>']))\n",
        "\n",
        "\n",
        "print(dia_result_withspeaker[:2])\n",
        "print(dia_result_withoutspeaker[:2])\n",
        "\n",
        "# Remove first diarized segment, which corresponds to the chair\n",
        "dia_result_withspeaker = dia_result_withspeaker[1:]\n",
        "dia_result_withoutspeaker = dia_result_withoutspeaker[1:]\n",
        "\n",
        "# Writing\n",
        "with open(os.path.join(output_dir, '57-17-prediction-withspeaker.rttm'), 'w') as f:\n",
        "    for d in dia_result_withspeaker:\n",
        "        f.write('%s\\n' % d)\n",
        "with open(os.path.join(output_dir, '57-17-prediction-withoutspeaker.rttm'), 'w') as f:\n",
        "    for d in dia_result_withoutspeaker:\n",
        "        f.write('%s\\n' % d)"
      ],
      "metadata": {
        "id": "p-2UgNQI3tGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print RTTM outputs\n",
        "dia_result_withspeaker, dia_result_withoutspeaker"
      ],
      "metadata": {
        "id": "gvD9TIbn511Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in DER metric and function to load RTTM files\n",
        "from pyannote.database.util import load_rttm\n",
        "from pyannote.metrics.diarization import DiarizationErrorRate\n",
        "\n",
        "# We specify a collar of 1.0 seconds meaning that we allow an error margin up\n",
        "# to one second before counting it as error. This takes annotation variability\n",
        "# into account\n",
        "metric = DiarizationErrorRate(collar=1.0, skip_overlap=True)"
      ],
      "metadata": {
        "id": "yW-Ou2a8uUGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the RTTM-files using the `load_rttm` function from the pyannote module\n",
        "prediction_withoutspeaker = load_rttm(os.path.join(output_dir, '57-17-prediction-withoutspeaker.rttm'))['57-17-class']\n",
        "groundtruth_withoutspeaker = load_rttm('/content/css_fall2023/data/audio/class11/57-17-groundtruth-withoutspeaker.rttm')['57-17-class']\n",
        "\n",
        "# Compute the DER\n",
        "der_withoutspeaker = metric(groundtruth_withoutspeaker, prediction_withoutspeaker, detailed=True)\n",
        "der_withoutspeaker"
      ],
      "metadata": {
        "id": "B2akQTiS1qGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the RTTM-files using the `load_rttm` function from the pyannote module\n",
        "prediction_withspeaker = load_rttm(os.path.join(output_dir, '57-17-prediction-withspeaker.rttm'))['57-17-class']\n",
        "groundtruth_withspeaker =  load_rttm('/content/css_fall2023/data/audio/class11/57-17-groundtruth-withspeaker.rttm')['57-17-class']\n",
        "\n",
        "# Compute the DER\n",
        "der_withspeaker = metric(groundtruth_withspeaker, prediction_withspeaker, detailed=True)\n",
        "der_withspeaker"
      ],
      "metadata": {
        "id": "Gf-ZR63k5VRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we get the same DER. This happens since pyannote automatically generates a speaker label mapping under the hood. When we deliberately mixes things up, we see that the confusion happens."
      ],
      "metadata": {
        "id": "uP_9csEW61ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We reload the modified versions\n",
        "prediction_withspeaker = load_rttm(os.path.join(output_dir, '57-17-prediction-withspeaker.rttm'))['57-17-class']\n",
        "groundtruth_withspeaker =  load_rttm('/content/css_fall2023/data/audio/class11/57-17-groundtruth-withspeaker.rttm')['57-17-class']\n",
        "\n",
        "# Recompute the DER\n",
        "der_withspeaker = metric(groundtruth_withspeaker, prediction_withspeaker, detailed=True)\n",
        "der_withspeaker"
      ],
      "metadata": {
        "id": "oQKZrRoIwECl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting Audio Files\n",
        "\n",
        "We often encounter situations where we want to split an audio recording into smaller segments, for instance timestamps that denote the start and end time of each speech. For this we use FFMPEG again. This time, we add arguments for the start and duration of our desired segment."
      ],
      "metadata": {
        "id": "Gfzn-6vN8ugz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split diarization output on whitespace\n",
        "dia_result = [x.split() for x in dia_result_withspeaker]"
      ],
      "metadata": {
        "id": "duv0vVoj8zHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sr = 16000\n",
        "channels = 1\n",
        "\n",
        "segment_dir = os.path.join(output_dir, 'segments')\n",
        "if not os.path.exists(segment_dir):\n",
        "  os.mkdir(segment_dir)\n",
        "\n",
        "# Loop through each segment\n",
        "for i, d in enumerate(dia_result):\n",
        "\n",
        "  segment_fpath = os.path.join(segment_dir, f\"segment_{i}-{d[7]}.wav\")\n",
        "  cmd = ['ffmpeg',\n",
        "           '-y',\n",
        "           '-i',\n",
        "           audio_fpath,\n",
        "           '-vn',\n",
        "           '-ar',\n",
        "           str(sr),\n",
        "           '-ac',\n",
        "           str(channels),\n",
        "           '-acodec',\n",
        "           'pcm_s16le',\n",
        "           '-ss',\n",
        "           str(d[3]),\n",
        "           '-t',\n",
        "           str(d[4]),\n",
        "           segment_fpath]\n",
        "\n",
        "  subprocess.call(cmd)"
      ],
      "metadata": {
        "id": "uEC-j0dj8t8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speaker Recognition with Speaker Embeddings\n",
        "\n",
        "The most common way conduct speaker recognition is through supervised learning. However, speaker embeddings computed on diarized segments can be very effective as well. We'll explore this now.\n",
        "\n",
        "We use a pretrained embedding model from `pyannote.audio` again. Once again, you need to have access to the model in the same way as for the diarization."
      ],
      "metadata": {
        "id": "GRC_D4tVA4dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define list of segments\n",
        "segments = os.listdir(segment_dir)"
      ],
      "metadata": {
        "id": "lyxYeNpF_G-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split segment in segment name and speaker label\n",
        "segment_split = [x.split('.')[0].split('-') for x in segments]\n",
        "\n",
        "# Construct a two-column pandas dataframe\n",
        "segment_df = pd.DataFrame(segment_split, columns=['segment', 'speaker_label'])\n",
        "\n",
        "# Add column denoting the path to each segment\n",
        "segment_df['segment_fpath'] = [os.path.join(segment_dir, x) for x in segments]\n",
        "\n",
        "# Add column with the number of each segment\n",
        "segment_df['segment_number'] = segment_df['segment'].apply(lambda x: int(x.split('_')[1]))\n",
        "\n",
        "# Sort values by segment number to get temporal order\n",
        "segment_df = segment_df.sort_values(by='segment_number').reset_index(drop=True)\n",
        "\n",
        "# Add timestamps to the dataframe\n",
        "segment_df['start'] = [float(x[3]) for x in dia_result]\n",
        "segment_df['dur'] = [float(x[4]) for x in dia_result]\n",
        "segment_df['end'] = segment_df['start'] + segment_df['dur']\n",
        "\n",
        "speaker_mapping = {'SPEAKER_09': 'Karen Bradley',\n",
        " 'SPEAKER_06': 'David Hanson',\n",
        " 'SPEAKER_07': 'John Whittingdale',\n",
        " 'SPEAKER_01': 'Christine Jardine',\n",
        " 'SPEAKER_03': 'Tracey Crouch',\n",
        " 'SPEAKER_10': 'Wes Streeting',\n",
        " 'SPEAKER_04': 'Amanda Milling',\n",
        " 'SPEAKER_11': 'Chris Elmore',\n",
        " 'SPEAKER_05': 'Nusrat Ghani',\n",
        " 'SPEAKER_00': 'Jim Shannon',\n",
        " 'SPEAKER_08': 'Tom Watson',\n",
        " 'SPEAKER_12': 'John Glen',\n",
        " 'SPEAKER_13': 'Luke Pollard',\n",
        " 'SPEAKER_02': 'CHAIR'}\n",
        "\n",
        "gender_mapping = {'SPEAKER_09': 'Woman',\n",
        " 'SPEAKER_06': 'Man',\n",
        " 'SPEAKER_07': 'Man',\n",
        " 'SPEAKER_01': 'Woman',\n",
        " 'SPEAKER_03': 'Woman',\n",
        " 'SPEAKER_10': 'Man',\n",
        " 'SPEAKER_04': 'Woman',\n",
        " 'SPEAKER_11': 'Man',\n",
        " 'SPEAKER_05': 'Woman',\n",
        " 'SPEAKER_00': 'Man',\n",
        " 'SPEAKER_08': 'Man',\n",
        " 'SPEAKER_12': 'Man',\n",
        " 'SPEAKER_13': 'Man',\n",
        " 'SPEAKER_02': 'Man'}\n",
        "\n",
        "segment_df['speaker_name'] = segment_df['speaker_label'].apply(lambda x: speaker_mapping[x])\n",
        "segment_df['speaker_gender'] = segment_df['speaker_label'].apply(lambda x: gender_mapping[x])\n",
        "\n",
        "\n",
        "# Reorder columns\n",
        "segment_df = segment_df[['segment', 'speaker_label', 'speaker_name', 'speaker_gender',\n",
        "                         'segment_number', 'start', 'end', 'dur', 'segment_fpath']]"
      ],
      "metadata": {
        "id": "Kozmvmtz_Knh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained embedding model\n",
        "from pyannote.audio import Model\n",
        "embedding_model = Model.from_pretrained(\"pyannote/embedding\",\n",
        "                              use_auth_token=access_token)"
      ],
      "metadata": {
        "id": "woBneh94_3i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Inference class\n",
        "from pyannote.audio import Inference\n",
        "\n",
        "# We use a sliding window with a duration of 1.6 seconds with a 0.2s step.\n",
        "embedding_inference = Inference(embedding_model, window=\"sliding\",\n",
        "                                duration=1.6, step=0.2)"
      ],
      "metadata": {
        "id": "NZWPExKKBGIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define list of segment filepaths\n",
        "segment_fpaths = list(segment_df.segment_fpath)\n",
        "\n",
        "# Define empty list to store the embeddings\n",
        "embeddings = []\n",
        "\n",
        "# Define empty lists to store speaker and gender labels\n",
        "speaker_labels, speaker_gender = [], []\n",
        "\n",
        "# Loop through each segment\n",
        "for ix, segment_fpath in enumerate(segment_fpaths[:]):\n",
        "  # Compute embedding for each segment and convert to numpy array\n",
        "  embed = np.array(embedding_inference(segment_fpath))\n",
        "\n",
        "  # Concatenate with previous embeddings\n",
        "  if len(embeddings) > 0:\n",
        "    embeddings = np.concatenate([embeddings, embed])\n",
        "  else:\n",
        "    embeddings = np.concatenate([embed,])\n",
        "\n",
        "  # Generate speaker and gender labels\n",
        "  speaker_labels += [segment_df.iloc[ix].speaker_name] * embed.shape[0]\n",
        "  speaker_gender += [segment_df.iloc[ix].speaker_gender] * embed.shape[0]"
      ],
      "metadata": {
        "id": "Kq_Zk9VYCAWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To see the effectivness of the embeddings, we reduce the 512-dimensional vectors\n",
        "# to two-dimensions using TSNE. TSNE is just like PCA, but is often better for\n",
        "# visualization purposes.\n",
        "from sklearn.manifold import TSNE\n",
        "random_state = 13\n",
        "learning_rate = 200\n",
        "n_iter = 5000\n",
        "perplexity = 50\n",
        "n_components = 2\n",
        "tsne = TSNE(n_components=n_components, verbose=1, perplexity=perplexity, n_iter=n_iter, learning_rate=learning_rate, random_state=random_state)\n",
        "tsne_results = tsne.fit_transform(embeddings)"
      ],
      "metadata": {
        "id": "C3xJAVNSB6VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert into dataframe\n",
        "tsne_df = pd.DataFrame(tsne_results, columns=['tsne1', 'tsne2'])\n",
        "\n",
        "# Add speaker and gender labels\n",
        "clusters_tsne = pd.concat([tsne_df, pd.DataFrame({'speaker': speaker_labels,\n",
        "                                                  'gender': speaker_gender})], axis=1)\n",
        "# Define hue and style for plotting\n",
        "speaker_hue = list(clusters_tsne.speaker)\n",
        "gender_style = list(clusters_tsne.gender)"
      ],
      "metadata": {
        "id": "i_wAoBqD676e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot TSNE-reduced embeddings\n",
        "plt.figure(figsize = (12,12))\n",
        "\n",
        "scatterplot = sns.scatterplot(x=clusters_tsne.iloc[:, 0], y=clusters_tsne.iloc[:, 1],\n",
        "                              hue=speaker_hue, style=gender_style,\n",
        "                              palette='tab20', s=100, alpha=0.7)\n",
        "\n",
        "# Get the current axes and legend\n",
        "ax = plt.gca()\n",
        "legend = ax.get_legend()\n",
        "\n",
        "# Create a new legend for the 'hue' (color) using the handles and labels from the original legend\n",
        "hue_legend = plt.legend(handles=legend.legendHandles[:-2],\n",
        "                        labels=list(clusters_tsne.speaker.unique()),\n",
        "                        title='', loc='lower left', frameon=False, ncol=2)\n",
        "\n",
        "# Create a new legend for the 'style' (marker) using the handles and labels from the original legend\n",
        "style_legend = plt.legend(handles=legend.legendHandles[-2:],\n",
        "                          labels=list(clusters_tsne.gender.unique()),\n",
        "                          title='', loc='upper left',\n",
        "                          bbox_to_anchor=(0.0, 0.25), frameon=False)\n",
        "\n",
        "# Add both legends to the plot\n",
        "ax.add_artist(hue_legend)\n",
        "ax.add_artist(style_legend)\n",
        "\n",
        "# plt.legend(loc='best', frameon=False)\n",
        "plt.xlabel('')\n",
        "plt.ylabel('')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c3Sg4Xyb7D1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to do the same with PCA\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=n_components, random_state=random_state)\n",
        "pca_results = pca.fit_transform(embeddings)\n",
        "\n",
        "# Convert into dataframe\n",
        "pca_df = pd.DataFrame(pca_results, columns=['pc1', 'pc2'])\n",
        "\n",
        "# Add speaker and gender labels\n",
        "clusters_pca = pd.concat([pca_df, pd.DataFrame({'speaker': speaker_labels,\n",
        "                                                'gender': speaker_gender})], axis=1)\n",
        "# Define hue and style for plotting\n",
        "speaker_hue = list(clusters_pca.speaker)\n",
        "gender_style = list(clusters_pca.gender)"
      ],
      "metadata": {
        "id": "3NMKZEto6rYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PLotting of PCA-reduced embeddings\n",
        "\n",
        "plt.figure(figsize = (12,12))\n",
        "\n",
        "scatterplot = sns.scatterplot(x=clusters_pca.iloc[:, 0], y=clusters_pca.iloc[:, 1],\n",
        "                              hue=speaker_hue, style=gender_style,\n",
        "                              palette='tab20', s=100, alpha=0.7)\n",
        "\n",
        "# Get the current axes and legend\n",
        "ax = plt.gca()\n",
        "legend = ax.get_legend()\n",
        "\n",
        "# Create a new legend for the 'hue' (color) using the handles and labels from the original legend\n",
        "hue_legend = plt.legend(handles=legend.legendHandles[:-2],\n",
        "                        labels=list(clusters_tsne.speaker.unique()),\n",
        "                        title='', loc='lower left', frameon=False, ncol=2)\n",
        "\n",
        "# Create a new legend for the 'style' (marker) using the handles and labels from the original legend\n",
        "style_legend = plt.legend(handles=legend.legendHandles[-2:],\n",
        "                          labels=list(clusters_tsne.gender.unique()),\n",
        "                          title='', loc='upper left',\n",
        "                          bbox_to_anchor=(0.0, 0.25), frameon=False)\n",
        "\n",
        "# Add both legends to the plot\n",
        "ax.add_artist(hue_legend)\n",
        "ax.add_artist(style_legend)\n",
        "\n",
        "# plt.legend(loc='best', frameon=False)\n",
        "plt.xlabel('')\n",
        "plt.ylabel('')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Et-U1iwkDuV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatic Speech Recognition\n",
        "\n",
        "https://github.com/guillaumekln/faster-whisper"
      ],
      "metadata": {
        "id": "Hv9hMGvcFiRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install faster-whisper\n",
        "!pip install faster-whisper"
      ],
      "metadata": {
        "id": "kh2E5SvkFSzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install jiwer to evaluate ASR output\n",
        "!pip install jiwer"
      ],
      "metadata": {
        "id": "lNe4oaN4BybE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ASR model\n",
        "from faster_whisper import WhisperModel"
      ],
      "metadata": {
        "id": "-_7DGKJLFbcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model specs\n",
        "model_size = \"small.en\"      # could also be medium.en, large.en, small, and medium, large\n",
        "language = 'en'\n",
        "beam_size = 5\n",
        "word_timestamps = True\n",
        "\n",
        "# Initiate ASR class\n",
        "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8\")"
      ],
      "metadata": {
        "id": "YG5jSY-w9TIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transcribe first segment\n",
        "segments, info = model.transcribe(segment_fpaths[0],\n",
        "                                  language=language,\n",
        "                                  beam_size=beam_size,\n",
        "                                  word_timestamps=word_timestamps)"
      ],
      "metadata": {
        "id": "PX6hQreHFoME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print each word and its timestamps\n",
        "for segment in segments:\n",
        "    for word in segment.words:\n",
        "        print(\"[%.2fs -> %.2fs] %s\" % (word.start, word.end, word.word))"
      ],
      "metadata": {
        "id": "1e3fqKfzF2fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that the output is a Python generator. This means that it can ONLY be used once.\n",
        "# If you want to re-run the code, you must run the ASR model first.\n",
        "list(segments)"
      ],
      "metadata": {
        "id": "s9JjrITwJ2dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-run the ASR model\n",
        "segments, _ = model.transcribe(segment_fpaths[0],\n",
        "                                  language=language,\n",
        "                                  beam_size=beam_size,\n",
        "                                  word_timestamps=word_timestamps)\n",
        "\n",
        "# Define list of segments - note that the `segments` object is now empty\n",
        "segment_list = list(segments)\n",
        "\n",
        "# Define empty list to store ASR output\n",
        "asr_output = []\n",
        "\n",
        "# Loop over each ASR segment\n",
        "for segment in segment_list:\n",
        "    # Loop over each word in each ASR-segment\n",
        "    for word in segment.words:\n",
        "        # Make dictionary\n",
        "        word_dict = {'speech_id': \"57-17\",\n",
        "                     'segment_id': segment.id,\n",
        "                     'start': word.start,\n",
        "                     'end': word.end,\n",
        "                     'word': word.word.strip(),\n",
        "                     'word_prob': round(word.probability, 2)}\n",
        "        # Save dictionary in list\n",
        "        asr_output.append(word_dict)"
      ],
      "metadata": {
        "id": "G91UwbsoF6GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct dataframe\n",
        "asr_df = pd.DataFrame(asr_output)\n",
        "asr_df"
      ],
      "metadata": {
        "id": "aR8S_fHgIJgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can reconstruct the text:\n",
        "asr_text = ' '.join(list(asr_df.word))\n",
        "asr_text"
      ],
      "metadata": {
        "id": "9_Y9BKWdGdLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in official transcript\n",
        "transcript = pd.read_csv('/content/css_fall2023/data/audio/class11/57-17-official_transcript.csv')"
      ],
      "metadata": {
        "id": "-oW4blSeBfuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual transcription to generate reference text\n",
        "reference_text = 'Mr. Speaker, we have been clear all along that this a publicly owned broadcaster. Channel 4 must provide for and reflect the country as a whole. \\\n",
        "We are still in discussions with Channel 4 about how it should do this, including relocating staff out of London, and we will set out next steps in due course.'\n",
        "\n",
        "reference_text"
      ],
      "metadata": {
        "id": "Hyene4L8CwaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_text = transcript.iloc[1].text\n",
        "transcript_text"
      ],
      "metadata": {
        "id": "OEH3C7jqBivv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove punctation and convert to lower\n",
        "import string\n",
        "asr_text = asr_text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "reference_text = reference_text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "transcript_text = transcript_text.translate(str.maketrans('', '', string.punctuation)).lower()"
      ],
      "metadata": {
        "id": "PTcsjXPiDRtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer"
      ],
      "metadata": {
        "id": "PmPM1Yb_91kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import wer from jiwer\n",
        "from jiwer import wer as WordErrorRate\n",
        "print(f\"Word Error Rate for reference and ASR text: {round(WordErrorRate(reference=reference_text, hypothesis=asr_text), 3)}\")\n",
        "print(f\"Word Error Rate for reference and transcript text: {round(WordErrorRate(reference=reference_text, hypothesis=transcript_text), 3)}\")"
      ],
      "metadata": {
        "id": "aQciF3PeBAkY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}