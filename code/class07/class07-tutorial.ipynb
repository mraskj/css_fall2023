{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf9108bc",
   "metadata": {},
   "source": [
    "# Class 7: Topic Modeling and Dictionary-based Analysis - Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b3b68",
   "metadata": {},
   "source": [
    "In this tutorial, we will see how we can preprocess text for a topic model and estimate a LDA in Python.\n",
    "\n",
    "The tutorial is based on this: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/?utm_content=cmp-true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab379a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import os\n",
    "import re                                                      # for regular expressions\n",
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint                                      # for nice prints\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "# Gensim modules\n",
    "import gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# Spacy module\n",
    "import spacy\n",
    "\n",
    "# NLTK module\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc12c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # Working Directory # # # #\n",
    "\n",
    "if platform.system() == 'Linux':\n",
    "    wd = '/home/rask/'\n",
    "else:\n",
    "    wd = 'C:/Users/au535365/'\n",
    "\n",
    "wd = os.path.join(wd, 'Dropbox/teaching/css_fall2023')\n",
    "    \n",
    "# Change directory\n",
    "os.chdir(wd)\n",
    "\n",
    "# Confirm that the working directory is as intended \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d8138",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992cece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK also has stopwords - we first download them\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# and then defines a list of english stopwords\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a398e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee48e6eb",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5b2488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5942527",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = fetch_20newsgroups(subset='train',  remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe2c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target categories\n",
    "data_dict.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f77f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect\n",
    "data_dict.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f3f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "topic_labels = data_dict.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641046e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build target dictionary\n",
    "target_dict = {i: k for i, k in enumerate(data_dict.target_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd55dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract corpus and convert it to a numpy array\n",
    "corpus = np.array(data_dict['data'])\n",
    "print(f\"Length of corpus before removing empty docs: {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589710a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only non-empty speeches\n",
    "indices = [ix for ix in range(len(corpus)) if len(corpus[ix]) > 0]\n",
    "corpus = corpus[indices]\n",
    "topic_labels = topic_labels[indices]\n",
    "print(f\"Length of corpus before removing empty docs: {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73fedb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect corpus\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1674420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We randomly sample 3000 documents\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(10)\n",
    "\n",
    "# Define number of samples\n",
    "n_samples = 3000\n",
    "\n",
    "# Random sampling\n",
    "sample_indices = np.random.choice(len(corpus), size=n_samples, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47387d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset based on random speeches\n",
    "samples = corpus[sample_indices]\n",
    "topic_labels_samples = topic_labels[sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca661781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of docs for each topic\n",
    "_, count = np.unique(topic_labels_samples, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb984ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pandas dataframe from the count\n",
    "topic_count_df = pd.DataFrame({'topic': target_dict.values(), 'count': count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f86a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the counts\n",
    "pprint(topic_count_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6502277",
   "metadata": {},
   "source": [
    "## Basic Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download spacy model for english if you not already have done so\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac75ed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define spacy pipeline\n",
    "spacy_pipeline_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a stopwords lists\n",
    "stop_words = sorted(list(spacy_pipeline_en.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove e-mails\n",
    "docs_clean = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in samples]\n",
    "\n",
    "# Remove new line characters\n",
    "docs_clean = [re.sub('\\s+', ' ', sent) for sent in docs_clean]\n",
    "docs_clean = [re.sub('\\n', '', sent) for sent in docs_clean]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "docs_clean = [re.sub(\"\\'\", \"\", sent) for sent in docs_clean]\n",
    "\n",
    "# Remove two or more consecutive whitespaces (pattern=' +', replacement=' ')\n",
    "docs_clean = [re.sub(' +', ' ', sent) for sent in docs_clean]\n",
    "\n",
    "# Remove trailing and leading whitespaces\n",
    "docs_clean = [sent.strip() for sent in docs_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11775dd1",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_raw = [[d for d in spacy_pipeline_en(doc)] for doc in notebook_tqdm(docs_clean, position=0, leave=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19144943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "tokens_cleaned = [[x for x in token if x.text not in stop_words] for token in tokens_raw]\n",
    "\n",
    "# Remove punctuation\n",
    "tokens_cleaned = [[x for x in token if not x.is_punct] for token in tokens_cleaned]\n",
    "\n",
    "# Remove digits\n",
    "tokens_cleaned = [[x for x in token if not x.is_digit] for token in tokens_cleaned]\n",
    "\n",
    "# Remove tokens shorter than 3 characters\n",
    "tokens_cleaned = [[x for x in token if len(x) >= 3] for token in tokens_cleaned]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ae83a",
   "metadata": {},
   "source": [
    "## Prepare Data for LDA\n",
    "\n",
    "The LDA models take a vocabulary and a vectorized version of our corpus as inputs.\n",
    "\n",
    "In `gensim`, this is done using the `Dictionary()` class and its `doc2bow` method. The former creates the vocabulary and the latter creates our BoW. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d527fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Dictionary class from gensim on our tokens\n",
    "vocab = Dictionary(tokens_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919a6173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get an error because the tokens are not strings. They still belong to the spacy module. \n",
    "# We can convert it to strings using the .text or .lower_ attribute\n",
    "tokens_cleaned = [[x.lower_ for x in doc] for doc in tokens_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277ed9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute word frequency and word-document frequency using the Counter() class from the collections module\n",
    "from collections import Counter\n",
    "\n",
    "word_freq = Counter()\n",
    "for doc in tokens_cleaned:\n",
    "    for word in doc:\n",
    "        word_freq[word] += 1\n",
    "\n",
    "word2doc_freq = Counter()\n",
    "for doc in tokens_cleaned:\n",
    "    unique_tokens = set(doc)\n",
    "    word2doc_freq.update(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e86bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1805d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect\n",
    "word2doc_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d82c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove frequent and rare occuring tokens\n",
    "tokens_final = [[x for x in doc if \n",
    "               word_freq[x]/len(samples) <= 0.10 and \n",
    "               word2doc_freq[x]/len(samples) <= 0.10 and\n",
    "               word_freq[x] >= 5 and\n",
    "               word2doc_freq[x] >= 5] for doc in tokens_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c563df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct vocabulary\n",
    "vocab = Dictionary(tokens_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c595c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct BoW\n",
    "bow = [vocab.doc2bow(doc) for doc in tokens_final]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8175d7c8",
   "metadata": {},
   "source": [
    "## Building the Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f56ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build lda_model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=bow,\n",
    "                                           id2word=vocab,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3cea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print topics\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d526a",
   "metadata": {},
   "source": [
    "Below, I generate a class called `TopicInspector` that are helpful in interpreting the results of the LDA. \n",
    "\n",
    "1. Initialization:\n",
    "    * The __init__ method is the constructor for the class. It takes several parameters:\n",
    "        \n",
    "        * lda_model: This is the LDA model that you want to inspect.\n",
    "        * vocab: This is a vocabulary mapping from word IDs to words.\n",
    "        * corpus: This is the corpus of documents used to train the LDA model.\n",
    "        * topn: This is an optional parameter specifying the number of top words in each topic to consider. It defaults to 10.\n",
    "        \n",
    "2. `id2token`:\n",
    "    * The id2token method is used to convert a word ID (wid) to its corresponding word in the vocabulary.\n",
    "\n",
    "3. `get_topic_words`:\n",
    "    * The get_topic_words method takes a topic ID (tid) and retrieves the top words associated with that topic from the LDA model. It returns a list of words.\n",
    "\n",
    "\n",
    "4. `get_topic_word_prob`:\n",
    "    * The get_topic_word_prob method is similar to get_topic_words, but it returns the probabilities (scores) of the top words in the topic instead of the actual words.n\n",
    "\n",
    "5. `topic_word_df`:\n",
    "    * The topic_word_df method creates a DataFrame that represents the top words for each topic. It iterates through all topics, retrieves the top words for each topic using get_topic_words, and constructs a DataFrame where each column is labeled with the topic number (e.g., 'topic0', 'topic1').\n",
    "\n",
    "\n",
    "6. `topic_doc_df` :\n",
    "    * The topic_doc_df method creates a DataFrame that represents the distribution of topics for each document in the corpus.\n",
    "    * It uses the LDA model's get_document_topics method to obtain the topic distribution for each document.\n",
    "    * The resulting DataFrame has columns labeled with topic numbers ('topic0', 'topic1', etc.).\n",
    "    * Additionally, it can include two optional columns:\n",
    "        * max_topic: If add_max_topic is True, it adds a column with the topic that has the highest probability for each document.\n",
    "        * max_score: If add_max_score is True, it adds a column with the highest probability score for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fd3255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicInspector:\n",
    "    \"\"\" A class for inspecting and analyzing Latent Dirichlet Allocation (LDA) models. \n",
    "        \n",
    "        Attributes:\n",
    "            lda_model (gensim.models.LdaModel): The LDA model to be inspected.\n",
    "            vocab (dict): A vocabulary mapping from word IDs to words.\n",
    "            corpus (list of list of tuples): The corpus of documents used to train the LDA model.\n",
    "            num_topics (int): The number of topics in the LDA model.\n",
    "            topn (int, optional): The number of top words in each topic to consider (default is 10).\n",
    "        \n",
    "        Methods:\n",
    "            id2token(wid): Convert a word ID to its corresponding word in the vocabulary.\n",
    "            get_topic_words(tid): Get the top words associated with a given topic.\n",
    "            get_topic_word_prob(tid): Get the probabilities of the top words in a given topic.\n",
    "            topic_word_df(): Create a DataFrame representing the top words for each topic.\n",
    "            topic_doc_df(add_max_topic=True, add_max_score=True): Create a DataFrame representing the topic distribution for each document in the corpus.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lda_model, vocab, corpus, topn=10):\n",
    "        self.lda_model = lda_model\n",
    "        self.vocab = vocab\n",
    "        self.corpus = corpus\n",
    "        self.num_topics = self.lda_model.num_topics\n",
    "        self.topn = topn\n",
    "    \n",
    "    def id2token(self, wid):\n",
    "        return self.vocab[wid]\n",
    "    \n",
    "    def get_topic_words(self, tid):\n",
    "        topic_terms = self.lda_model.get_topic_terms(tid, topn=self.topn)\n",
    "        wordids, score = zip(*topic_terms)\n",
    "        return [self.id2token(x) for x in wordids]\n",
    "    \n",
    "    def get_topic_word_prob(self, tid):\n",
    "        topic_terms = self.lda_model.get_topic_terms(tid, topn=self.topn)\n",
    "        wordids, score = zip(*topic_terms)\n",
    "        return score\n",
    "    \n",
    "    def topic_word_df(self):\n",
    "        \n",
    "        topic_df_list = []\n",
    "        for k in range(0, self.num_topics - 1):\n",
    "            words = self.get_topic_words(tid=k)\n",
    "            topic_df_ = pd.DataFrame(words, columns=[f'topic{k}'])\n",
    "            topic_df_list.append(topic_df_)\n",
    "        \n",
    "        topic_word_df = pd.concat(topic_df_list, axis=1)\n",
    "        \n",
    "        return topic_word_df\n",
    "            \n",
    "    \n",
    "    def topic_doc_df(self, add_max_topic=True, add_max_score=True):\n",
    "        \n",
    "        topic_docs = self.lda_model.get_document_topics(self.corpus, minimum_probability=0)\n",
    "        doc_dist_list = []\n",
    "        for d in range(len(topic_docs)):\n",
    "            doc_dist = [x[1] for x in topic_docs[d]]\n",
    "            doc_dist_list.append(doc_dist)\n",
    "        \n",
    "        topic_doc_df = pd.DataFrame(doc_dist_list, columns=[f'topic{x}' for x in range(self.num_topics)])\n",
    "        if add_max_topic:\n",
    "            max_topics = topic_doc_df.idxmax(axis=1)\n",
    "        else:\n",
    "            max_topics = None\n",
    "        \n",
    "        if add_max_score:\n",
    "            max_scores = topic_doc_df.max(axis=1)\n",
    "        else:\n",
    "            max_scores = None\n",
    "        \n",
    "        if add_max_topic:\n",
    "            topic_doc_df['max_topic'] = max_topics\n",
    "        \n",
    "        if add_max_score: \n",
    "            topic_doc_df['max_score'] = max_scores\n",
    "\n",
    "        return topic_doc_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dce1b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define instance of the TopicInspector\n",
    "lda_inspect = TopicInspector(lda_model=lda_model, vocab=vocab, corpus=bow, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6df186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words for each topic\n",
    "topic_word_df = lda_inspect.topic_word_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d0f04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print topic word df\n",
    "print(topic_word_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b802b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic distribution for each doc\n",
    "topic_doc_df = lda_inspect.topic_doc_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad24c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print\n",
    "topic_doc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03759165",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e4838",
   "metadata": {},
   "source": [
    "## Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a1aa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "text_1 = samples[0]\n",
    "text_2 = samples[6]\n",
    "sent_1 = sentiment.polarity_scores(text_1)\n",
    "sent_2 = sentiment.polarity_scores(text_2)\n",
    "print(\"Sentiment of text 1:\", sent_1)\n",
    "print(\"Sentiment of text 2:\", sent_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b28d2b2",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b5a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from danlp.models import load_bert_tone_model\n",
    "classifier = load_bert_tone_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e0710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the classifier\n",
    "classifier.predict('Analysen viser, at økonomien bliver forfærdelig dårlig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b8d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict('Jeg tror alligvel, det bliver godt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dd228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probabilities and matching classes names\n",
    "probs = classifier.predict_proba('Analysen viser, at økonomien bliver forfærdelig dårlig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd69bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Polarity probabilities for classes 'positive', 'neutral' and 'negative': {probs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b64f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Subjectivity/objectivity probabilities for classes 'objective' and 'subjective': {probs[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
