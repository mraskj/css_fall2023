{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "679bac2d",
   "metadata": {},
   "source": [
    "# Class 7: Topics Modeling and Dictionary-based Analysis - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e0dc56",
   "metadata": {},
   "source": [
    "## 0 Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82760f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic Python modules\n",
    "import os\n",
    "import platform\n",
    "\n",
    "# Data mangement libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# SpaCy\n",
    "import spacy\n",
    "\n",
    "# For regular expressions\n",
    "import re\n",
    "\n",
    "# For sentiment analysis\n",
    "from sentida import Sentida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69246e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # Working Directory # # # #\n",
    "\n",
    "if platform.system() == 'Linux':\n",
    "    wd = '/home/rask/'\n",
    "else:\n",
    "    wd = 'C:/Users/au535365/'\n",
    "\n",
    "wd = os.path.join(wd, 'Dropbox/teaching/css_fall2023')\n",
    "    \n",
    "# Change directory\n",
    "os.chdir(wd)\n",
    "\n",
    "# Confirm that the working directory is as intended \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a57ead",
   "metadata": {},
   "source": [
    "#### Exercise 0.0: Reading in Data\n",
    "\n",
    "We start by reading in data. We work the same data as in *class05* and *class06* but restrict ourselves to a single year. You can choose whatever year you like, but make sure to have at least $10,000$ in the dataframe when you have loaded the data. Call the dataframe `df`\n",
    "\n",
    "Note also tha we can read the data directly from GitHub. See the notebook `class05-filereading.ipynb` for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226e7cf2",
   "metadata": {},
   "source": [
    "#### Solution 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97abb0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate file ids\n",
    "files = ['20151']\n",
    "\n",
    "# Specify base url\n",
    "base_url = 'https://raw.githubusercontent.com/mraskj/css_fall2023/master/data/ft-speeches/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36133e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "df = pd.DataFrame()\n",
    "for file in tqdm(files, leave=True, position=0):\n",
    "    df_term = pd.read_csv(base_url + file + '.csv')\n",
    "    df_term['term'] = file\n",
    "    df = pd.concat([df, df_term])\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bfc55b",
   "metadata": {},
   "source": [
    "#### Exercise 0.1: Removal of Short Texts\n",
    "\n",
    "Text-based methods generally work better with longer documents. Hence, we remove short texts, which are likely to be uninformative anyway. \n",
    "\n",
    "Compute the number of characters in each speech and keep only speeches with $V$ or more characters. Argue for your choice of $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2841ef0b",
   "metadata": {},
   "source": [
    "#### Solution 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a8468",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_threshold = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Two steps\n",
    "df['chars'] = df['text'].apply(lambda x: len(x))\n",
    "df = df.loc[df['chars'] >= ch_threshold].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52a42d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: One step\n",
    "df = df.loc[df['text'].apply(lambda x: len(x)) >= ch_threshold].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a62c2ef",
   "metadata": {},
   "source": [
    "## 1 Topic Modeling\n",
    "\n",
    "The exercises in the next section regards topic modeling. Our task is to identify topics discussed in parliamentary speeches from the Danish parliament in the given term that you decided upon in *exercise 0.0*. This is **very** common task when you work with text data. Mastering it will make your life easier :-) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17943a2",
   "metadata": {},
   "source": [
    "#### Exercise 1.0: Load SpaCy Model\n",
    "\n",
    "1) Load in the spacy model we used in *class06* and assign to an object called `spacy_pipeline_da`.\n",
    "2) Define a list called `texts` with the `text` column from `df`. Remember to type cast it as a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99038f",
   "metadata": {},
   "source": [
    "#### Solution 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93339b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model \"da_core_news_sm\"\n",
    "spacy_pipeline_da = spacy.load(\"da_core_news_sm\")\n",
    "\n",
    "# Define a list called `texts' based on the 'text' column in the sample_df dataframe\n",
    "texts = list(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b70fc9d",
   "metadata": {},
   "source": [
    "#### Exercise 1.1: Text Cleaning I\n",
    "\n",
    "Cleaning our text is a very important step in working with text data. \n",
    "\n",
    "  1. Remove the weird encoding '\\xa0'. This encoding does not always appear, but sometimes it will. Remove it.\n",
    "  2. Remove two or more consecutive whitespaces.\n",
    "    \n",
    "You can use the `re` module for these tasks wrapped in list comprehensions. We also did this in the tutorial.\n",
    "\n",
    "A good tip is to assign the cleaned text to a new object, for instance called `texts_cleaned` or another name of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a4bbb",
   "metadata": {},
   "source": [
    "#### Solution 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e139554",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove \\xa0 from the text.\n",
    "# The `re` module is used to search for elements in strings using regular expressions.\n",
    "# re.sub substitutes the searched pattern (here '\\xa0') with a specified replacement (here '')\n",
    "texts_cleaned = [re.sub('\\xa0', '', text) for text in tqdm(texts, position=0, leave=True)]\n",
    "\n",
    "# Remove two or more consecutive whitespaces (pattern=' +', replacement=' ')\n",
    "texts_cleaned = [re.sub(' +', ' ', text) for text in tqdm(texts_cleaned, position=0, leave=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ad336",
   "metadata": {},
   "source": [
    "#### Exercise 1.2: Text Cleaning II\n",
    "\n",
    "Each language has a set of stopwords, which we typically wants to remove. \n",
    "\n",
    "Each corpus also has a set of corpus-specific characteristics that can be viewed as stopwords. In parliamentary speeches, this includes words like *ordfører*, *lovforslag*, and so on. Furthermore, mentionings of legislators and parties are very common and often uninformative, at least for the perspective of topic modelling.  \n",
    "\n",
    "Below, I provide you a bunch of lists with names of politicians, procedural words, and parties that we want to remove in `removal_words`. I also make the regular expression for you `removal_pattern`. Try to figure out what's going on if you like. Maybe ask ChatGPT.\n",
    "\n",
    "Your task is now to:\n",
    "\n",
    "1. Define a list of Danish stopwords using the loaded pipeline `spacy_pipeline_da` (we also did this in *class06* and in the *class07-tutorial*). Call the object `stopwords`.\n",
    "\n",
    "2. Remove the words defined in the regular expression `removal_pattern` from your cleaned text from *exercise 1.3* \n",
    "\n",
    "3. Remove two or more consecutive whitespaces (reuse your code from *exercise 1.3*).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159cefb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_remove = [x[:-1] + '[a-z]+' for x in list(df.speaker.unique())]\n",
    "\n",
    "procedural_to_remove = ['[Ll]ovforsla[a-z]+', 'ordfør[a-z]+', 'spørgsmå[a-z]+',\n",
    "                        'forsla[a-z]+', 'L', 'B', '[Hh]r', '[Ff]ru', '[Aa]fstemnin[a-z]+',\n",
    "                        '[Ff]orhandlin[a-z]+', '[Hh]r', '[Ff]ru']\n",
    "                   \n",
    "parties_to_remove = ['[Ll]iberal [Aa]llianc[a-z]+', 'LA', '[Dd]et [Kk]onservative [Ff]olkepar[a-z]+', 'KF',\n",
    "                   '[Dd]e [Kk]onservati[a-z]+', 'Venst[a-z]+', '[Dd]ansk [Ff]olkepart[a-z]+', \n",
    "                   '[Nn]ye [Bb]orgerli[a-z]+', '[Dd]e [Rr]adikal[a-z]+', '[Ss]ocialdemokratie[a-z+]',\n",
    "                   '[Ss]ocialdemokra[a-z]+', '[Ss]ocialistis[a-z]+ [Ff]olkepart[a-z]+', 'SF',\n",
    "                   '[Aa]lternative[a-z]+', '[Ee]nhedslist[a-z]+', '[Rr]adika[a-z]+']        \n",
    "\n",
    "removal_words = parties_to_remove + procedural_to_remove + names_to_remove\n",
    "        \n",
    "removal_pattern = r'\\b(?:' + '|'.join(removal_words) + r')\\b'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cba542d",
   "metadata": {},
   "source": [
    "#### Solution 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f00c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of Danish stopwords\n",
    "stopwords = sorted(list(spacy_pipeline_da.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47155281",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_cleaned = [re.sub(removal_pattern, '', text) for text in texts_cleaned]\n",
    "# texts_cleaned = [re.sub('\\(.*\\)|\\[.*\\]', '', text) for text in texts_cleaned]\n",
    "texts_cleaned = [re.sub(' +', ' ', text) for text in texts_cleaned]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b5a178",
   "metadata": {},
   "source": [
    "#### Exercise 1.3: Tokenization\n",
    "\n",
    "We now want to tokenize our speeches. Use the pipeline `spacy_pipeline_da`, which you loaded in *exercise 1.2*.\n",
    "\n",
    "You should iterate over our cleaned text from *exercise 1.4*. Assign the tokens to an object called `tokens_raw` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157640e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize \n",
    "tokens_raw = [[d for d in spacy_pipeline_da(doc)] for doc in tqdm(texts_cleaned, position=0, leave=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70b036d",
   "metadata": {},
   "source": [
    "#### Solution 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f0f39f",
   "metadata": {},
   "source": [
    "#### Exercise 1.4 Preprocessing I\n",
    "\n",
    "Preprocessing can influence our results in both positive and negative ways. Apply a range of preprocessing steps of your choice on your `tokens_raw` object from *exercise 1.6*. You should remove stopwords, but what you do besides that depends on your intution and line of reasoning. Whatever you decide, you should return tokens using SpaCy's `.text` attribute or `.lower_` attribute if you want to lower your terms.\n",
    "\n",
    "Assign your result to an object called `tokens_clean`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98224335",
   "metadata": {},
   "source": [
    "#### Solution 1.4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c044cd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords, digits and punctuation\n",
    "tokens_clean = [[x.lower_ for x in tok \n",
    "                 if not x.is_punct and \n",
    "                 not x.is_digit and \n",
    "                 x.text not in stopwords and\n",
    "                 len(x.text) > 2] for tok in tqdm(tokens_raw, position=0, leave=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e230eb",
   "metadata": {},
   "source": [
    "#### Exercise 1.5 Preprocessing II\n",
    "\n",
    "As a final preprocessing step, we consider whether we should remove rare and frequent occuring terms. We have already removed many frequent terms when we removed stopwords and also the domain-specific stopwords such as the names of legislators, party names, and procedural words. \n",
    "\n",
    "1. Argue why removal of frequent and rare terms can benefit topic modeling\n",
    "2. Compute the word frequency and word-document frequency based on your `tokens_clean`. I have provided you with code to do in the tutorial (**Note**: you can not use the code if you have not returned the tokens using the `.text` or `.lower_` attribute in *exercise 1.6*)\n",
    "3. Remove frequent and rare terms according to thresholds of your choice (e.g. terms that occur in what corresponds to 10% of the total speeches or words that occur in at least 5 speeches across corpus)\n",
    "4. Compute the number of tokens for the first speech before and after removal of rare/frequent words) \n",
    "\n",
    "Assign your final tokens to an object called `tokens_final`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d595fb0",
   "metadata": {},
   "source": [
    "#### Solution 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6801f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute word frequency and word-document frequency\n",
    "from collections import Counter\n",
    "\n",
    "word_freq = Counter()\n",
    "for doc in tokens_clean:\n",
    "    for word in doc:\n",
    "        word_freq[word]+= 1\n",
    "\n",
    "word2doc_freq = Counter()\n",
    "for doc in tokens_clean:\n",
    "    unique_tokens = set(doc)\n",
    "    word2doc_freq.update(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64da668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(word2doc_freq.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008aa4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove frequent and rare occurind terms\n",
    "tokens_final = [[x for x in doc if \n",
    "               word_freq[x]/len(texts) <= .10 and \n",
    "               word2doc_freq[x]/len(texts) <= .10 and\n",
    "               word_freq[x] >= 5 and\n",
    "               word2doc_freq[x] >= 5] for doc in tqdm(tokens_clean, position=0, leave=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f3d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of tokens for document 0 before and after removal of frequent and rare terms\n",
    "len(tokens_clean[0]), len(tokens_final[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf8ccf",
   "metadata": {},
   "source": [
    "#### Exercise 1.6\n",
    "\n",
    "1. Construct the vocabulary using the Dictionary class from gensim on `tokens_final`\n",
    "2. Construct the BoW from the vocabulary from step 1\n",
    "\n",
    "See the tutorial for the class if you can't remember how to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d523a",
   "metadata": {},
   "source": [
    "#### Solution 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278caf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vocab and BoW\n",
    "vocab = Dictionary(tokens_final)\n",
    "bow = [id2word.doc2bow(tok) for tok in tokens_final]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb074c0c",
   "metadata": {},
   "source": [
    "#### Exercise 1.7\n",
    "\n",
    "Estimate a LDA model with a topic number $k$ of your choice. Argue for your choice of $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740aabda",
   "metadata": {},
   "source": [
    "#### Solution 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f454451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate topic model with k=30 topics\n",
    "lda = LdaModel(corpus=bow,\n",
    "               id2word=vocab,\n",
    "               num_topics=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ef6cf9",
   "metadata": {},
   "source": [
    "#### Exercise 1.8: Interpretation of Results\n",
    "\n",
    "1. Print the results from the topic using the `.print_topics()` method specifying the argument `num_topics` with your choice of $k$. If you do not specify $k$, it leaves out some of the topics.\n",
    "2. Use the `TopicInspector` class from the tutorial to see the topic-word distributions and topic-document distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a29d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicInspector:\n",
    "    \"\"\" A class for inspecting and analyzing Latent Dirichlet Allocation (LDA) models. \n",
    "        \n",
    "        Attributes:\n",
    "            lda_model (gensim.models.LdaModel): The LDA model to be inspected.\n",
    "            vocab (dict): A vocabulary mapping from word IDs to words.\n",
    "            corpus (list of list of tuples): The corpus of documents used to train the LDA model.\n",
    "            num_topics (int): The number of topics in the LDA model.\n",
    "            topn (int, optional): The number of top words in each topic to consider (default is 10).\n",
    "        \n",
    "        Methods:\n",
    "            id2token(wid): Convert a word ID to its corresponding word in the vocabulary.\n",
    "            get_topic_words(tid): Get the top words associated with a given topic.\n",
    "            get_topic_word_prob(tid): Get the probabilities of the top words in a given topic.\n",
    "            topic_word_df(): Create a DataFrame representing the top words for each topic.\n",
    "            topic_doc_df(add_max_topic=True, add_max_score=True): Create a DataFrame representing the topic distribution for each document in the corpus.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lda_model, vocab, corpus, topn=10):\n",
    "        self.lda_model = lda_model\n",
    "        self.vocab = vocab\n",
    "        self.corpus = corpus\n",
    "        self.num_topics = self.lda_model.num_topics\n",
    "        self.topn = topn\n",
    "    \n",
    "    def id2token(self, wid):\n",
    "        return self.vocab[wid]\n",
    "    \n",
    "    def get_topic_words(self, tid):\n",
    "        topic_terms = self.lda_model.get_topic_terms(tid, topn=self.topn)\n",
    "        wordids, score = zip(*topic_terms)\n",
    "        return [self.id2token(x) for x in wordids]\n",
    "    \n",
    "    def get_topic_word_prob(self, tid):\n",
    "        topic_terms = self.lda_model.get_topic_terms(tid, topn=self.topn)\n",
    "        wordids, score = zip(*topic_terms)\n",
    "        return score\n",
    "    \n",
    "    def topic_word_df(self):\n",
    "        \n",
    "        topic_df_list = []\n",
    "        for k in range(0, self.num_topics - 1):\n",
    "            words = self.get_topic_words(tid=k)\n",
    "            topic_df_ = pd.DataFrame(words, columns=[f'topic{k}'])\n",
    "            topic_df_list.append(topic_df_)\n",
    "        \n",
    "        topic_word_df = pd.concat(topic_df_list, axis=1)\n",
    "        \n",
    "        return topic_word_df\n",
    "            \n",
    "    \n",
    "    def topic_doc_df(self, add_max_topic=True, add_max_score=True):\n",
    "        \n",
    "        topic_docs = self.lda_model.get_document_topics(self.corpus, minimum_probability=0)\n",
    "        doc_dist_list = []\n",
    "        for d in range(len(topic_docs)):\n",
    "            doc_dist = [x[1] for x in topic_docs[d]]\n",
    "            doc_dist_list.append(doc_dist)\n",
    "        \n",
    "        topic_doc_df = pd.DataFrame(doc_dist_list, columns=[f'topic{x}' for x in range(self.num_topics)])\n",
    "        if add_max_topic:\n",
    "            max_topics = topic_doc_df.idxmax(axis=1)\n",
    "        else:\n",
    "            max_topics = None\n",
    "        \n",
    "        if add_max_score:\n",
    "            max_scores = topic_doc_df.max(axis=1)\n",
    "        else:\n",
    "            max_scores = None\n",
    "        \n",
    "        if add_max_topic:\n",
    "            topic_doc_df['max_topic'] = max_topics\n",
    "        \n",
    "        if add_max_score: \n",
    "            topic_doc_df['max_score'] = max_scores\n",
    "\n",
    "        return topic_doc_df\n",
    "10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edffe549",
   "metadata": {},
   "source": [
    "#### Solution 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318c9c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "printer = sorted(lda.print_topics(num_topics=30))\n",
    "for i, k in enumerate(printer):\n",
    "    print(f\"Topic {k[0]}:\")\n",
    "    print(f\"Terms: {k[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a29c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define instance of the TopicInspector\n",
    "lda_inspect = TopicInspector(lda_model=lda, vocab=vocab, corpus=bow, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f446c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic-word dataframe\n",
    "topic_word_df = lda_inspect.topic_word_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec955ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic-doc dataframe\n",
    "topic_doc_df = lda_inspect.topic_doc_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22b1e0c",
   "metadata": {},
   "source": [
    "## 2.0 Sentiment\n",
    "\n",
    "The exercises in the this section regards sentiment analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f873fb97",
   "metadata": {},
   "source": [
    "#### Exercise 2.0\n",
    "\n",
    "We will work with dictionaries of positive and negative words compiled by the Lexicoder Sentiment Dictionary (LSD), which contains a list of positive and negative words. The LSD was originally compiled by Young and Soroka (2012) for English text with the purpose of studying newspaper content. \n",
    "\n",
    "The LSD dictionary was translated by Proksch et al. (2019) in their article *Multilingual sentiment analysis: A new approach to measuring conflict in legislative speeches* to study the government/opposition conflict-dynamic at the level of each bill in Western democracies.\n",
    " \n",
    "Note that the dictionaries contain bi-grams and maybe also tri-grams. We only, however, rely on the one-grams, i.e. single words.\n",
    "\n",
    "1. Load the two dictionaries from directly from the GitHub repo as pandas dataframes:\n",
    "- *lsd_pos.csv*\n",
    "- *lsd_neg.csv*\n",
    "2. Keep only single words (i.e. exclude bi-grams, tri-grams, and so on) and words with at least three characters\n",
    "\n",
    "3. Define a list called `docs` based on the dataframe `df`, which you loaded and filtered in *exercise 0.0* and *0.1*, respectively. \n",
    "\n",
    "4. Generate lazy tokens from the `docs` list using a simple `.split()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2763ddc0",
   "metadata": {},
   "source": [
    "#### Solution 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922aa9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in LSD dicts directly from GitHub\n",
    "base_url = 'https://raw.githubusercontent.com/mraskj/css_fall2023/master/data/'\n",
    "positive_words = pd.read_csv(base_url + 'lsd_pos.csv')\n",
    "negative_words = pd.read_csv(base_url + 'lsd_neg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dbfaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter word lists\n",
    "positive_words = [x for x in positive_words if len(x.split()) == 1 and len(x) >= 3]\n",
    "negative_words = [x for x in negative_words if len(x.split()) == 1 and len(x) >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8dbd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a list with the speeches called `docs`\n",
    "docs = list(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ab333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate lazy tokens using a simple .split()-method\n",
    "tokens = [doc.split() for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46590284",
   "metadata": {},
   "source": [
    "#### Exercise  2.1\n",
    "\n",
    "In this exercise, we will compute sentiment scores for each text using three different approaches.\n",
    "\n",
    "- Approach 1: Sentiment Difference: Differene of positive and negative words:\n",
    "    \\begin{align}\n",
    "        \\frac{(tokens_i\\cap \\text{positive words}) - (tokens_i\\cap \\text{negative words})}{N_{tokens_i}}\n",
    "    \\end{align}\n",
    "- Approach 2: Sentiment Ratio: Ratio between positive and negative words:\n",
    "    \\begin{align}\n",
    "        \\log_{10}\\frac{(tokens_i\\cap \\text{positive words}) + 0.5}{(tokens_i\\cap \\text{negative words}) + 0.5}\n",
    "    \\end{align}\n",
    "- Approach 3: Sentiment Weights: Weighted average of positive and negative words\n",
    "\n",
    "\n",
    "For *Approach 1* and *Approach 2*, we rely on the intersection of words, which is basically the same as the summation notation we saw on the slides. For *Approach 3*, we rely on the Python module `Sentida` (https://github.com/Guscode/Sentida), which computes a weighted average.\n",
    "\n",
    "The intersection of two lists are:\n",
    "    \\begin{align}\n",
    "        a &= [1,2,3] \\\\\n",
    "        b &= [1,4,3] \\\\\n",
    "        a\\cap b &= [1,3]\n",
    "    \\end{align}\n",
    "\n",
    "1. Define a function that computes the intersection of two lists. If you can't figure a solution out, check the solutions.\n",
    "2. Compute the intersection between words in each speech and positive and negative words respectively. Assign the results to two lists: `positivity_words` and `negavitity_words`.\n",
    "3. Compute the sentiment scores for the three different approaches and assign to objects called:\n",
    "    - `sentiment_diff`\n",
    "    - `sentiment_ratio`\n",
    "    - `sentiment_weight`\n",
    "4. Compute the highest and lowest sentiment score for each of three approaches and their resulting indices (*Hints*: np.max/min vs. np.argmax/argmin)\n",
    "5. Compute the pairwise correlation between the three approaches and visualize them. I provide you with code for this step.\n",
    "6. Interpret the results. Which approaches correlate the most? Does it make sense? How can we interpret the scores? Are they absolute or relative?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f61527d",
   "metadata": {},
   "source": [
    "#### Solution 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd5acbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to compute intersection between two lists\n",
    "# We use the function for words, but the function generalizes to any type of data\n",
    "def intersection(l1, l2):\n",
    "    return [v for v in l1 if v in l2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae0ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute intersection between tokens positive and negative words, respectively\n",
    "positivity_words = [intersection(toks, positive_words) for toks in tokens]\n",
    "negativity_words = [intersection(toks, negative_words) for toks in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac1d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: \n",
    "pos_neg_diff = [len(a) - len(b) for a, b in zip(positivity_words, negativity_words)]\n",
    "sentiment_diff = [x / len(y) for x, y in zip(pos_neg_diff, tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df762338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2\n",
    "sentiment_ratio = [np.log10((len(a) + 0.5 )/ (len(b) + 0.5)) for a, b in zip(positivity_words, negativity_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d994d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 3\n",
    "sentiment_weight = [round(Sentida().sentida(doc, output = \"mean\"), 3) for doc in tqdm(docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8781af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should have sentiment scores of equal length if everything is done properly\n",
    "assert len(sentiment_diff) == len(sentiment_ratio) == len(sentiment_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65e37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents with lowest/highest sentiment scores\n",
    "print(f\"Lowest sentiment scores:\\n\"\n",
    "      f\"-----------------------------\\n\"\n",
    "      f\"Approach 1 (Difference): {round(np.min(sentiment_diff), 3)}\\n\"\n",
    "      f\"Approach 2 (Ratio): {round(np.min(sentiment_ratio), 3)}\\n\"\n",
    "      f\"Approach 3 (Weights): {round(np.min(sentiment_weight), 3)}\\n\\n\"\n",
    "      f\"-----------------------------\\n\"\n",
    "      f\"Highest sentiment scores:\\n\"\n",
    "      f\"Approach 1 (Difference): {round(np.max(sentiment_diff), 3)}\\n\"\n",
    "      f\"Approach 2 (Ratio): {round(np.max(sentiment_ratio), 3)}\\n\"\n",
    "      f\"Approach 3 (Weights): {round(np.max(sentiment_weight), 3)}\\n\\n\"\n",
    "      f\"-----------------------------\\n\\n\"\n",
    "      f\"Documents with lowest sentiment scores:\\n\\n\"\n",
    "      f\"------------\\n\"\n",
    "      f\"Approach 1 (Difference): {docs[np.argmin(sentiment_diff)]}\\n\\n\"\n",
    "      f\"------------\\n\"\n",
    "      f\"Approach 2 (Ratio): {docs[np.argmin(sentiment_ratio)]}\\n\\n\"\n",
    "      f\"------------\\n\"\n",
    "      f\"Approach 3 (Weights): {docs[np.argmin(sentiment_weight)]}\\n\\n\"\n",
    "      f\"-----------------------------\\n\\n\"\n",
    "      f\"Documents with highest sentiment scores:\\n\\n\"\n",
    "      f\"------------\\n\"\n",
    "      f\"Approach 1 (Difference): {docs[np.argmax(sentiment_diff)]}\\n\\n\"\n",
    "      f\"------------\\n\"\n",
    "      f\"Approach 2 (Ratio): {docs[np.argmax(sentiment_ratio)]}\\n\\n\"\n",
    "      f\"------------\\n\"\n",
    "      f\"Approach 3 (Weights): {docs[np.argmax(sentiment_weight)]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6f48a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module to plot heatmap as image\n",
    "import plotly.express as px\n",
    "\n",
    "# Convert the sentiment scores to a dataframe \n",
    "sent_df = pd.DataFrame({'sent_diff': sentiment_diff,\n",
    "                  'sent_ratio': sentiment_ratio,\n",
    "                  'sent_weight': sentida_scores})\n",
    "\n",
    "# Compute the pairwise correlation using pearson's r\n",
    "corr_df = sent_df.corr(method=\"pearson\")\n",
    "\n",
    "# Generate visualization\n",
    "corr_heatmap = px.imshow(corr_df.to_numpy(), \n",
    "          x=list(corr_df.index),\n",
    "          y=list(corr_df.index),\n",
    "          labels=dict(color=\"Similarity Score\"),\n",
    "          color_continuous_scale='GnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb7d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print heatmap\n",
    "corr_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the words used by Sentida\n",
    "# aarup_dict = pd.read_csv('https://raw.githubusercontent.com/Guscode/Sentida/master/python/sentida/aarup.csv', sep=',',\n",
    "#                         encoding='latin1', index_col=[0])\n",
    "# aarup_dict = aarup_dict.iloc[:,1:]\n",
    "\n",
    "# It uses a bunch of rules to arrive at the estimate - see https://github.com/Guscode/Sentida/blob/master/python/sentida/__main__.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
